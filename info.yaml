(project):
    Q&A:
        - Whats the main idea(s)?:
            - there are two models; the agent and the coach ("dynamics" in the code)
            - the agent is a SAC model that is pretty typical
            - the coach tries to forcast multiple steps into the future in a way that helps the model
        - How do I judge performance?:
            - Smaller epsilon (lieniency of predicted_state ~~== actual_future_state)
            - Larger forcast (the avergage length of predictions that were correct)
            - Rewards that are most similar to the optimial policy (green line in .ipynb graph)
        - What is the main loss function?: |
            there's a few, and they're defined in main/training/train_dynamics.py on the DynamicsModel class
            
    tasks/questions:
        - question: what should the env_settings be for the 5 non-lunar lander env's
        - question:
            for predpolicy inside of train_afrl:
                - why is it a copy of the actor?
                - why is it being trained?
                - why is it not saved?
        - question: why set deterministic=True, and then sample from normal curve (dynamics experience function)
        - question: whats the difference between train_afrl.py and train_predpolicy.py
        - question: why does dynamics.forward have no_grad
        
        - todo: read the paper more
        - question: whats the difference between the different Q functions
        - task: redo the dynamics.forward and .predict method (predict shouldnt have gradient tracking and forward should)

    # a central place for filepaths
    (path_to):
        folder:
            agent_models: "./data/models/agents/"
            dynamics_models: "./data/models/dynamics/"
            results: "./data/results/"
            visuals: "./visuals/"

    (local_data): ./local_data.ignore.yaml
    (profiles):
        (default):
            experiment_name: null
            mode: development
            force_cpu: false
            env_names: ["LunarLanderContinuous-v2", "MountainCarContinuous-v0"]
            train_agent:
                iterations: 100000
            train_dynamics:
                number_of_episodes: 20
                number_of_epochs: 100
                train_test_split: 0.7
                minibatch_size: 32
                loss_function: state_prediction_loss # "value_prediction_loss", "action_prediction_loss", "state_prediction_loss", "consistent_coach_loss" <- each is a method on class DynamicsModel
            train_predictive:
                weight_update_frequency: 32 # every 32 replan iterations
                initial_horizon_size: 5
                loss_threshold: 0.5
            
            gym_env_settings:
                LunarLanderContinuous-v2:
                    number_of_experiments: 50
                    agent_discount_factor: 0.98
                    actor_copy_learning_rate: 0.0001
                    max_score: 70  # discounted ep reward
                    min_score: -100
                    horizons: # acceptability threshold (epsilon) => forecast length (horizon)
                        0.001: 5
                        0.0025: 5
                        0.005: 10
                        0.0075: 15
                        0.01: 20
                MountainCarContinuous-v0: # FIXME: this is just a copy of LunarLanderContinuous-v2 to avoid the code throwing errors
                    number_of_experiments: 50
                    agent_discount_factor: 0.98
                    actor_copy_learning_rate: 0.0001
                    max_score: 70
                    min_score: -100
                    horizons:
                        0.001: 5
                        0.0025: 5
                        0.005: 10
                        0.0075: 15
                        0.01: 20
                AntBulletEnv-v0: # FIXME: this is just a copy of LunarLanderContinuous-v2 to avoid the code throwing errors
                    number_of_experiments: 50
                    agent_discount_factor: 0.98
                    actor_copy_learning_rate: 0.0001
                    max_score: 70
                    min_score: -100
                    horizons:
                        0.001: 5
                        0.0025: 5
                        0.005: 10
                        0.0075: 15
                        0.01: 20
                HopperBulletEnv-v0: # FIXME: this is just a copy of LunarLanderContinuous-v2 to avoid the code throwing errors
                    number_of_experiments: 50
                    agent_discount_factor: 0.98
                    actor_copy_learning_rate: 0.0001
                    max_score: 70
                    min_score: -100
                    horizons:
                        0.001: 5
                        0.0025: 5
                        0.005: 10
                        0.0075: 15
                        0.01: 20
                BipedalWalker-v3: # FIXME: this is just a copy of LunarLanderContinuous-v2 to avoid the code throwing errors
                    number_of_experiments: 50
                    agent_discount_factor: 0.98
                    actor_copy_learning_rate: 0.0001
                    max_score: 70
                    min_score: -100
                    horizons:
                        0.001: 5
                        0.0025: 5
                        0.005: 10
                        0.0075: 15
                        0.01: 20
                HalfCheetahBulletEnv-v0: # FIXME: this is just a copy of LunarLanderContinuous-v2 to avoid the code throwing errors
                    number_of_experiments: 50
                    agent_discount_factor: 0.98
                    actor_copy_learning_rate: 0.0001
                    max_score: 70
                    min_score: -100
                    horizons:
                        0.001: 5
                        0.0025: 5
                        0.005: 10
                        0.0075: 15
                        0.01: 20
        
        FORCE_CPU:
            force_cpu: true
        
        ENVS=BASIC:
            env_names: [ "LunarLanderContinuous-v2", "MountainCarContinuous-v0" ]
        ENVS=HARD:
            env_names: [ "AntBulletEnv-v0", "HopperBulletEnv-v0", BipedalWalker-v3, "HalfCheetahBulletEnv-v0" ]
        ENVS=FULL:
            env_names: [ "LunarLanderContinuous-v2", "MountainCarContinuous-v0", "AntBulletEnv-v0", "HopperBulletEnv-v0", BipedalWalker-v3, "HalfCheetahBulletEnv-v0" ]
            # inside the .ipynb:
                # Lander
                # Humanoid
                # Hopper
                # Cheetah
                # Ant
                # Walker
            # Available: 
                # Reacher-v2
                # Pusher-v2
                # Thrower-v2
                # Striker-v2
                # InvertedPendulum-v2
                # InvertedDoublePendulum-v2
                # HalfCheetah-v2
                # HalfCheetah-v3
                # Hopper-v2
                # Hopper-v3
                # Swimmer-v2
                # Swimmer-v3
                # Walker2d-v2
                # Walker2d-v3
                # Ant-v2
                # Ant-v3
                # Humanoid-v2
                # Humanoid-v3
                # HumanoidStandup-v2
        