(project):
    Q&A:
        - Whats the main idea(s)?:
            - there are two models; the agent and the coach
            - the agent is a SAC model that is pretty typical
            - the coach tries to forcast multiple steps into the future in a way that helps the agent
        - How do I judge performance?:
            - Smaller epsilon (lieniency of predicted_state ~~== actual_future_state)
            - Larger forcast (the avergage length of predictions that were correct)
            - Rewards that are most similar to the optimial policy (green line in .ipynb graph)
        - What is the main loss function?: |
            there's a few, and they're defined in main/training/train_coach.py on the CoachClass class
            
    tasks/questions:
        - question: why would q do worse than state prediction
        - task: attempt phasing transition from state based to q loss and future
        - todo: research why loss combination isn't working
        - attempt: attempt seperate optimizers for multiple losses
        - attempt: look 2 or 3 steps into the future loss
        - future_task: make horizon obsolete by having a retroactive planner. Dont plan, until the plan comes into question
        - task: 
            original: discuss possible theory problems with size of min/max skewing epsilon if they're abnormally large
            resovled by: auto-optimize the epsilon, ignore the min/max sizing

    # a central place for filepaths
    (path_to):
        folder:
            agent_models: "./data/models/agents/"
            coach_models: "./data/models/coach/"
            results: "./results/"

    (local_data): ./local_data.ignore.yaml
    (profiles): # NOTE: more than one profile is picked, and the values are merged. The equal signs are just part of the name-string (not special)
        (default):
            experiment_name: null
            env_name: "LunarLanderContinuous-v2" # can be any of: "LunarLanderContinuous-v2", "MountainCarContinuous-v0", "AntBulletEnv-v0", "HopperBulletEnv-v0", BipedalWalker-v3, "HalfCheetahBulletEnv-v0"
            force_cpu: false
            load:
                agent_path: null # there is a default path, and this overrides the default path (designed to be used from CLI)
                coach_path: null # there is a default path, and this overrides the default path (designed to be used from CLI)
            
            agent_compare_test:
                number_of_agents: 10
                number_of_episodes: 10
                base_name: default_sac
            
            train_agent:
                model_name: default_sac_4 # was the best (by far) out of 10
                iterations: 200000
                force_retrain: false
                    
            train_coach:
                force_retrain: false
                learning_rate: 0.0001
                hidden_sizes: [64, 64, 64, 64]
                loss_function: state_prediction_loss # "value_prediction_loss", "action_prediction_loss", "state_prediction_loss", "consistent_coach_loss" <- each is a method on class Coach
                consistent_coach_loss:
                    scale_future_state_loss: 1000000
                delayed_coach_loss:
                    epochs_of_delay: 50
                value_plus_state_loss:
                    value_proportion: 0.5
                number_of_episodes: 100
                number_of_epochs: 100 # should be 100
                train_test_split: 0.7
                batch_size: 512
                with_card: true
    
            test_predictor:
                force_recompute: false
                graph_smoothing: 3 # rolling average of _ number values
                number_of_episodes_for_baseline: 30
                number_of_epochs_for_optimal_parameters: 15
                number_of_episodes_for_testing: 10 # original value is 50 
                initial_epsilon: 100.1 # overshooting this initial value is better than undershooting
                initial_horizon: 16 # units = timesteps. Mostly irrelevent
                reward_discount: 0.98
                acceptable_performance_loss: 1 # units = standard deviations
                confidence_interval_for_convergence: 90
                horizons:
                    0.001: 5
                    0.0025: 5
                    0.005: 10
                    0.0075: 15
                    0.01: 20
        
        FORCE_CPU_PROFILE:
            force_cpu: true
            
        ENV=LUNAR:
            env_name: "LunarLanderContinuous-v2"
            load:
                agent_path: "./subrepos/rl-trained-agents/sac/LunarLanderContinuous-v2_1/LunarLanderContinuous-v2.zip" 
            train_agent:
                iterations: 200000
                force_retrain: false
                learning_rate: 0.0001
                hidden_sizes: [64, 64, 64, 64]
            test_predictor:
                number_of_timesteps: 2000000
                min_reward_single_timestep: -100
                max_reward_single_timestep: 70
                horizons:
                    0.0: 1
                    0.005: 10
                    0.01: 20
        ENV=HOPPER:
            env_name: "HopperBulletEnv-v0"    
            train_agent:
                reward_discount: 0.98
            test_predictor:
                min_reward_single_timestep: 18
                max_reward_single_timestep: 110
                horizons:
                0.1: 26
                0.01: 16
                0.001: 11
        ENV=CHEETAH:
            env_name: "HalfCheetahBulletEnv-v0"    
            train_agent:
                reward_discount: 0.98
                learning_rate: 0.00073
            test_predictor:
                min_reward_single_timestep: -60
                max_reward_single_timestep: 100
                number_of_timesteps: 1000000
                horizons:
                    0: 1
                    0.001: 11
                    0.01: 13
                    0.1: 26
            hparams: 
                learning_rate: 7.3e-4
                buffer_size: 300000
                batch_size: 512
                ent_coef: auto
                gamma: 0.98
                tau: 0.02
                train_freq: 8
                gradient_steps: 8
                learning_starts: 10000
        ENV=WALKER:
            env_name: "BipedalWalker-v3"    
            train_agent:
                reward_discount: 0.98
            test_predictor:
                min_reward_single_timestep: -23
                max_reward_single_timestep: 5
                horizons:
                    0.1: 26
                    0.01: 16
                    0.001: 11
        ENV=ANT:
            env_name: "AntBulletEnv-v0"    
            train_agent:
                reward_discount: 0.98
            test_predictor:
                min_reward_single_timestep: 19
                max_reward_single_timestep: 105
                horizons:
                    0: 1
                    0.0007: 4
                    0.0015: 16
                    0.0020: 26
            # Max Episode Reward: 852.4950843086151
            # Min Episode Reward: 153.87762677698575
            # Max Timestep Reward: 2.3122164298644683
            # Min Timestep Reward: -1.7494330256972288
        ENV=HUMANOID:
            env_name: "HumanoidBulletEnv-v0"
            reward_discount: 0.98
            train_coach:
                learning_rate: 0.0003
            test_predictor:
                min_reward_single_timestep: -30
                max_reward_single_timestep: 113
                number_of_timesteps: 3000000
                horizons:
                    0.1: 26
                    0.01: 11
                    0.001: 5
            hparams: 
                learning_rate: 0.0003
                buffer_size: 1000000
                batch_size: 64
                ent_coef: auto
                train_freq: 1
                gradient_steps: 1
                learning_starts: 1000
                
                # Available: 
                    # Reacher-v2
                    # Pusher-v2
                    # Thrower-v2
                    # Striker-v2
                    # InvertedPendulum-v2
                    # InvertedDoublePendulum-v2
                    # HalfCheetah-v2
                    # HalfCheetah-v3
                    # Hopper-v2
                    # Hopper-v3
                    # Swimmer-v2
                    # Swimmer-v3
                    # Walker2d-v2
                    # Walker2d-v3
                    # Ant-v2
                    # Ant-v3
                    # Humanoid-v2
                    # Humanoid-v3
                    # HumanoidStandup-v2
        # 
        # BASELINE
        # 
        LOSS=BASELINE:
            train_coach:
                force_retrain: false
                learning_rate: 0.0001
                hidden_sizes: [64, 64, 64, 64]
                loss_function: state_prediction_loss # "value_prediction_loss", "action_prediction_loss", "state_prediction_loss", "consistent_coach_loss" <- each is a method on class Coach
                consistent_coach_loss:
                    scale_future_state_loss: 1000000
                delayed_coach_loss:
                    epochs_of_delay: 50
                value_plus_state_loss:
                    value_proportion: 0.5
                number_of_episodes: 100
                number_of_epochs: 100 # should be 100
                train_test_split: 0.7
                batch_size: 512
                with_card: true
            train_coach:
                force_retrain: false
        
        CUSTOM=LUNAR_BASELINE_V1:
            # python ./main/run/full.py -- @ENV=LUNAR @LOSS=BASELINE @CUSTOM=LUNAR_BASELINE_V1 experiment_name:baseline_1
            train_coach:
                number_of_episodes: 100
            test_predictor:
                force_recompute: false
                number_of_episodes: 10
                horizons:
                    0.0: 1
                    0.005: 10
                    0.01: 20
        
        CUSTOM=LUNAR_BASELINE_V2:
            # python ./main/run/full.py -- @ENV=LUNAR @LOSS=BASELINE @CUSTOM=LUNAR_BASELINE_V2 experiment_name:baseline_2
            test_predictor:
                acceptable_performance_loss: 2 # units = standard deviations
                force_recompute: true
                number_of_episodes_for_baseline: 30
                number_of_epochs_for_optimal_parameters: 15
                number_of_episodes_for_testing: 9
                initial_epsilon: 2.1170
                initial_horizon: 20
        
        CUSTOM=LUNAR_BASELINE_V3:
            # python ./main/run/full.py -- @ENV=LUNAR @LOSS=BASELINE @CUSTOM=LUNAR_BASELINE_V3 experiment_name:baseline_3
            test_predictor:
                acceptable_performance_loss: 2 # units = standard deviations
                force_recompute: true
                number_of_episodes_for_baseline: 30
                number_of_epochs_for_optimal_parameters: 15
                number_of_episodes_for_testing: 9
                initial_epsilon: &baseline_lunar_initial_epsilon 1.5 
                initial_horizon: &baseline_lunar_initial_horizon 45  
        
        CUSTOM=CHEETAH_BASELINE:
            # python ./main/run/full.py -- @ENV=CHEETAH @LOSS=BASELINE @CUSTOM=CHEETAH_BASELINE experiment_name:baseline_2
            test_predictor:
                force_recompute: false
                acceptable_performance_loss: 2 # units = standard deviations
                number_of_episodes_for_baseline: 30
                number_of_epochs_for_optimal_parameters: 15
                number_of_episodes_for_testing: 10
                initial_epsilon: *baseline_lunar_initial_epsilon
                initial_horizon: *baseline_lunar_initial_horizon
        
        CUSTOM=ANT_BASELINE:
            # python ./main/run/full.py -- @ENV=ANT @LOSS=BASELINE @CUSTOM=ANT_BASELINE experiment_name:baseline_2
            test_predictor:
                force_recompute: false
                acceptable_performance_loss: 2 # units = standard deviations
                number_of_episodes_for_baseline: 30
                number_of_epochs_for_optimal_parameters: 15
                number_of_episodes_for_testing: 10
                initial_epsilon: *baseline_lunar_initial_epsilon
                initial_horizon: *baseline_lunar_initial_horizon
        
        # 
        # Q LOSS (aka value_prediction_loss)
        # 
        LOSS=Q:
            train_coach:
                loss_function: "value_prediction_loss"
                force_retrain: false
                consistent_coach_loss:
                    scale_future_state_loss: 1
                value_plus_state_loss:
                    value_proportion: 0.5 # 50-50 split
                number_of_epochs: 120
                batch_size: 512
            test_predictor:
                force_recompute: false
        
        CUSTOM=LUNAR_Q:
            # python ./main/run/full.py -- @ENV=LUNAR @LOSS=Q @CUSTOM=LUNAR_Q experiment_name:q_loss_1
            test_predictor:
                force_recompute: false
                acceptable_performance_loss: 2 # units = standard deviations
                number_of_episodes_for_baseline: 30
                number_of_epochs_for_optimal_parameters: 15
                number_of_episodes_for_testing: 9
                initial_epsilon: *baseline_lunar_initial_epsilon
                initial_horizon: *baseline_lunar_initial_horizon
        
        CUSTOM=CHEETAH_Q:
            # python ./main/run/full.py -- @ENV=CHEETAH @LOSS=Q @CUSTOM=CHEETAH_Q experiment_name:q_loss_1
            test_predictor:
                force_recompute: false
                acceptable_performance_loss: 2 # units = standard deviations
                number_of_episodes_for_baseline: 30
                number_of_epochs_for_optimal_parameters: 15
                number_of_episodes_for_testing: 10
                initial_epsilon: *baseline_lunar_initial_epsilon
                initial_horizon: *baseline_lunar_initial_horizon
        
        CUSTOM=ANT_Q:
            # python ./main/run/full.py -- @ENV=ANT @LOSS=Q @CUSTOM=ANT_Q experiment_name:q_loss_1
            test_predictor:
                force_recompute: false
                acceptable_performance_loss: 2 # units = standard deviations
                number_of_episodes_for_baseline: 30
                number_of_epochs_for_optimal_parameters: 15
                number_of_episodes_for_testing: 10
                initial_epsilon: *baseline_lunar_initial_epsilon
                initial_horizon: *baseline_lunar_initial_horizon
        
        # 
        # Q PLUS STATE
        # 
        LOSS=Q_PLUS_STATE:
            train_coach:
                loss_function: "value_plus_state_loss"
                force_retrain: false
                number_of_epochs: 120
                batch_size: 512
                consistent_coach_loss:
                    scale_future_state_loss: 1
                value_plus_state_loss:
                    value_proportion: 0.01
        
        CUSTOM=LUNAR_Q_PLUS_STATE:
            # python ./main/run/full.py -- @ENV=LUNAR @LOSS=Q @CUSTOM=LUNAR_Q_PLUS_STATE experiment_name:q_plus_state_loss_1
            test_predictor:
                force_recompute: false
                acceptable_performance_loss: 2 # units = standard deviations
                number_of_episodes_for_baseline: 30
                number_of_epochs_for_optimal_parameters: 15
                number_of_episodes_for_testing: 9
                initial_epsilon: *baseline_lunar_initial_epsilon
                initial_horizon: *baseline_lunar_initial_horizon
        
        CUSTOM=LUNAR_Q_UNFAVORED_PLUS_STATE:
            # python ./main/run/full.py -- @ENV=LUNAR @LOSS=Q_PLUS_STATE @CUSTOM=LUNAR_Q_UNFAVORED_PLUS_STATE experiment_name:q_unfavored_plus_state_loss_1
            train_coach:
                force_retrain: false
                value_plus_state_loss:
                    value_proportion: 0.01
        
        CUSTOM=LUNAR_Q_IGNORED_PLUS_STATE: # should be equivlent to baseline (sanity check)
            # python ./main/run/full.py -- @ENV=LUNAR @LOSS=Q_PLUS_STATE @CUSTOM=LUNAR_Q_IGNORED_PLUS_STATE experiment_name:q_ignored_plus_state_loss_1
            train_coach:
                value_plus_state_loss:
                    value_proportion: 0
        
        CUSTOM=CHEETAH_Q_PLUS_STATE:
            # python ./main/run/full.py -- @ENV=CHEETAH @LOSS=Q @CUSTOM=CHEETAH_Q_PLUS_STATE experiment_name:q_plus_state_loss_1
            test_predictor:
                force_recompute: false
                acceptable_performance_loss: 2 # units = standard deviations
                number_of_episodes_for_baseline: 30
                number_of_epochs_for_optimal_parameters: 15
                number_of_episodes_for_testing: 9
                initial_epsilon: *baseline_lunar_initial_epsilon
                initial_horizon: *baseline_lunar_initial_horizon
        
        CUSTOM=CHEETAH_Q_UNFAVORED_PLUS_STATE:
            # python ./main/run/full.py -- @ENV=CHEETAH @LOSS=Q_PLUS_STATE @CUSTOM=CHEETAH_Q_UNFAVORED_PLUS_STATE experiment_name:q_unfavored_plus_state_loss_1
            train_coach:
                force_retrain: false
                value_plus_state_loss:
                    value_proportion: 0.01
            test_predictor:
                force_recompute: false
                acceptable_performance_loss: 2 # units = standard deviations
                number_of_episodes_for_baseline: 30
                number_of_epochs_for_optimal_parameters: 15
                number_of_episodes_for_testing: 9
                initial_epsilon: *baseline_lunar_initial_epsilon
                initial_horizon: *baseline_lunar_initial_horizon
        
        CUSTOM=CHEETAH_Q_IGNORED_PLUS_STATE: # should be equivlent to baseline (sanity check)
            # python ./main/run/full.py -- @ENV=CHEETAH @LOSS=Q_PLUS_STATE @CUSTOM=CHEETAH_Q_IGNORED_PLUS_STATE experiment_name:q_ignored_plus_state_loss_1
            train_coach:
                value_plus_state_loss:
                    value_proportion: 0
            test_predictor:
                force_recompute: false
                acceptable_performance_loss: 2 # units = standard deviations
                number_of_episodes_for_baseline: 30
                number_of_epochs_for_optimal_parameters: 15
                number_of_episodes_for_testing: 9
                initial_epsilon: *baseline_lunar_initial_epsilon
                initial_horizon: *baseline_lunar_initial_horizon
        
        CUSTOM=ANT_Q_PLUS_STATE:
            # python ./main/run/full.py -- @ENV=ANT @LOSS=Q @CUSTOM=ANT_Q_PLUS_STATE experiment_name:q_plus_state_loss_1
            test_predictor:
                force_recompute: false
                acceptable_performance_loss: 2 # units = standard deviations
                number_of_episodes_for_baseline: 30
                number_of_epochs_for_optimal_parameters: 15
                number_of_episodes_for_testing: 9
                initial_epsilon: *baseline_lunar_initial_epsilon
                initial_horizon: *baseline_lunar_initial_horizon
        
        CUSTOM=ANT_Q_UNFAVORED_PLUS_STATE:
            # python ./main/run/full.py -- @ENV=ANT @LOSS=Q_PLUS_STATE @CUSTOM=ANT_Q_UNFAVORED_PLUS_STATE experiment_name:q_unfavored_plus_state_loss_1
            train_coach:
                force_retrain: false
                value_plus_state_loss:
                    value_proportion: 0.01
            test_predictor:
                force_recompute: false
                acceptable_performance_loss: 2 # units = standard deviations
                number_of_episodes_for_baseline: 30
                number_of_epochs_for_optimal_parameters: 15
                number_of_episodes_for_testing: 9
                initial_epsilon: *baseline_lunar_initial_epsilon
                initial_horizon: *baseline_lunar_initial_horizon
        
        CUSTOM=ANT_Q_IGNORED_PLUS_STATE: # should be equivlent to baseline (sanity check)
            # python ./main/run/full.py -- @ENV=ANT @LOSS=Q_PLUS_STATE @CUSTOM=ANT_Q_IGNORED_PLUS_STATE experiment_name:q_ignored_plus_state_loss_1
            train_coach:
                value_plus_state_loss:
                    value_proportion: 0
            test_predictor:
                force_recompute: false
                acceptable_performance_loss: 2 # units = standard deviations
                number_of_episodes_for_baseline: 30
                number_of_epochs_for_optimal_parameters: 15
                number_of_episodes_for_testing: 9
                initial_epsilon: *baseline_lunar_initial_epsilon
                initial_horizon: *baseline_lunar_initial_horizon
        
        # 
        # CONSISTENT_COACH (main loss function)
        #
        LOSS=CONSISTENT_COACH:
            train_coach:
                loss_function: consistent_coach_loss
                consistent_coach_loss:
                    scale_future_state_loss: 1
        
        CUSTOM=LUNAR_CONSISTENT_COACH:
            # python ./main/run/full.py -- @ENV=LUNAR @LOSS=CONSISTENT_COACH @CUSTOM=LUNAR_CONSISTENT_COACH experiment_name:consistent_coach_loss_1
            test_predictor:
                force_recompute: false
        
        CUSTOM=CHEETAH_CONSISTENT_COACH:
            # python ./main/run/full.py -- @ENV=CHEETAH @LOSS=CONSISTENT_COACH @CUSTOM=CHEETAH_CONSISTENT_COACH experiment_name:consistent_coach_loss_1
            test_predictor:
                force_recompute: false
                acceptable_performance_loss: 2 # units = standard deviations
                number_of_episodes_for_baseline: 30
                number_of_epochs_for_optimal_parameters: 15
                number_of_episodes_for_testing: 9
                initial_epsilon: *baseline_lunar_initial_epsilon
                initial_horizon: *baseline_lunar_initial_horizon
        
        CUSTOM=ANT_CONSISTENT_COACH:
            # python ./main/run/full.py -- @ENV=ANT @LOSS=CONSISTENT_COACH @CUSTOM=ANT_CONSISTENT_COACH experiment_name:consistent_coach_loss_1
            test_predictor:
                force_recompute: false
                acceptable_performance_loss: 2 # units = standard deviations
                number_of_episodes_for_baseline: 30
                number_of_epochs_for_optimal_parameters: 15
                number_of_episodes_for_testing: 9
                initial_epsilon: *baseline_lunar_initial_epsilon
                initial_horizon: *baseline_lunar_initial_horizon
        
        # 
        # CONSISTENT_VALUE
        # 
        LOSS=CONSISTENT_VALUE:
            train_coach:
                loss_function: consistent_value_loss
        
        CUSTOM=CHEETAH_CONSISTENT_VALUE:
            # python ./main/run/full.py -- @ENV=CHEETAH @LOSS=CONSISTENT_VALUE @CUSTOM=CHEETAH_CONSISTENT_VALUE experiment_name:consistent_value_loss_1
            test_predictor:
                force_recompute: false
                acceptable_performance_loss: 2 # units = standard deviations
                number_of_episodes_for_baseline: 30
                number_of_epochs_for_optimal_parameters: 15
                number_of_episodes_for_testing: 9
                initial_epsilon: *baseline_lunar_initial_epsilon
                initial_horizon: *baseline_lunar_initial_horizon
        
        CUSTOM=ANT_CONSISTENT_VALUE:
            # python ./main/run/full.py -- @ENV=ANT @LOSS=CONSISTENT_VALUE @CUSTOM=ANT_CONSISTENT_VALUE experiment_name:consistent_value_loss_1
            test_predictor:
                force_recompute: false
                acceptable_performance_loss: 2 # units = standard deviations
                number_of_episodes_for_baseline: 30
                number_of_epochs_for_optimal_parameters: 15
                number_of_episodes_for_testing: 9
                initial_epsilon: *baseline_lunar_initial_epsilon
                initial_horizon: *baseline_lunar_initial_horizon
        
        # 
        # HALF_CONSISTENT_COACH
        # 
        LOSS=HALF_CONSISTENT_COACH:
            train_coach:
                loss_function: half_consistent_coach_loss
        
        CUSTOM=CHEETAH_HALF_CONSISTENT_COACH:
            # python ./main/run/full.py -- @ENV=CHEETAH @LOSS=HALF_CONSISTENT_COACH @CUSTOM=CHEETAH_HALF_CONSISTENT_COACH experiment_name:half_consistent_coach_loss_1
            test_predictor:
                force_recompute: false
                acceptable_performance_loss: 2 # units = standard deviations
                number_of_episodes_for_baseline: 30
                number_of_epochs_for_optimal_parameters: 15
                number_of_episodes_for_testing: 9
                initial_epsilon: *baseline_lunar_initial_epsilon
                initial_horizon: *baseline_lunar_initial_horizon
        
        CUSTOM=ANT_HALF_CONSISTENT_COACH:
            # python ./main/run/full.py -- @ENV=ANT @LOSS=HALF_CONSISTENT_COACH @CUSTOM=ANT_HALF_CONSISTENT_COACH experiment_name:half_consistent_coach_loss_1
            test_predictor:
                force_recompute: false
                acceptable_performance_loss: 2 # units = standard deviations
                number_of_episodes_for_baseline: 30
                number_of_epochs_for_optimal_parameters: 15
                number_of_episodes_for_testing: 9
                initial_epsilon: *baseline_lunar_initial_epsilon
                initial_horizon: *baseline_lunar_initial_horizon
        
        # 
        # STATE_TRIPLE
        # 
        LOSS=STATE_TRIPLE:
            train_coach:
                loss_function: state_triple
        
        CUSTOM=CHEETAH_STATE_TRIPLE:
            # python ./main/run/full.py -- @ENV=CHEETAH @LOSS=STATE_TRIPLE @CUSTOM=CHEETAH_STATE_TRIPLE experiment_name:state_triple_1
            test_predictor:
                force_recompute: false
                acceptable_performance_loss: 2 # units = standard deviations
                number_of_episodes_for_baseline: 30
                number_of_epochs_for_optimal_parameters: 15
                number_of_episodes_for_testing: 9
                initial_epsilon: *baseline_lunar_initial_epsilon
                initial_horizon: *baseline_lunar_initial_horizon
        
        CUSTOM=ANT_STATE_TRIPLE:
            # python ./main/run/full.py -- @ENV=ANT @LOSS=STATE_TRIPLE @CUSTOM=ANT_STATE_TRIPLE experiment_name:state_triple_1
            test_predictor:
                force_recompute: false
                acceptable_performance_loss: 2 # units = standard deviations
                number_of_episodes_for_baseline: 30
                number_of_epochs_for_optimal_parameters: 15
                number_of_episodes_for_testing: 9
                initial_epsilon: *baseline_lunar_initial_epsilon
                initial_horizon: *baseline_lunar_initial_horizon
        
        # 
        # DELAYED_COACH_LOSS
        # 
        LOSS=DELAYED_COACH_LOSS:
            train_coach:
                loss_function: delayed_coach_loss
        
        CUSTOM=CHEETAH_DELAYED_COACH_LOSS:
            # python ./main/run/full.py -- @ENV=CHEETAH @LOSS=DELAYED_COACH_LOSS @CUSTOM=CHEETAH_DELAYED_COACH_LOSS experiment_name:delayed_coach_loss_1
            test_predictor:
                force_recompute: false
                acceptable_performance_loss: 2 # units = standard deviations
                number_of_episodes_for_baseline: 30
                number_of_epochs_for_optimal_parameters: 15
                number_of_episodes_for_testing: 9
                initial_epsilon: *baseline_lunar_initial_epsilon
                initial_horizon: *baseline_lunar_initial_horizon
        
        CUSTOM=ANT_DELAYED_COACH_LOSS:
            # python ./main/run/full.py -- @ENV=ANT @LOSS=DELAYED_COACH_LOSS @CUSTOM=ANT_DELAYED_COACH_LOSS experiment_name:delayed_coach_loss_1
            test_predictor:
                force_recompute: false
                acceptable_performance_loss: 2 # units = standard deviations
                number_of_episodes_for_baseline: 30
                number_of_epochs_for_optimal_parameters: 15
                number_of_episodes_for_testing: 9
                initial_epsilon: *baseline_lunar_initial_epsilon
                initial_horizon: *baseline_lunar_initial_horizon
        
        # 
        # HALF_CONSISTENT_COACH
        # 
        LOSS=HALF_CONSISTENT_COACH:
            train_coach:
                loss_function: half_consistent_coach_loss
        
        CUSTOM=CHEETAH_HALF_CONSISTENT_COACH:
            # python ./main/run/full.py -- @ENV=CHEETAH @LOSS=HALF_CONSISTENT_COACH @CUSTOM=CHEETAH_HALF_CONSISTENT_COACH experiment_name:half_consistent_coach_loss_1
            test_predictor:
                force_recompute: false
                acceptable_performance_loss: 2 # units = standard deviations
                number_of_episodes_for_baseline: 30
                number_of_epochs_for_optimal_parameters: 15
                number_of_episodes_for_testing: 9
                initial_epsilon: *baseline_lunar_initial_epsilon
                initial_horizon: *baseline_lunar_initial_horizon
        
        CUSTOM=ANT_HALF_CONSISTENT_COACH:
            # python ./main/run/full.py -- @ENV=ANT @LOSS=HALF_CONSISTENT_COACH @CUSTOM=ANT_HALF_CONSISTENT_COACH experiment_name:half_consistent_coach_loss_1
            test_predictor:
                force_recompute: false
                acceptable_performance_loss: 2 # units = standard deviations
                number_of_episodes_for_baseline: 30
                number_of_epochs_for_optimal_parameters: 15
                number_of_episodes_for_testing: 9
                initial_epsilon: *baseline_lunar_initial_epsilon
                initial_horizon: *baseline_lunar_initial_horizon
        
        # 
        # DELAYED_COACH
        # 
        LOSS=DELAYED_COACH:
            train_coach:
                force_retrain: false
                loss_function: "delayed_coach_loss"
                consistent_coach_loss:
                    scale_future_state_loss: 1
                number_of_epochs: 120
                batch_size: 512
                
            test_predictor:
                force_recompute: false
        
        CUSTOM=LUNAR_DELAYED_COACH:
            # python ./main/run/full.py -- @ENV=LUNAR @LOSS=DELAYED_COACH @CUSTOM=LUNAR_DELAYED_COACH experiment_name:delayed_coach_loss_1
            test_predictor:
                acceptable_performance_loss: 2 # units = standard deviations
                force_recompute: true
                number_of_episodes_for_baseline: 30
                number_of_epochs_for_optimal_parameters: 5 # normally 15
                number_of_episodes_for_testing: 9
                initial_epsilon: 0.026220759371288625
                initial_horizon: 40
        
        CUSTOM=CHEETAH_DELAYED_COACH:
            # python ./main/run/full.py -- @ENV=CHEETAH @LOSS=DELAYED_COACH @CUSTOM=CHEETAH_DELAYED_COACH experiment_name:delayed_coach_loss_1
            experiment_name: "delayed_coach_loss_1"
            test_predictor:
                force_recompute: true
                number_of_episodes_for_baseline: 30
                number_of_epochs_for_optimal_parameters: 15
                number_of_episodes_for_testing: 10
                initial_epsilon: 0.0100
                initial_horizon: 76
        
        CUSTOM=ANT_DELAYED_COACH:
            # python ./main/run/full.py -- @ENV=ANT @LOSS=DELAYED_COACH @CUSTOM=ANT_DELAYED_COACH experiment_name:delayed_coach_loss_1
            experiment_name: "delayed_coach_loss_1"
            test_predictor:
                force_recompute: true
                number_of_episodes_for_baseline: 30
                number_of_epochs_for_optimal_parameters: 15
                number_of_episodes_for_testing: 10
                initial_epsilon: 0.0100
                initial_horizon: 76
