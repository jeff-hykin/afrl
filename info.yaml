(project):
    Q&A:
        - Whats the main idea(s)?:
            - there are two models; the agent and the coach ("dynamics" in the code)
            - the agent is a SAC model that is pretty typical
            - the coach tries to forcast multiple steps into the future in a way that helps the model
        - How do I judge performance?:
            - Smaller epsilon (lieniency of predicted_state ~~== actual_future_state)
            - Larger forcast (the avergage length of predictions that were correct)
            - Rewards that are most similar to the optimial policy (green line in .ipynb graph)
        - What is the main loss function?: |
            # given agent, coach, state, action, next_state
            
            predicted_next_state  = coach.predict_state(state, action)
            predicted_next_action = agent.choose_action(predicted_next_state)
            predicted_next_value  = agent.value_estimate(next_state, predicted_next_action)
            
            best_next_action = agent.choose_action(next_state)
            best_next_value = agent.value_estimate(next_state, best_next_action)
            
            coach.loss = best_next_value - predicted_next_value # when predicted_next_value is high, loss is low (negative)
            
            # which in theory, when differentiated simplifies to
            - predicted_next_value
            # TODO: I'm not yet convinced the simplification works.
            # => Yes best_next_value is constant for a single update step of the coach
            #    but with best_next_value changing and a replay buffer 
            #    I'm inclined to think it would be safer to compute/store the difference
            #    particularly for plotting the losses on a graph to see how 
            #    bad they are, while they're being measured against the Q val
            #    I don't think there's any computational savings as we should already have best_next_value
    meeting updates:
        - found another inline loss
        - found main issue, the inline loss was using an actor copy, and the copy wasnt being frozen
        - found core issue, how states are being sequentially predicted
        - train_afrl seems to only update the actor, not the dynamics. Cant tell if thats intentional
        - note: afrl is never saved as a model even though it is trained
        - note: epsilon plays a very different role now because its about how much worse the action is. If the actions were very different but had approximately the same value then planning will be viewed as successful
        - check: what if the prediction is only good because it doesn't matter what the agent does at those timesteps
    tasks/questions:
        - temp:
            - when is dynamics.forward used? (why does it have no_grad)
        - todo: read the paper
        - todo: change the dynamics testing loss to match the training
        - question: why does dynamics.forward have no_grad
        - question: what should the env_settings be for the 5 non-lunar lander env's
        - question: whats the difference between train_afrl.py and train_predpolicy.py
        - question: whats the difference between the different Q functions
        - question: why set deterministic=True, and then sample from normal curve (dynamics experience function)
        - check: maybe use torch.no_grad for parts of the loss (when running data through agent)
        - check: whats going on with the forecast indicies
        - problem: SAC doesn't have a pytorch-trackable method (can't be used in a loss function)

    # a central place for filepaths
    (path_to):
        folder:
            agent_models: "./data/models/agents/"
            dynamics_models: "./data/models/dynamics/"
            results: "./data/results"

    (local_data): ./local_data.ignore.yaml
    (profiles):
        (default):
            mode: development
            force_cpu: false
            env_names: ["LunarLanderContinuous-v2", "MountainCarContinuous-v0"]
            gamma: 0.98 # QUESTION: discount factor? its used in the .ipynb, but I don't know what its for exactly
            train_agent:
                iterations: 100000
            train_dynamics:
                number_of_episodes: 20
                number_of_epochs: 100
                train_test_split: 0.7
                minibatch_size: 32
                loss_function: state_prediction_loss # "value_prediction_loss", "action_prediction_loss", "state_prediction_loss" <- each is a method on class DynamicsModel
            train_predictive:
                weight_update_frequency: 32 # every 32 replan iterations
                initial_horizon_size: 5
                loss_threshold: 0.5
            
            gym_env_settings:
                LunarLanderContinuous-v2:
                    number_of_experiments: 50
                    actor_copy_learning_rate: 0.0001
                    max_score: 70  # discounted ep reward
                    min_score: -100
                    horizons: # acceptability threshold (epsilon) => forecast length (horizon)
                        0.001: 5
                        0.0025: 5
                        0.005: 10
                        0.0075: 15
                        0.01: 20
                MountainCarContinuous-v0: # FIXME: this is just a copy of LunarLanderContinuous-v2 to avoid the code throwing errors
                    number_of_experiments: 50
                    actor_copy_learning_rate: 0.0001
                    max_score: 70
                    min_score: -100
                    horizons:
                        0.001: 5
                        0.0025: 5
                        0.005: 10
                        0.0075: 15
                        0.01: 20
                AntBulletEnv-v0: # FIXME: this is just a copy of LunarLanderContinuous-v2 to avoid the code throwing errors
                    number_of_experiments: 50
                    actor_copy_learning_rate: 0.0001
                    max_score: 70
                    min_score: -100
                    horizons:
                        0.001: 5
                        0.0025: 5
                        0.005: 10
                        0.0075: 15
                        0.01: 20
                HopperBulletEnv-v0: # FIXME: this is just a copy of LunarLanderContinuous-v2 to avoid the code throwing errors
                    number_of_experiments: 50
                    actor_copy_learning_rate: 0.0001
                    max_score: 70
                    min_score: -100
                    horizons:
                        0.001: 5
                        0.0025: 5
                        0.005: 10
                        0.0075: 15
                        0.01: 20
                BipedalWalker-v3: # FIXME: this is just a copy of LunarLanderContinuous-v2 to avoid the code throwing errors
                    number_of_experiments: 50
                    actor_copy_learning_rate: 0.0001
                    max_score: 70
                    min_score: -100
                    horizons:
                        0.001: 5
                        0.0025: 5
                        0.005: 10
                        0.0075: 15
                        0.01: 20
                HalfCheetahBulletEnv-v0: # FIXME: this is just a copy of LunarLanderContinuous-v2 to avoid the code throwing errors
                    number_of_experiments: 50
                    actor_copy_learning_rate: 0.0001
                    max_score: 70
                    min_score: -100
                    horizons:
                        0.001: 5
                        0.0025: 5
                        0.005: 10
                        0.0075: 15
                        0.01: 20
        
        FORCE_CPU:
            force_cpu: true
        
        ENVS=BASIC:
            env_names: [ "LunarLanderContinuous-v2", "MountainCarContinuous-v0" ]
        ENVS=HARD:
            env_names: [ "AntBulletEnv-v0", "HopperBulletEnv-v0", BipedalWalker-v3, "HalfCheetahBulletEnv-v0" ]
        ENVS=FULL:
            env_names: [ "LunarLanderContinuous-v2", "MountainCarContinuous-v0", "AntBulletEnv-v0", "HopperBulletEnv-v0", BipedalWalker-v3, "HalfCheetahBulletEnv-v0" ]
            # inside the .ipynb:
                # Lander
                # Humanoid
                # Hopper
                # Cheetah
                # Ant
                # Walker
            # Available: 
                # Reacher-v2
                # Pusher-v2
                # Thrower-v2
                # Striker-v2
                # InvertedPendulum-v2
                # InvertedDoublePendulum-v2
                # HalfCheetah-v2
                # HalfCheetah-v3
                # Hopper-v2
                # Hopper-v3
                # Swimmer-v2
                # Swimmer-v3
                # Walker2d-v2
                # Walker2d-v3
                # Ant-v2
                # Ant-v3
                # Humanoid-v2
                # Humanoid-v3
                # HumanoidStandup-v2
        