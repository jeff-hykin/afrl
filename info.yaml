(project):
    Q&A:
        - Whats the main idea(s)?:
            - there are two models; the agent and the coach
            - the agent is a SAC model that is pretty typical
            - the coach tries to forcast multiple steps into the future in a way that helps the agent
        - How do I judge performance?:
            - Smaller epsilon (lieniency of predicted_state ~~== actual_future_state)
            - Larger forcast (the avergage length of predictions that were correct)
            - Rewards that are most similar to the optimial policy (green line in .ipynb graph)
        - What is the main loss function?: |
            there's a few, and they're defined in main/training/train_coach.py on the CoachClass class
            
    tasks/questions:
        - TODO: randomize the batches!!
        - question:
            - why is current method doing worst than state prediction
            - look 2 or 3 steps into the future loss
            - try large coefficients for q loss
            - phasing: transition from state based to q loss and future
        - task: log the losses independently and graph them
        - check: DONE; epsilon size compared to timestep q values for plan/replan (check as a percentage)
        - task: DONE; add graph of the average q-values so they can be compared as a percentage to the epsilon
        - task: DONE; graph a baseline using the state_prediction_loss with the good agent
        - discuss: stable agent 
        - discuss: loss function spread
        - discuss: new loss function coach_value_predict
        - task: discuss possible theory problems with size of min/max skewing epsilon if they're abnormally large
        - problem: epsilon is highly effected by max/min possible score. There's conflicting info on if min/max is supposed to be on a per-episode or per-timestep basis, and if its supposed to be discounted or not
        - solve: why slice [horizon:] instead of [:horizon], which of the two is correct
        - todo: make sure the graphs are actually measuring the correct thing

    # a central place for filepaths
    (path_to):
        folder:
            agent_models: "./data/models/agents/"
            coach_models: "./data/models/coach/"
            results: "./results/"

    (local_data): ./local_data.ignore.yaml
    (profiles):
        (default):
            experiment_name: null
            env_name: "LunarLanderContinuous-v2" # can be any of: "LunarLanderContinuous-v2", "MountainCarContinuous-v0", "AntBulletEnv-v0", "HopperBulletEnv-v0", BipedalWalker-v3, "HalfCheetahBulletEnv-v0"
            force_cpu: false
            mode: development
            load:
                agent_path: null # there is a default path, and this overrides the default path (designed to be used from CLI)
                coach_path: null # there is a default path, and this overrides the default path (designed to be used from CLI)
            
            agent_compare_test:
                number_of_agents: 10
                number_of_episodes: 10
                base_name: default_sac
            
            gym_env_settings: &env_settings
                LunarLanderContinuous-v2: 
                    reward_discount: 0.98
                    min_reward_single_timestep: -100
                    max_reward_single_timestep: 70
                    number_of_timesteps: 2000000
                    # horizons: # <- from those settings
                    #     0.1: 26  # epslion of 0.1 is really big. Causes major drop in rewards 
                    #     0.01: 31 # <- out of order?
                    #     0.001: 21
                    # horizons: # <- what was in the codebase I had
                    #     0.001: 5
                    #     0.0025: 5
                    #     0.005: 10
                    #     0.0075: 15
                    #     0.01: 20
                    horizons:  # <- what I think makes the most sense
                        0.0: 1
                        # 0.001: 5
                        # 0.0025: 8
                        0.005: 10
                        # 0.0075: 15
                        0.01: 20
                
                HopperBulletEnv-v0: 
                    reward_discount: 0.98
                    min_reward_single_timestep: 18
                    max_reward_single_timestep: 110
                    horizons:
                        0.1: 26
                        0.01: 16
                        0.001: 11
                
                HalfCheetahBulletEnv-v0: 
                    reward_discount: 0.98
                    min_reward_single_timestep: -60
                    max_reward_single_timestep: 100
                    number_of_timesteps: 1000000
                    horizons:
                        0: 1
                        0.001: 11
                        0.01: 13
                        0.1: 26
                    hparams: 
                        learning_rate: 7.3e-4
                        buffer_size: 300000
                        batch_size: 256
                        ent_coef: auto
                        gamma: 0.98
                        tau: 0.02
                        train_freq: 8
                        gradient_steps: 8
                        learning_starts: 10000
                
                BipedalWalker-v3: 
                    reward_discount: 0.98
                    min_reward_single_timestep: -23
                    max_reward_single_timestep: 5
                    horizons:
                        0.1: 26
                        0.01: 16
                        0.001: 11
                
                AntBulletEnv-v0: 
                    reward_discount: 0.98
                    min_reward_single_timestep: 19
                    max_reward_single_timestep: 105
                    horizons:
                        0: 1
                        0.0007: 4
                        0.0015: 16
                        0.0020: 26
                    # Max Episode Reward: 852.4950843086151
                    # Min Episode Reward: 153.87762677698575
                    # Max Timestep Reward: 2.3122164298644683
                    # Min Timestep Reward: -1.7494330256972288
                
                HumanoidBulletEnv-v0:
                    reward_discount: 0.98
                    min_reward_single_timestep: -30
                    max_reward_single_timestep: 113
                    number_of_timesteps: 3000000
                    horizons:
                        0.1: 26
                        0.01: 11
                        0.001: 5
                    hparams: 
                        learning_rate: 3e-4
                        buffer_size: 1000000
                        batch_size: 64
                        ent_coef: auto
                        train_freq: 1
                        gradient_steps: 1
                        learning_starts: 1000
                
                # Available: 
                    # Reacher-v2
                    # Pusher-v2
                    # Thrower-v2
                    # Striker-v2
                    # InvertedPendulum-v2
                    # InvertedDoublePendulum-v2
                    # HalfCheetah-v2
                    # HalfCheetah-v3
                    # Hopper-v2
                    # Hopper-v3
                    # Swimmer-v2
                    # Swimmer-v3
                    # Walker2d-v2
                    # Walker2d-v3
                    # Ant-v2
                    # Ant-v3
                    # Humanoid-v2
                    # Humanoid-v3
                    # HumanoidStandup-v2
            train_agent:
                model_name: default_sac_4 # was the best (by far) out of 10
                iterations: 200000
                force_retrain: false
                env_overrides:
                    *env_settings
                    
            train_coach:
                force_retrain: false
                learning_rate: 0.0001
                hidden_sizes: [64, 64, 64, 64]
                loss_function: state_prediction_loss # "value_prediction_loss", "action_prediction_loss", "state_prediction_loss", "consistent_coach_loss" <- each is a method on class Coach
                consistent_coach_loss:
                    scale_future_state_loss: 1000000
                delayed_coach_loss:
                    epochs_of_delay: 50
                value_plus_state_loss:
                    value_proportion: 0.5
                number_of_episodes: 100
                number_of_epochs: 100 # should be 100
                train_test_split: 0.7
                batch_size: 256
                with_card: true
                env_overrides:
                    *env_settings
    
            test_predictor:
                force_recompute: false
                graph_smoothing: 3 # rolling average of _ number values
                number_of_episodes: 10 # original value is 50 
                reward_discount: 0.98
                horizons:
                    0.001: 5
                    0.0025: 5
                    0.005: 10
                    0.0075: 15
                    0.01: 20
                env_overrides:
                    *env_settings
                
        
        FORCE_CPU:
            force_cpu: true
        
        ENV=LUNAR:
            env_name: "LunarLanderContinuous-v2"
            load:
                agent_path: "./subrepos/rl-trained-agents/sac/LunarLanderContinuous-v2_1/LunarLanderContinuous-v2.zip"
        
        ENV=ANT:
            env_name: "AntBulletEnv-v0"
        ENV=CHEETA:
            env_name: "HalfCheetahBulletEnv-v0"
        
        delayed_coach_loss_1:
            experiment_name: "delayed_coach_loss_1"
            
            train_coach:
                force_retrain: false
                loss_function: "delayed_coach_loss"
                consistent_coach_loss:
                    scale_future_state_loss: 1
                number_of_epochs: 120
                batch_size: 256
                
            test_predictor:
                force_recompute: true
        
        LOSS=Q:
            # python ./main/run/full.py -- @ENV=LUNAR @LOSS=Q experiment_name:q_loss_1
            train_coach:
                loss_function: "value_prediction_loss"
                force_retrain: false
                consistent_coach_loss:
                    scale_future_state_loss: 1
                number_of_epochs: 120
                batch_size: 256
            test_predictor:
                force_recompute: false
        LOSS=Q_STATE:
            # python ./main/run/full.py -- @ENV=LUNAR @LOSS=Q_STATE experiment_name:q_plus_state_loss_1
            train_coach:
                loss_function: "value_plus_state_loss"
                force_retrain: false
                consistent_coach_loss:
                    scale_future_state_loss: 1
                number_of_epochs: 120
                batch_size: 256
        LOSS=Q_STATE_FAVORED:
            # python ./main/run/full.py -- @ENV=LUNAR @LOSS=Q_STATE_FAVORED experiment_name:q_favored_plus_state_loss_1
            train_coach:
                loss_function: "value_plus_state_loss"
                force_retrain: false
                consistent_coach_loss:
                    scale_future_state_loss: 1
                value_plus_state_loss:
                    value_proportion: 0.01
                number_of_epochs: 120
                batch_size: 256