(project):
    Q&A:
        - Whats the main idea(s)?:
            - there are two models; the agent and the coach ("dynamics" in the code)
            - the agent is a SAC model that is pretty typical
            - the coach tries to forcast multiple steps into the future in a way that helps the model
        - How do I judge performance?:
            - Smaller epsilon (lieniency of predicted_state ~~== actual_future_state)
            - Larger forcast (the avergage length of predictions that were correct)
            - Rewards that are most similar to the optimial policy (green line in .ipynb graph)
        - What is the main loss function?: |
            # given agent, coach, state, action, next_state
            
            predicted_next_state  = coach.predict_state(state, action)
            predicted_next_action = agent.choose_action(predicted_next_state)
            predicted_next_value  = agent.value_estimate(next_state, predicted_next_action)
            
            best_next_action = agent.choose_action(next_state)
            best_next_value = agent.value_estimate(next_state, best_next_action)
            
            coach.loss = best_next_value - predicted_next_value # when predicted_next_value is high, loss is low (negative)
            
            # which in theory, when differentiated simplifies to
            - predicted_next_value
            # TODO: I'm not yet convinced the simplification works.
            # => Yes best_next_value is constant for a single update step of the coach
            #    but with best_next_value changing and a replay buffer 
            #    I'm inclined to think it would be safer to compute/store the difference
            #    particularly for plotting the losses on a graph to see how 
            #    bad they are, while they're being measured against the Q val
            #    I don't think there's any computational savings as we should already have best_next_value
            
    tasks/questions:
        - read the paper
        - ask about env_settings for the 5 non-lunar lander env's
        - rerun everything and evaluate new loss
        - check: difference between testing/training loss
        - check: maybe use torch.no_grad for parts of the loss (when running data through agent)

    # a central place for filepaths
    (path_to):
        folder:
            agent_models: "./data/models/agents/"
            dynamics_models: "./data/models/dynamics/"
            results: "./data/results"
        file:
            humanoid_agent_model: "./log/best_model.zip"

    (local_data): ./local_data.ignore.yaml
    (profiles):
        (default):
            mode: development
            force_cpu: false
            env_names: ["LunarLanderContinuous-v2", "MountainCarContinuous-v0"]
            gamma: 0.98 # QUESTION: discount factor? its used in the .ipynb, but I don't know what its for exactly
            train_agent:
                iterations: 100000
            train_dynamics:
                number_of_episodes: 20
                number_of_epochs: 100
                train_test_split: 0.7
                minibatch_size: 32
            train_afrl:
                number_of_experiments: 50
                env_settings:
                    LunarLanderContinuous-v2:
                        max_score: 70  # discounted ep reward
                        min_score: -100
                        horizons: # maps "epsilon coefficients" to horizons
                            0.001: 5
                            0.0025: 5
                            0.005: 10
                            0.0075: 15
                            0.01: 20
                    MountainCarContinuous-v0: # FIXME: this is just a copy of LunarLanderContinuous-v2 to avoid the code throwing errors
                        max_score: 70
                        min_score: -100
                        horizons:
                            0.001: 5
                            0.0025: 5
                            0.005: 10
                            0.0075: 15
                            0.01: 20
                    AntBulletEnv-v0: # FIXME: this is just a copy of LunarLanderContinuous-v2 to avoid the code throwing errors
                        max_score: 70
                        min_score: -100
                        horizons:
                            0.001: 5
                            0.0025: 5
                            0.005: 10
                            0.0075: 15
                            0.01: 20
                    HopperBulletEnv-v0: # FIXME: this is just a copy of LunarLanderContinuous-v2 to avoid the code throwing errors
                        max_score: 70
                        min_score: -100
                        horizons:
                            0.001: 5
                            0.0025: 5
                            0.005: 10
                            0.0075: 15
                            0.01: 20
                    BipedalWalker-v3: # FIXME: this is just a copy of LunarLanderContinuous-v2 to avoid the code throwing errors
                        max_score: 70
                        min_score: -100
                        horizons:
                            0.001: 5
                            0.0025: 5
                            0.005: 10
                            0.0075: 15
                            0.01: 20
                    HalfCheetahBulletEnv-v0: # FIXME: this is just a copy of LunarLanderContinuous-v2 to avoid the code throwing errors
                        max_score: 70
                        min_score: -100
                        horizons:
                            0.001: 5
                            0.0025: 5
                            0.005: 10
                            0.0075: 15
                            0.01: 20
        
        FORCE_CPU:
            force_cpu: true
        
        ENVS=BASIC:
            env_names: [ "MountainCarContinuous-v0", "LunarLanderContinuous-v2" ]
        ENVS=HARD:
            env_names: [ "AntBulletEnv-v0", "HopperBulletEnv-v0", BipedalWalker-v3, "HalfCheetahBulletEnv-v0" ]
        ENVS=FULL:
            env_names: [ "MountainCarContinuous-v0", "LunarLanderContinuous-v2", "AntBulletEnv-v0", "HopperBulletEnv-v0", BipedalWalker-v3, "HalfCheetahBulletEnv-v0" ]
            # inside the .ipynb:
                # Lander
                # Humanoid
                # Hopper
                # Cheetah
                # Ant
                # Walker
            # Available: 
                # Reacher-v2
                # Pusher-v2
                # Thrower-v2
                # Striker-v2
                # InvertedPendulum-v2
                # InvertedDoublePendulum-v2
                # HalfCheetah-v2
                # HalfCheetah-v3
                # Hopper-v2
                # Hopper-v3
                # Swimmer-v2
                # Swimmer-v3
                # Walker2d-v2
                # Walker2d-v3
                # Ant-v2
                # Ant-v3
                # Humanoid-v2
                # Humanoid-v3
                # HumanoidStandup-v2
        