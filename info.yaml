(project):

    # a central place for filepaths
    (path_to):
        folder:
            agent_models: "./data/models/agents/"
            dynamics_models: "./data/models/dynamics/"
            results: "./data/results"
        file:
            humanoid_agent_model: "./log/best_model.zip"

    (local_data): ./local_data.ignore.yaml
    (profiles):
        (default):
            mode: development
            force_cpu: false
            default_env_names: ["LunarLanderContinuous-v2", "MountainCarContinuous-v0"] # "AntBulletEnv-v0", "LunarLanderContinuous-v2", BipedalWalker-v3, "HalfCheetahBulletEnv-v0"
            train_agent:
                iterations: 100000
            train_dynamics:
                number_of_episodes: 20
                number_of_epochs: 100
                train_test_split: 0.7
                minibatch_size: 32
            train_afrl:
                number_of_experiments: 50
                env_settings:
                    LunarLanderContinuous-v2:
                        max_score: 70  # discounted ep reward
                        min_score: -100
                        horizons: # maps "epsilon coefficients" to horizons
                            0.001: 5
                            0.0025: 5
                            0.005: 10
                            0.0075: 15
                            0.01: 20
        ENVS=BASIC:
            default_env_names: ["LunarLanderContinuous-v2", "MountainCarContinuous-v0"]
        ENVS=HARD:
            default_env_names: [ "AntBulletEnv-v0", "HopperBulletEnv-v0", BipedalWalker-v3, "HalfCheetahBulletEnv-v0" ]
        ENVS=FULL:
            default_env_names: [ "LunarLanderContinuous-v2", "MountainCarContinuous-v0", "AntBulletEnv-v0", "HopperBulletEnv-v0", BipedalWalker-v3, "HalfCheetahBulletEnv-v0" ]
            # HopperBulletEnv
            # HalfCheetahBulletEnv
            # Walker2DBulletEnv
            # AntBulletEnv
        
        FORCE_CPU:
            force_cpu: true
        