(project):
    Q&A:
        - Whats the main idea(s)?:
            - there are two models; the agent and the coach
            - the agent is a SAC model that is pretty typical
            - the coach tries to forcast multiple steps into the future in a way that helps the agent
        - How do I judge performance?:
            - Smaller epsilon (lieniency of predicted_state ~~== actual_future_state)
            - Larger forcast (the avergage length of predictions that were correct)
            - Rewards that are most similar to the optimial policy (green line in .ipynb graph)
        - What is the main loss function?: |
            there's a few, and they're defined in main/training/train_coach.py on the CoachClass class
            
    tasks/questions:
        - problem: epsilon is highly effected by max/min possible score. I'm unsure if min/max is supposed to be on a per-episode or per-timestep basis
          discusson: |
            because of spiking high values, this is not a good scaling metric.
            Out of 99 values the min is -314, out of 100 its -701.2.
            I know makes the math complicated but I think the only way to avoid 2 hyperparameters would be to have a multiplier
        - solve: why is epsilon gigantic when printed
        - solve: why slice [horizon:] instead of [:horizon]
        - task: set env_settings be for the 5 non-lunar lander env's
        - question: why set deterministic=True, and then sample from normal curve (coach experience function)
          answer: because of robustness and exploring states (good agents have a narrow path that doesnt sample the state space very well)
        - todo: read the paper more
        - question: whats the difference between the different Q functions

    # a central place for filepaths
    (path_to):
        folder:
            agent_models: "./data/models/agents/"
            coach_models: "./data/models/coach/"
            results: "./data/results/"
            visuals: "./visuals/"

    (local_data): ./local_data.ignore.yaml
    (profiles):
        (default):
            experiment_name: null
            env_name: "LunarLanderContinuous-v2" # can be any of: "LunarLanderContinuous-v2", "MountainCarContinuous-v0", "AntBulletEnv-v0", "HopperBulletEnv-v0", BipedalWalker-v3, "HalfCheetahBulletEnv-v0"
            force_cpu: false
            mode: development
            load:
                agent_path: null # there is a default path, and this overrides the default path (designed to be used from CLI)
                coach_path: null # there is a default path, and this overrides the default path (designed to be used from CLI)
            
            agent_compare_test:
                number_of_agents: 10
                number_of_episodes: 10
                base_name: default_sac
            
            train_agent:
                model_name: default_sac_4 # was the best (by far) out of 10
                iterations: 100000
                force_retrain: false
            
            train_coach:
                force_retrain: false
                learning_rate: 0.0001
                hidden_sizes: [64, 64, 64, 64]
                loss_api: batched  # "timestep" or "batched" # loss functions are exclusively compatible with one or the other (this should probably be handled internally)
                loss_function: state_prediction_loss # "value_prediction_loss", "action_prediction_loss", "state_prediction_loss", "consistent_coach_loss" <- each is a method on class Coach
                consistent_coach_loss:
                    scale_value_prediction: 1.0
                number_of_episodes: 20
                number_of_epochs: 10 # should be 100
                train_test_split: 0.7
                minibatch_size: 32
                with_card: True
                env_overrides:
                    LunarLanderContinuous-v2:
                        # this is an example
                        hidden_sizes: [64, 64, 64, 64]
                        learning_rate: 0.0001
            test_predictor:
                number_of_episodes: 50
                agent_discount_factor: 0.98
                horizons:
                    0.001: 5
                    0.0025: 5
                    0.005: 10
                    0.0075: 15
                    0.01: 20
                env_overrides:
                    LunarLanderContinuous-v2:
                        number_of_episodes: 50
                        agent_discount_factor: 0.98
                        max_reward_single_timestep: 100
                        min_reward_single_timestep: -100
                        # max_reward_single_episode: 350  # 304.0547650001378 is largest I've seen
                        # min_reward_single_episode: -1500 # -1210.429 is worst I've seen
                
            gym_env_settings:
                LunarLanderContinuous-v2:
                    agent_discount_factor: 0.98
                    actor_copy_learning_rate: 0.0001
                    max_reward_single_timestep: 100  # "discounted ep reward" was the comment, but I don't think thats right
                    min_reward_single_timestep: -100
                    horizons: # acceptability threshold (epsilon) => forecast length (horizon)
                        0.001: 5
                        0.0025: 5
                        0.005: 10
                        0.0075: 15
                        0.01: 20
                MountainCarContinuous-v0: # FIXME: this is just a copy of LunarLanderContinuous-v2 to avoid the code throwing errors
                    agent_discount_factor: 0.98
                    actor_copy_learning_rate: 0.0001
                    max_reward_single_timestep: 100
                    min_reward_single_timestep: -100
                    horizons:
                        0.001: 5
                        0.0025: 5
                        0.005: 10
                        0.0075: 15
                        0.01: 20
                AntBulletEnv-v0: # FIXME: this is just a copy of LunarLanderContinuous-v2 to avoid the code throwing errors
                    agent_discount_factor: 0.98
                    actor_copy_learning_rate: 0.0001
                    max_reward_single_timestep: 100
                    min_reward_single_timestep: -100
                    horizons:
                        0.001: 5
                        0.0025: 5
                        0.005: 10
                        0.0075: 15
                        0.01: 20
                HopperBulletEnv-v0: # FIXME: this is just a copy of LunarLanderContinuous-v2 to avoid the code throwing errors
                    agent_discount_factor: 0.98
                    actor_copy_learning_rate: 0.0001
                    max_reward_single_timestep: 100
                    min_reward_single_timestep: -100
                    horizons:
                        0.001: 5
                        0.0025: 5
                        0.005: 10
                        0.0075: 15
                        0.01: 20
                BipedalWalker-v3: # FIXME: this is just a copy of LunarLanderContinuous-v2 to avoid the code throwing errors
                    agent_discount_factor: 0.98
                    actor_copy_learning_rate: 0.0001
                    max_reward_single_timestep: 100
                    min_reward_single_timestep: -100
                    horizons:
                        0.001: 5
                        0.0025: 5
                        0.005: 10
                        0.0075: 15
                        0.01: 20
                HalfCheetahBulletEnv-v0: # FIXME: this is just a copy of LunarLanderContinuous-v2 to avoid the code throwing errors
                    agent_discount_factor: 0.98
                    actor_copy_learning_rate: 0.0001
                    max_reward_single_timestep: 100
                    min_reward_single_timestep: -100
                    horizons:
                        0.001: 5
                        0.0025: 5
                        0.005: 10
                        0.0075: 15
                        0.01: 20
                # Available: 
                    # Reacher-v2
                    # Pusher-v2
                    # Thrower-v2
                    # Striker-v2
                    # InvertedPendulum-v2
                    # InvertedDoublePendulum-v2
                    # HalfCheetah-v2
                    # HalfCheetah-v3
                    # Hopper-v2
                    # Hopper-v3
                    # Swimmer-v2
                    # Swimmer-v3
                    # Walker2d-v2
                    # Walker2d-v3
                    # Ant-v2
                    # Ant-v3
                    # Humanoid-v2
                    # Humanoid-v3
                    # HumanoidStandup-v2
        
        FORCE_CPU:
            force_cpu: true
        
        COACH_LOSS:
            train_coach:
                loss_api: timestep
                loss_function: consistent_coach_loss 