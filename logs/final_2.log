

-------------------------------------------------------

 Environment: ReacherBulletEnv-v0

-------------------------------------------------------




-----------------------------------------------------------------------------------------------------
 Agent Model Exists, loading: ./subrepos/rl-trained-agents/sac/ReacherBulletEnv-v0_1/ReacherBulletEnv-v0.zip
-----------------------------------------------------------------------------------------------------


pybullet build time: Oct 11 2021 20:52:37
Wrapping the env with a `Monitor` wrapper
Wrapping the env in a DummyVecEnv.


-----------------------------------------------------------------------------------------------------
 Coach Model Exists, loading: models.ignore/coach/ReacherBulletEnv-v0/baseline_3
-----------------------------------------------------------------------------------------------------


coach settings = {
    "force_retrain": False, 
    "learning_rate": 0.0001, 
    "hidden_sizes": [64, 64, 64, 64, ], 
    "loss_function": "state_prediction_loss", 
    "consistent_coach_loss": {"scale_future_state_loss": 1000000, }, 
    "delayed_coach_loss": {"epochs_of_delay": 50, }, 
    "value_plus_state_loss": {"value_proportion": 0.5, }, 
    "number_of_episodes": 100, 
    "number_of_epochs": 100, 
    "train_test_split": 0.7, 
    "batch_size": 128, 
    "with_card": True, 
}


-----------------------------------------------------------------------------------------------------
 Testing Agent+Coach
-----------------------------------------------------------------------------------------------------


test settings = {
    "override_save_path": False, 
    "increment_factor": 1.2, 
    "force_recompute": False, 
    "graph_smoothing": 0, 
    "number_of_episodes_for_baseline": 30, 
    "number_of_epochs_for_optimal_parameters": 50, 
    "number_of_episodes_for_testing": 50, 
    "initial_epsilon": 3.5, 
    "initial_horizon": 10, 
    "reward_discount": 0.98, 
    "acceptable_performance_levels": [0.5, 0.8, 0.9, 0.95, 0.97, 0.99, 1.0, ], 
    "acceptable_performance_loss": 1, 
    "acceptable_performance_level": 0.8, 
    "confidence_interval_for_convergence": 90, 
    "min_reward_single_timestep": 0, 
    "max_reward_single_timestep": 100, 
}
no data found for: results/final_2/ReacherBulletEnv-v0//experiments.csv
no data found for: results/final_2/ReacherBulletEnv-v0//experiments.csv
    running optimal method
    running random method
    # 
    # acceptable_performance_level = 0.5
    # 
    running ppac method
argv[0]=
argv[0]=
/home/jeffhykin/repos/AFRL/.venv/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(
Traceback (most recent call last):
  File "./main/run/compare_methods.py", line 50, in <module>
    full_run(
  File "./main/run/compare_methods.py", line 26, in full_run
    results = Tester.smart_load(
  File "/home/jeffhykin/repos/AFRL/main/test_prediction.py", line 630, in create_comparisons
    ) = self.ppac_experience_episode(
  File "/home/jeffhykin/repos/AFRL/main/test_prediction.py", line 309, in ppac_experience_episode
    self.rewards_per_episode_per_timestep[episode_index]             = rewards_per_timestep            
IndexError: list assignment index out of range


-------------------------------------------------------

 Environment: AntBulletEnv-v0

-------------------------------------------------------




-----------------------------------------------------------------------------------------------------
 Agent Model Exists, loading: subrepos/rl-trained-agents/sac/AntBulletEnv-v0_1/AntBulletEnv-v0.zip
-----------------------------------------------------------------------------------------------------


pybullet build time: Oct 11 2021 20:52:37
Wrapping the env with a `Monitor` wrapper
Wrapping the env in a DummyVecEnv.


-----------------------------------------------------------------------------------------------------
 Coach Model Exists, loading: models.ignore/coach/AntBulletEnv-v0/baseline_3
-----------------------------------------------------------------------------------------------------


coach settings = {
    "force_retrain": False, 
    "learning_rate": 0.0001, 
    "hidden_sizes": [64, 64, 64, 64, ], 
    "loss_function": "state_prediction_loss", 
    "consistent_coach_loss": {"scale_future_state_loss": 1000000, }, 
    "delayed_coach_loss": {"epochs_of_delay": 50, }, 
    "value_plus_state_loss": {"value_proportion": 0.5, }, 
    "number_of_episodes": 100, 
    "number_of_epochs": 100, 
    "train_test_split": 0.7, 
    "batch_size": 128, 
    "with_card": True, 
}


-----------------------------------------------------------------------------------------------------
 Testing Agent+Coach
-----------------------------------------------------------------------------------------------------


test settings = {
    "override_save_path": False, 
    "increment_factor": 1.2, 
    "force_recompute": False, 
    "graph_smoothing": 0, 
    "number_of_episodes_for_baseline": 30, 
    "number_of_epochs_for_optimal_parameters": 50, 
    "number_of_episodes_for_testing": 50, 
    "initial_epsilon": 3.5, 
    "initial_horizon": 10, 
    "reward_discount": 0.98, 
    "acceptable_performance_levels": [0.5, 0.8, 0.9, 0.95, 0.97, 0.99, 1.0, ], 
    "acceptable_performance_loss": 1, 
    "acceptable_performance_level": 0.8, 
    "confidence_interval_for_convergence": 90, 
    "min_reward_single_timestep": 19, 
    "max_reward_single_timestep": 105, 
    "horizons": {0: 1, 0.0007: 4, 0.0015: 16, 0.002: 26, }, 
}
no data found for: results/final_2/AntBulletEnv-v0//experiments.csv
no data found for: results/final_2/AntBulletEnv-v0//experiments.csv
    running optimal method
    running random method
    # 
    # acceptable_performance_level = 0.5
    # 
    running ppac method
/home/jeffhykin/repos/AFRL/.venv/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(
argv[0]=
argv[0]=
Traceback (most recent call last):
  File "./main/run/compare_methods.py", line 50, in <module>
    full_run(
  File "./main/run/compare_methods.py", line 26, in full_run
    results = Tester.smart_load(
  File "/home/jeffhykin/repos/AFRL/main/test_prediction.py", line 630, in create_comparisons
    ) = self.ppac_experience_episode(
  File "/home/jeffhykin/repos/AFRL/main/test_prediction.py", line 309, in ppac_experience_episode
    self.rewards_per_episode_per_timestep[episode_index]             = rewards_per_timestep            
IndexError: list assignment index out of range


-------------------------------------------------------

 Environment: BipedalWalker-v3

-------------------------------------------------------




-----------------------------------------------------------------------------------------------------
 Agent Model Exists, loading: ./subrepos/rl-trained-agents/sac/BipedalWalker-v3_1/BipedalWalker-v3.zip
-----------------------------------------------------------------------------------------------------


Wrapping the env with a `Monitor` wrapper
Wrapping the env in a DummyVecEnv.


-----------------------------------------------------------------------------------------------------
 Training Coach Model from scrach
-----------------------------------------------------------------------------------------------------


coach settings = {
    "force_retrain": False, 
    "learning_rate": 0.0001, 
    "hidden_sizes": [64, 64, 64, 64, ], 
    "loss_function": "state_prediction_loss", 
    "consistent_coach_loss": {"scale_future_state_loss": 1000000, }, 
    "delayed_coach_loss": {"epochs_of_delay": 50, }, 
    "value_plus_state_loss": {"value_proportion": 0.5, }, 
    "number_of_episodes": 100, 
    "number_of_epochs": 100, 
    "train_test_split": 0.7, 
    "batch_size": 128, 
    "with_card": True, 
}
Starting Train/Test
