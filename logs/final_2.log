

-------------------------------------------------------

 Environment: ReacherBulletEnv-v0

-------------------------------------------------------




-----------------------------------------------------------------------------------------------------
 Agent Model Exists, loading: ./subrepos/rl-trained-agents/sac/ReacherBulletEnv-v0_1/ReacherBulletEnv-v0.zip
-----------------------------------------------------------------------------------------------------


pybullet build time: Oct 11 2021 20:52:37
Wrapping the env with a `Monitor` wrapper
Wrapping the env in a DummyVecEnv.


-----------------------------------------------------------------------------------------------------
 Coach Model Exists, loading: models.ignore/coach/ReacherBulletEnv-v0/baseline_3
-----------------------------------------------------------------------------------------------------


coach settings = {
    "force_retrain": False, 
    "learning_rate": 0.0001, 
    "hidden_sizes": [64, 64, 64, 64, ], 
    "loss_function": "state_prediction_loss", 
    "consistent_coach_loss": {"scale_future_state_loss": 1000000, }, 
    "delayed_coach_loss": {"epochs_of_delay": 50, }, 
    "value_plus_state_loss": {"value_proportion": 0.5, }, 
    "number_of_episodes": 100, 
    "number_of_epochs": 100, 
    "train_test_split": 0.7, 
    "batch_size": 128, 
    "with_card": True, 
}


-----------------------------------------------------------------------------------------------------
 Testing Agent+Coach
-----------------------------------------------------------------------------------------------------


test settings = {
    "override_save_path": False, 
    "increment_factor": 1.2, 
    "force_recompute": False, 
    "graph_smoothing": 0, 
    "number_of_episodes_for_baseline": 30, 
    "number_of_epochs_for_optimal_parameters": 15, 
    "number_of_episodes_for_testing": 10, 
    "initial_epsilon": 3.5, 
    "initial_horizon": 10, 
    "reward_discount": 0.98, 
    "acceptable_performance_levels": [0.5, 0.8, 0.9, 0.95, 0.97, 0.99, 1.0, ], 
    "acceptable_performance_loss": 1, 
    "acceptable_performance_level": 0.8, 
    "confidence_interval_for_convergence": 90, 
    "min_reward_single_timestep": 0, 
    "max_reward_single_timestep": 100, 
}
no data found for: results/final_2/ReacherBulletEnv-v0//experiments.csv
no data found for: results/final_2/ReacherBulletEnv-v0//experiments.csv
    running optimal method
    running random method
    # 
    # acceptable_performance_level = 0.5
    # 
    running ppac method
        baseline = {
    "max": 27.408153307600873, 
    "min": 4.0474202858457, 
    "range": 23.360733021755173, 
    "count": 30, 
    "sum": 454.13727095447496, 
    "average": 15.137909031815832, 
    "stdev": 5.9778181034421936, 
    "median": 16.38933864084101, 
}
        baseline_min = 13.283489826310669, baseline_max = 16.99232823732099,
        baseline_confidence_size = 1.8544192055051605
        ----- finding optimal epsilon -------------------------------------------------------------------------------------------------------------------------------------
argv[0]=
argv[0]=
                reward_single_sum=10.233159659368416, 
                reward_single_sum=-14.396144738820155,     confidence_size=77.75165397627822, confidence_max=75.6701614365523, new_horizon=2
                reward_single_sum=4.096936257321669,     confidence_size=21.6141392046305, confidence_max=21.592122930587138, new_horizon=2
                reward_single_sum=-6.3758876477626645,     confidence_size=12.872502413195829, confidence_max=11.262018295722642, new_horizon=2
            episode=0, horizon=20, effective_score=-1.61, baseline_lowerbound=7.57 baseline_stdev=1.09, new_epsilon=2.9167, bad=True, gap_average=0.4330462498962879
                reward_single_sum=-7.240661089023011, 
                reward_single_sum=-6.047949566150075,     confidence_size=3.765242092129766, confidence_max=-2.8790632354567802, new_horizon=22
            episode=1, horizon=20, effective_score=-6.64, baseline_lowerbound=7.57 baseline_stdev=1.09, new_epsilon=2.4306, bad=True, gap_average=0.5363671506444613
                reward_single_sum=0.33314742664618274, 
                reward_single_sum=-1.3582846362460521,     confidence_size=5.33964087463436, confidence_max=4.827072269834423, new_horizon=11
            episode=2, horizon=20, effective_score=-0.51, baseline_lowerbound=7.57 baseline_stdev=1.09, new_epsilon=2.0255, bad=True, gap_average=0.3994487979014715
                reward_single_sum=2.807916707291994, 
                reward_single_sum=-5.190068518376966,     confidence_size=25.24864566696145, confidence_max=24.05756976141895, new_horizon=15
                reward_single_sum=4.138437917117207,     confidence_size=8.506442697940365, confidence_max=9.091871399951106, new_horizon=10
            episode=3, horizon=20, effective_score=0.59, baseline_lowerbound=7.57 baseline_stdev=1.09, new_epsilon=1.6879, bad=True, gap_average=0.5217423600620693
                reward_single_sum=-11.429035951360063, 
                reward_single_sum=0.9895160268848093,     confidence_size=39.20382568213886, confidence_max=33.98406571990122, new_horizon=25
                reward_single_sum=0.3335427206950916,     confidence_size=11.781075192382966, confidence_max=8.412416124456241, new_horizon=28
            episode=4, horizon=20, effective_score=-3.37, baseline_lowerbound=7.57 baseline_stdev=1.09, new_epsilon=1.4066, bad=True, gap_average=0.42800547109709847
                reward_single_sum=1.34478412407671, 
                reward_single_sum=14.64149183941178,     confidence_size=41.97605423978104, confidence_max=49.96919222152527, new_horizon=20
                reward_single_sum=8.045304085365197,     confidence_size=11.20827201656913, confidence_max=19.218798699520356, new_horizon=20
                reward_single_sum=1.5418637166969973,     confidence_size=7.435341584800897, confidence_max=13.828702526188566, new_horizon=16
                reward_single_sum=0.6963598327420725,     confidence_size=5.755006957910832, confidence_max=11.008967677569382, new_horizon=11
            episode=5, horizon=20, effective_score=5.25, baseline_lowerbound=7.57 baseline_stdev=1.09, new_epsilon=1.1721, bad=True, gap_average=0.4500768587986628
                reward_single_sum=10.106312327049004, 
                reward_single_sum=15.475471897479727,     confidence_size=16.949769685507455, confidence_max=29.740661797771814, new_horizon=4
                reward_single_sum=8.854902425487161,     confidence_size=5.929553867625652, confidence_max=17.408449417630948, new_horizon=7
                reward_single_sum=-9.54181115150556,     confidence_size=12.820693489465178, confidence_max=19.044412364092757, new_horizon=7
                reward_single_sum=13.100714467356799,     confidence_size=9.461881802322722, confidence_max=17.060999795496144, new_horizon=9
                reward_single_sum=9.08538262350731,     confidence_size=7.319368423585514, confidence_max=15.166197188481252, new_horizon=9
                reward_single_sum=-6.701575988676589,     confidence_size=7.2038787661257295, confidence_max=12.97236399479685, new_horizon=8
            episode=6, horizon=20, effective_score=5.77, baseline_lowerbound=7.57 baseline_stdev=1.09, new_epsilon=0.9768, bad=True, gap_average=0.3974727791547775
                reward_single_sum=15.088189836116504, 
                reward_single_sum=7.411308636255276,     confidence_size=24.23496015228533, confidence_max=35.4847093884712, new_horizon=6
                reward_single_sum=6.373373866092184,     confidence_size=8.025087238152112, confidence_max=17.649378017640096, new_horizon=6
                reward_single_sum=8.750952723760907,     confidence_size=4.602213808302567, confidence_max=14.008170073858782, new_horizon=10
                reward_single_sum=3.6746526472238057,     confidence_size=4.049674463809474, confidence_max=12.309370005699208, new_horizon=9
            episode=7, horizon=8, effective_score=8.26, baseline_lowerbound=7.57 baseline_stdev=1.09, new_epsilon=1.1721, bad=False, gap_average=0.3404440216223399
                reward_single_sum=-7.690814916399959, 
                reward_single_sum=1.2922416909957102,     confidence_size=28.35839363124348, confidence_max=25.15910701854134, new_horizon=10
                reward_single_sum=3.641389533895512,     confidence_size=10.0830601394181, confidence_max=9.163998908915184, new_horizon=9
            episode=8, horizon=9, effective_score=-0.92, baseline_lowerbound=7.57 baseline_stdev=1.09, new_epsilon=0.9768, bad=True, gap_average=0.40364502204789054
                reward_single_sum=14.270878001109985, 
                reward_single_sum=8.16801797530868,     confidence_size=19.265970866260528, confidence_max=30.48541885446985, new_horizon=8
                reward_single_sum=12.236488030543402,     confidence_size=5.238645991874653, confidence_max=16.797107327528675, new_horizon=8
                reward_single_sum=6.707609490032606,     confidence_size=4.130141836026185, confidence_max=14.475890210274851, new_horizon=10
                reward_single_sum=16.455752664446205,     confidence_size=3.8968452134219955, confidence_max=15.46459444571017, new_horizon=8
                reward_single_sum=-9.269601564997659,     confidence_size=7.616910824304513, confidence_max=15.711768257045048, new_horizon=10
                reward_single_sum=1.727159961143214,     confidence_size=6.454628455362701, confidence_max=13.639814820732191, new_horizon=8
                reward_single_sum=12.421422611396672,     confidence_size=5.5893572818456025, confidence_max=13.42907317796849, new_horizon=8
                reward_single_sum=-9.694522065068359,     confidence_size=6.04428424633643, confidence_max=11.935751480104734, new_horizon=8
            episode=9, horizon=20, effective_score=5.89, baseline_lowerbound=7.57 baseline_stdev=1.09, new_epsilon=0.8140, bad=True, gap_average=0.43810053492033924
                reward_single_sum=4.165377402082538, 
                reward_single_sum=5.3724573277880205,     confidence_size=3.810601354704395, confidence_max=8.579518719639672, new_horizon=4
            episode=10, horizon=20, effective_score=4.77, baseline_lowerbound=7.57 baseline_stdev=1.09, new_epsilon=0.6783, bad=True, gap_average=0.4466364764173826
                reward_single_sum=4.630601989264196, 
                reward_single_sum=22.34886417122677,     confidence_size=55.934352345503164, confidence_max=69.42408542574861, new_horizon=12
                reward_single_sum=5.6619400535966395,     confidence_size=16.76632776091135, confidence_max=27.646796498940546, new_horizon=15
                reward_single_sum=13.551326517028512,     confidence_size=9.68336087771118, confidence_max=21.231544060490208, new_horizon=14
                reward_single_sum=-0.9143031303114235,     confidence_size=8.625670908367516, confidence_max=17.68135682852845, new_horizon=14
                reward_single_sum=1.9536009918212396,     confidence_size=7.0713742925416865, confidence_max=14.943379391312673, new_horizon=12
                reward_single_sum=3.735262597920073,     confidence_size=5.876542247506643, confidence_max=13.157584131870358, new_horizon=10
            episode=11, horizon=20, effective_score=7.28, baseline_lowerbound=7.57 baseline_stdev=1.09, new_epsilon=0.5653, bad=True, gap_average=0.2995651659795216
                reward_single_sum=5.644926046423021, 
                reward_single_sum=10.876958252352026,     confidence_size=16.516875632835767, confidence_max=24.77781778222328, new_horizon=6
                reward_single_sum=0.7730050214236472,     confidence_size=8.51870022620874, confidence_max=14.283663332941634, new_horizon=6
                reward_single_sum=-1.2912477503899407,     confidence_size=6.387735767543908, confidence_max=10.388646159996094, new_horizon=8
            episode=12, horizon=20, effective_score=4.00, baseline_lowerbound=7.57 baseline_stdev=1.09, new_epsilon=0.4711, bad=True, gap_average=0.23290326133370398
                reward_single_sum=6.574240306923615, 
                reward_single_sum=20.03032989769265,     confidence_size=42.47920301850755, confidence_max=55.78148812081566, new_horizon=6
                reward_single_sum=15.5416212332916,     confidence_size=11.550026108533833, confidence_max=25.59875658783645, new_horizon=8
                reward_single_sum=4.053142245120196,     confidence_size=8.826684155992051, confidence_max=20.376517576749062, new_horizon=8
                reward_single_sum=5.419636466077283,     confidence_size=6.722479721157724, confidence_max=17.04627375097879, new_horizon=8
                reward_single_sum=-3.240549655222255,     confidence_size=6.904295306913062, confidence_max=14.96736538922691, new_horizon=8
                reward_single_sum=13.158276243217589,     confidence_size=5.802118556663203, confidence_max=14.593075233391872, new_horizon=8
                reward_single_sum=-2.840180736228111,     confidence_size=5.620371304162246, confidence_max=12.957435804271316, new_horizon=8
            episode=13, horizon=20, effective_score=7.34, baseline_lowerbound=7.57 baseline_stdev=1.09, new_epsilon=0.3925, bad=True, gap_average=0.20557284964869418
                reward_single_sum=12.70924195557897, 
                reward_single_sum=14.231985439805849,     confidence_size=4.807111990095358, confidence_max=18.277725687787765, new_horizon=8
                reward_single_sum=22.046187985685695,     confidence_size=8.44496621427572, confidence_max=24.774104674632554, new_horizon=8
                reward_single_sum=6.909183178596193,     confidence_size=7.340143473936116, confidence_max=21.31429311385279, new_horizon=6
                reward_single_sum=18.864277089469596,     confidence_size=5.556492131511339, confidence_max=20.508667261338598, new_horizon=8
                reward_single_sum=7.386803348290992,     confidence_size=4.984471198137798, confidence_max=18.675751031042346, new_horizon=8
                reward_single_sum=9.231424631689896,     confidence_size=4.2468630097159705, confidence_max=17.301020671018424, new_horizon=8
                reward_single_sum=6.609692415605748,     confidence_size=3.8971732158956707, confidence_max=16.145772721486036, new_horizon=8
                reward_single_sum=12.836106239561559,     confidence_size=3.3756175290254147, confidence_max=15.68949556061258, new_horizon=8
                reward_single_sum=9.595174946355955,     confidence_size=3.0177590470004114, confidence_max=15.059766770064456, new_horizon=8
                reward_single_sum=8.09356708214623,     confidence_size=2.776218601010675, confidence_max=14.459277174900372, new_horizon=8
                reward_single_sum=-0.9843427898594153,     confidence_size=3.1463960788771432, confidence_max=13.773837872454415, new_horizon=8
                reward_single_sum=-2.37100244727656,     confidence_size=3.3802637538602776, confidence_max=13.007825221218024, new_horizon=8
            episode=14, horizon=8, effective_score=9.63, baseline_lowerbound=7.57 baseline_stdev=1.09, new_epsilon=0.4711, bad=False, gap_average=0.1917785282853322
        optimal_epsilon = 0.9767857653177872
        optimal_horizon = 8
    running theory method
    running n_step horizon method
    running n_step planlen method
    # 
    # acceptable_performance_level = 0.8
    # 
    running ppac method
    running theory method
    running n_step horizon method
    running n_step planlen method
    # 
    # acceptable_performance_level = 0.9
    # 
    running ppac method
    running theory method
    running n_step horizon method
    running n_step planlen method
    # 
    # acceptable_performance_level = 0.95
    # 
    running ppac method
    running theory method
    running n_step horizon method
    running n_step planlen method
    # 
    # acceptable_performance_level = 0.97
    # 
    running ppac method
        baseline = {
    "max": 27.408153307600873, 
    "min": 4.0474202858457, 
    "range": 23.360733021755173, 
    "count": 30, 
    "sum": 454.13727095447496, 
    "average": 15.137909031815832, 
    "stdev": 5.9778181034421936, 
    "median": 16.38933864084101, 
}
        baseline_min = 13.283489826310669, baseline_max = 16.99232823732099,
        baseline_confidence_size = 1.8544192055051605
        ----- finding optimal epsilon -------------------------------------------------------------------------------------------------------------------------------------
                reward_single_sum=-15.14052449944033, 
                reward_single_sum=1.3326951641259,     confidence_size=52.003907802244925, confidence_max=45.09999313458768, new_horizon=20
                reward_single_sum=-15.198900014352159,     confidence_size=16.06233927443706, confidence_max=6.393429491214862, new_horizon=11
            episode=0, horizon=20, effective_score=-9.67, baseline_lowerbound=14.68 baseline_stdev=1.09, new_epsilon=2.9167, bad=True, gap_average=0.6212582133875952
                reward_single_sum=-5.0953586097004155, 
                reward_single_sum=2.0829633485302863,     confidence_size=22.661070568753956, confidence_max=21.154872938168882, new_horizon=11
                reward_single_sum=-1.3576733975471487,     confidence_size=6.0525297142577, confidence_max=4.595840161351938, new_horizon=2
            episode=1, horizon=20, effective_score=-1.46, baseline_lowerbound=14.68 baseline_stdev=1.09, new_epsilon=2.4306, bad=True, gap_average=0.3961291203233931
                reward_single_sum=-0.016602442848786758, 
                reward_single_sum=-8.235871384582506,     confidence_size=25.947210865713775, confidence_max=21.820973951998113, new_horizon=2
                reward_single_sum=-3.2858222333677807,     confidence_size=6.976368000816589, confidence_max=3.1302693138835633, new_horizon=2
            episode=2, horizon=20, effective_score=-3.85, baseline_lowerbound=14.68 baseline_stdev=1.09, new_epsilon=2.0255, bad=True, gap_average=0.4482865858740277
                reward_single_sum=-4.3151841712224845, 
                reward_single_sum=-8.466048037149404,     confidence_size=13.10376151061428, confidence_max=6.7131454064283265, new_horizon=18
            episode=3, horizon=20, effective_score=-6.39, baseline_lowerbound=14.68 baseline_stdev=1.09, new_epsilon=1.6879, bad=True, gap_average=0.49243940035502115
                reward_single_sum=-1.4782951224605827, 
                reward_single_sum=1.8080622285958867,     confidence_size=10.374621851704987, confidence_max=10.539505404772633, new_horizon=16
            episode=4, horizon=20, effective_score=0.16, baseline_lowerbound=14.68 baseline_stdev=1.09, new_epsilon=1.4066, bad=True, gap_average=0.3633583621184031
                reward_single_sum=-7.148936171378861, 
                reward_single_sum=-4.319167400905818,     confidence_size=8.933228430555278, confidence_max=3.199176644412935, new_horizon=20
            episode=5, horizon=20, effective_score=-5.73, baseline_lowerbound=14.68 baseline_stdev=1.09, new_epsilon=1.1721, bad=True, gap_average=0.494694086710612
                reward_single_sum=-3.3597823152039945, 
                reward_single_sum=14.940663389104683,     confidence_size=57.772233393555595, confidence_max=63.5626739305059, new_horizon=22
                reward_single_sum=-1.772932045102318,     confidence_size=17.092500836172455, confidence_max=20.361817179105238, new_horizon=21
                reward_single_sum=4.798023760295896,     confidence_size=9.782329839791846, confidence_max=13.433823037065409, new_horizon=18
                reward_single_sum=8.749990961147443,     confidence_size=7.200121377380638, confidence_max=11.871314127428978, new_horizon=7
            episode=6, horizon=20, effective_score=4.67, baseline_lowerbound=14.68 baseline_stdev=1.09, new_epsilon=0.9768, bad=True, gap_average=0.4197557561794917
                reward_single_sum=3.023609674747014, 
                reward_single_sum=5.239677778198772,     confidence_size=6.995851672535284, confidence_max=11.127495399008174, new_horizon=14
            episode=7, horizon=20, effective_score=4.13, baseline_lowerbound=14.68 baseline_stdev=1.09, new_epsilon=0.8140, bad=True, gap_average=0.5095191898941994
                reward_single_sum=-8.715565797836963, 
                reward_single_sum=9.315987272083696,     confidence_size=56.9233727547125, confidence_max=57.22358349183583, new_horizon=8
                reward_single_sum=-0.5531253138262979,     confidence_size=15.221963902655403, confidence_max=15.23772928946221, new_horizon=8
                reward_single_sum=13.42373660643463,     confidence_size=11.725249585599695, confidence_max=15.09300777731346, new_horizon=10
                reward_single_sum=-7.595008496545931,     confidence_size=9.462494141062475, confidence_max=10.637698995124298, new_horizon=8
            episode=8, horizon=20, effective_score=1.18, baseline_lowerbound=14.68 baseline_stdev=1.09, new_epsilon=0.6783, bad=True, gap_average=0.3641754269997279
                reward_single_sum=2.141206960790814, 
                reward_single_sum=14.264449793685136,     confidence_size=38.271571400243054, confidence_max=46.47439977748101, new_horizon=16
                reward_single_sum=7.135646844975388,     confidence_size=10.271666451726038, confidence_max=18.118767651543145, new_horizon=12
                reward_single_sum=2.747965187755108,     confidence_size=6.577733258456014, confidence_max=13.150050455257624, new_horizon=16
            episode=9, horizon=20, effective_score=6.57, baseline_lowerbound=14.68 baseline_stdev=1.09, new_epsilon=0.5653, bad=True, gap_average=0.2801489399373531
                reward_single_sum=1.9622632415055448, 
                reward_single_sum=16.232202721219544,     confidence_size=45.048426003080976, confidence_max=54.14565898444349, new_horizon=11
                reward_single_sum=-0.7058662849005921,     confidence_size=15.353440256056635, confidence_max=21.18297348199813, new_horizon=12
                reward_single_sum=-2.467809867226454,     confidence_size=10.019479562807291, confidence_max=13.774677015456799, new_horizon=12
                reward_single_sum=7.4330973991605,     confidence_size=7.2032882177091935, confidence_max=11.6940656596609, new_horizon=12
            episode=10, horizon=20, effective_score=4.49, baseline_lowerbound=14.68 baseline_stdev=1.09, new_epsilon=0.4711, bad=True, gap_average=0.2627456982930501
                reward_single_sum=3.5914465606353523, 
                reward_single_sum=1.9820458508433008,     confidence_size=5.080678084685633, confidence_max=7.867424290424957, new_horizon=4
            episode=11, horizon=20, effective_score=2.79, baseline_lowerbound=14.68 baseline_stdev=1.09, new_epsilon=0.3925, bad=True, gap_average=0.23610673487186432
                reward_single_sum=11.451869519840853, 
                reward_single_sum=19.65459657964079,     confidence_size=25.894990199655233, confidence_max=41.44822324939604, new_horizon=9
                reward_single_sum=7.526502882505239,     confidence_size=10.43288076641085, confidence_max=23.310537093739807, new_horizon=8
                reward_single_sum=11.947131754852084,     confidence_size=5.970777134619361, confidence_max=18.6158023188291, new_horizon=8
                reward_single_sum=14.03388369718043,     confidence_size=4.231250796222236, confidence_max=17.154047683026114, new_horizon=8
                reward_single_sum=16.07670651061476,     confidence_size=3.4330114448072164, confidence_max=16.881459935579574, new_horizon=8
                reward_single_sum=14.485775255884183,     confidence_size=2.8127219940456625, confidence_max=16.40936002269114, new_horizon=8
                reward_single_sum=6.320160059536189,     confidence_size=2.9342771805500902, confidence_max=15.621355463056906, new_horizon=8
                reward_single_sum=3.631309364153661,     confidence_size=3.154711974994597, confidence_max=14.835593711017728, new_horizon=8
                reward_single_sum=-0.5832287361287211,     confidence_size=3.576476807988274, confidence_max=14.03094749679622, new_horizon=8
                reward_single_sum=12.321824797521906,     confidence_size=3.213363401243531, confidence_max=13.83759355448002, new_horizon=8
                reward_single_sum=6.8985783061911405,     confidence_size=2.959554710236678, confidence_max=13.273313876219387, new_horizon=8
            episode=12, horizon=20, effective_score=10.31, baseline_lowerbound=14.68 baseline_stdev=1.09, new_epsilon=0.3271, bad=True, gap_average=0.19277814658151732
                reward_single_sum=5.016417688279427, 
                reward_single_sum=13.561918449911127,     confidence_size=26.97708418924235, confidence_max=36.266252258337616, new_horizon=10
                reward_single_sum=9.715883501264209,     confidence_size=7.21519931951871, confidence_max=16.64660586600363, new_horizon=8
                reward_single_sum=0.12959326789791348,     confidence_size=6.845246823556447, confidence_max=13.951200050394615, new_horizon=10
                reward_single_sum=20.49746923244382,     confidence_size=7.461359285410458, confidence_max=17.245615713369755, new_horizon=10
                reward_single_sum=9.256120593699185,     confidence_size=5.761130783733215, confidence_max=15.45736457264916, new_horizon=10
                reward_single_sum=8.330663451783801,     confidence_size=4.710663822386496, confidence_max=14.211816134569279, new_horizon=10
                reward_single_sum=6.340819916765182,     confidence_size=4.047322177801791, confidence_max=13.153432940557373, new_horizon=10
            episode=13, horizon=20, effective_score=9.11, baseline_lowerbound=14.68 baseline_stdev=1.09, new_epsilon=0.2726, bad=True, gap_average=0.14492154203355312
                reward_single_sum=8.835498625033605, 
                reward_single_sum=18.918002918008753,     confidence_size=31.829213376379386, confidence_max=45.705964147900545, new_horizon=6
                reward_single_sum=10.181492440739332,     confidence_size=9.228550856773246, confidence_max=21.87354885136714, new_horizon=8
                reward_single_sum=12.772309524536281,     confidence_size=5.259819573442641, confidence_max=17.936645450522132, new_horizon=6
                reward_single_sum=6.775626226971585,     confidence_size=4.466796904710904, confidence_max=15.963382851768813, new_horizon=6
                reward_single_sum=14.105293162663138,     confidence_size=3.5568951688163564, confidence_max=15.48826565180847, new_horizon=8
                reward_single_sum=5.8405653203291505,     confidence_size=3.355957187948312, confidence_max=14.41721264770286, new_horizon=8
                reward_single_sum=6.604147810341857,     confidence_size=3.0238644487390465, confidence_max=13.527981452317007, new_horizon=8
                reward_single_sum=9.793950101036613,     confidence_size=2.621598511199741, confidence_max=13.046808081161998, new_horizon=8
            episode=14, horizon=20, effective_score=10.43, baseline_lowerbound=14.68 baseline_stdev=1.09, new_epsilon=0.2272, bad=True, gap_average=0.14165761605457022
        optimal_epsilon = 0.8139881377648227
        optimal_horizon = 8
    running theory method
    running n_step horizon method
    running n_step planlen method
    # 
    # acceptable_performance_level = 0.99
    # 
    running ppac method
    running theory method
    running n_step horizon method
    running n_step planlen method
