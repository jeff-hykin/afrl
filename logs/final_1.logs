

-------------------------------------------------------

 Environment: LunarLanderContinuous-v2

-------------------------------------------------------




-----------------------------------------------------------------------------------------------------
 Agent Model Exists, loading: ./subrepos/rl-trained-agents/sac/LunarLanderContinuous-v2_1/LunarLanderContinuous-v2.zip
-----------------------------------------------------------------------------------------------------


Wrapping the env with a `Monitor` wrapper
Wrapping the env in a DummyVecEnv.


-----------------------------------------------------------------------------------------------------
 Coach Model Exists, loading: models.ignore/coach/LunarLanderContinuous-v2/final_1_80%/
-----------------------------------------------------------------------------------------------------


coach settings = {
    "force_retrain": False, 
    "learning_rate": 0.0001, 
    "hidden_sizes": [64, 64, 64, 64, ], 
    "loss_function": "state_prediction_loss", 
    "consistent_coach_loss": {"scale_future_state_loss": 1000000, }, 
    "delayed_coach_loss": {"epochs_of_delay": 50, }, 
    "value_plus_state_loss": {"value_proportion": 0.5, }, 
    "number_of_episodes": 100, 
    "number_of_epochs": 100, 
    "train_test_split": 0.7, 
    "batch_size": 128, 
    "with_card": True, 
}


-----------------------------------------------------------------------------------------------------
 Testing Agent+Coach
-----------------------------------------------------------------------------------------------------


test settings = {
    "force_recompute": True, 
    "graph_smoothing": 0, 
    "number_of_episodes_for_baseline": 30, 
    "number_of_epochs_for_optimal_parameters": 15, 
    "number_of_episodes_for_testing": 10, 
    "initial_epsilon": 10.1, 
    "initial_horizon": 16, 
    "reward_discount": 0.98, 
    "acceptable_performance_loss": 1, 
    "acceptable_performance_level": 0.8, 
    "confidence_interval_for_convergence": 90, 
    "min_reward_single_timestep": -100, 
    "max_reward_single_timestep": 70, 
    "horizons": {0.0: 1, 0.005: 10, 0.01: 20, }, 
}
    scaled_epsilon: 1.9951, forecast_average: 5.6503, episode_reward:89.31, max_timestep_reward: 20.75, min_timestep_reward: -100.00
    scaled_epsilon: 1.9951, forecast_average: 5.9546, episode_reward:277.31, max_timestep_reward: 100.00, min_timestep_reward: -2.38
    scaled_epsilon: 1.9951, forecast_average: 6.0173, episode_reward:298.21, max_timestep_reward: 100.00, min_timestep_reward: -3.05
    scaled_epsilon: 1.9951, forecast_average: 6.1015, episode_reward:289.18, max_timestep_reward: 100.00, min_timestep_reward: -9.87
    scaled_epsilon: 1.9951, forecast_average: 6.0789, episode_reward:301.21, max_timestep_reward: 100.00, min_timestep_reward: -4.69
    scaled_epsilon: 1.9951, forecast_average: 6.0172, episode_reward:259.90, max_timestep_reward: 100.00, min_timestep_reward: -9.44
    scaled_epsilon: 1.9951, forecast_average: 6.0015, episode_reward:42.64, max_timestep_reward: 9.64, min_timestep_reward: -100.00
    scaled_epsilon: 1.9951, forecast_average: 5.8947, episode_reward:3.44, max_timestep_reward: 13.11, min_timestep_reward: -100.00
    scaled_epsilon: 1.9951, forecast_average: 5.8760, episode_reward:258.02, max_timestep_reward: 100.00, min_timestep_reward: -7.20
    scaled_epsilon: 1.9951, forecast_average: 5.8756, episode_reward:257.09, max_timestep_reward: 100.00, min_timestep_reward: -9.63
self.recorder = {
    number_of_records: 0,
    records: [ ... ],
    local_data: {},
    parent_data:    {
    }
}


-------------------------------------------------------

 Environment: HopperBulletEnv-v0

-------------------------------------------------------




-----------------------------------------------------------------------------------------------------
 Agent Model Exists, loading: ./subrepos/rl-trained-agents/sac/HopperBulletEnv-v0_1/HopperBulletEnv-v0.zip
-----------------------------------------------------------------------------------------------------


pybullet build time: Oct 11 2021 20:52:37
Wrapping the env with a `Monitor` wrapper
Wrapping the env in a DummyVecEnv.


-----------------------------------------------------------------------------------------------------
 Coach Model Exists, loading: models.ignore/coach/HopperBulletEnv-v0/final_1_80%/
-----------------------------------------------------------------------------------------------------


coach settings = {
    "force_retrain": False, 
    "learning_rate": 0.0001, 
    "hidden_sizes": [64, 64, 64, 64, ], 
    "loss_function": "state_prediction_loss", 
    "consistent_coach_loss": {"scale_future_state_loss": 1000000, }, 
    "delayed_coach_loss": {"epochs_of_delay": 50, }, 
    "value_plus_state_loss": {"value_proportion": 0.5, }, 
    "number_of_episodes": 100, 
    "number_of_epochs": 100, 
    "train_test_split": 0.7, 
    "batch_size": 128, 
    "with_card": True, 
}


-----------------------------------------------------------------------------------------------------
 Testing Agent+Coach
-----------------------------------------------------------------------------------------------------


test settings = {
    "force_recompute": True, 
    "graph_smoothing": 0, 
    "number_of_episodes_for_baseline": 30, 
    "number_of_epochs_for_optimal_parameters": 15, 
    "number_of_episodes_for_testing": 10, 
    "initial_epsilon": 10.1, 
    "initial_horizon": 16, 
    "reward_discount": 0.98, 
    "acceptable_performance_loss": 1, 
    "acceptable_performance_level": 0.8, 
    "confidence_interval_for_convergence": 90, 
    "min_reward_single_timestep": 18, 
    "max_reward_single_timestep": 110, 
    "horizons": {0.1: 26, 0.01: 16, 0.001: 11, }, 
}
----- getting a reward baseline -----------------------------------------------------------------------------------------------------------------------------------
  episode_index=0, episode_discounted_reward_sum=109.9969396219113
  episode_index=1, episode_discounted_reward_sum=109.10104547243334
  episode_index=2, episode_discounted_reward_sum=112.4374018031179
  episode_index=3, episode_discounted_reward_sum=110.9502333147074
  episode_index=4, episode_discounted_reward_sum=110.41940564040175
  episode_index=5, episode_discounted_reward_sum=111.32043786558123
  episode_index=6, episode_discounted_reward_sum=111.9963352517592
  episode_index=7, episode_discounted_reward_sum=108.90152457119423
  episode_index=8, episode_discounted_reward_sum=111.59672602461853
  episode_index=9, episode_discounted_reward_sum=107.0778736046189
  episode_index=10, episode_discounted_reward_sum=111.40758339624064
  episode_index=11, episode_discounted_reward_sum=112.7322078348281
  episode_index=12, episode_discounted_reward_sum=111.81315138915986
  episode_index=13, episode_discounted_reward_sum=112.76325785689613
  episode_index=14, episode_discounted_reward_sum=110.4894849625362
  episode_index=15, episode_discounted_reward_sum=110.29650629089976
  episode_index=16, episode_discounted_reward_sum=111.03068553041196
  episode_index=17, episode_discounted_reward_sum=111.49187376525647
  episode_index=18, episode_discounted_reward_sum=107.68725850962996
  episode_index=19, episode_discounted_reward_sum=111.9041771789836
  episode_index=20, episode_discounted_reward_sum=107.89013898853536
  episode_index=21, episode_discounted_reward_sum=110.0944670184727
  episode_index=22, episode_discounted_reward_sum=109.08352742276985
  episode_index=23, episode_discounted_reward_sum=110.39930398061928
  episode_index=24, episode_discounted_reward_sum=107.63108074199619
  episode_index=25, episode_discounted_reward_sum=110.63980292264837
  episode_index=26, episode_discounted_reward_sum=107.70534267597252
  episode_index=27, episode_discounted_reward_sum=111.71547068074186
  episode_index=28, episode_discounted_reward_sum=109.08653954123986
  episode_index=29, episode_discounted_reward_sum=110.30637025195641
baseline = {
    "max": 112.76325785689613, 
    "min": 107.0778736046189, 
    "range": 5.685384252277231, 
    "count": 30, 
    "sum": 3309.9661541101386, 
    "average": 110.33220513700462, 
    "stdev": 1.6201111137907513, 
    "median": 110.45444530146898, 
}
baseline_min = 109.8296195593779, baseline_max = 110.83479071463134,
baseline_confidence_size = 0.5025855776267178
----- finding optimal epsilon -------------------------------------------------------------------------------------------------------------------------------------
            reward_single_sum=58.02526740758567, 
            reward_single_sum=22.33846637357914, confidence_size=112.65879704342916, confidence_max=152.8406639340115, new_horizon=7
            reward_single_sum=21.655066194966295, confidence_size=35.072299056340704, confidence_max=69.07856571505107, new_horizon=7
        episode=0, horizon=32, effective_score=34.01, baseline_lowerbound=88.27 baseline_stdev=0.30, new_epsilon=6.7333, bad=True, gap_average=4.966516186209286
            reward_single_sum=21.409202859361372, 
            reward_single_sum=60.299499415156006, confidence_size=122.77183439510299, confidence_max=163.6261855323616, new_horizon=12
            reward_single_sum=85.28108731247882, confidence_size=54.263185500719295, confidence_max=109.92644869638468, new_horizon=14
            reward_single_sum=61.75679107387877, confidence_size=31.131324436050463, confidence_max=88.3179696012692, new_horizon=16
        episode=1, horizon=32, effective_score=57.19, baseline_lowerbound=88.27 baseline_stdev=0.30, new_epsilon=4.4889, bad=True, gap_average=2.441789935771427
            reward_single_sum=22.89603616149221, 
            reward_single_sum=86.48725737128866, confidence_size=200.74958462069688, confidence_max=255.4412313870872, new_horizon=10
            reward_single_sum=91.04035126304554, confidence_size=64.22575144978352, confidence_max=131.0336330483923, new_horizon=10
            reward_single_sum=96.05307282799912, confidence_size=40.44432396465962, confidence_max=114.56350337061599, new_horizon=10
            reward_single_sum=100.28876014636232, confidence_size=30.493901853959343, confidence_max=109.8469974079969, new_horizon=10
            reward_single_sum=22.848546925244854, confidence_size=30.23180536186607, confidence_max=100.16747614443818, new_horizon=10
        episode=2, horizon=32, effective_score=69.94, baseline_lowerbound=88.27 baseline_stdev=0.30, new_epsilon=2.9926, bad=True, gap_average=2.00711519347955
            reward_single_sum=58.79001957796211, 
            reward_single_sum=96.81843851745245, confidence_size=120.05099384234586, confidence_max=197.85522289005308, new_horizon=8
            reward_single_sum=37.89530398424957, confidence_size=50.36298250544748, confidence_max=114.86423653200217, new_horizon=8
            reward_single_sum=21.269070303157495, confidence_size=38.35008608373626, confidence_max=92.04329417944166, new_horizon=8
        episode=3, horizon=32, effective_score=53.69, baseline_lowerbound=88.27 baseline_stdev=0.30, new_epsilon=1.9951, bad=True, gap_average=1.8809662472587938
            reward_single_sum=101.83366358268596, 
            reward_single_sum=98.63800475807938, confidence_size=10.088297872323388, confidence_max=110.32413204270605, new_horizon=8
            reward_single_sum=107.32727336173177, confidence_size=7.409305505306989, confidence_max=110.00895273947269, new_horizon=8
            reward_single_sum=23.154578004999223, confidence_size=46.93112121263731, confidence_max=129.66950113951137, new_horizon=8
            reward_single_sum=101.89096146671903, confidence_size=33.928286816220165, confidence_max=120.49718305106322, new_horizon=8
            reward_single_sum=106.43303374899482, confidence_size=27.02106018994416, confidence_max=116.90064601047918, new_horizon=8
            reward_single_sum=103.887167001173, confidence_size=22.363122603778223, confidence_max=114.24379145011868, new_horizon=8
            reward_single_sum=105.37076399731, confidence_size=19.15098923394872, confidence_max=112.71791997416037, new_horizon=8
            reward_single_sum=107.42803688054028, confidence_size=16.82286914388152, confidence_max=111.92992278857412, new_horizon=8
            reward_single_sum=104.53725840541689, confidence_size=14.93331911925955, confidence_max=110.98339324002458, new_horizon=8
            reward_single_sum=102.1020918435304, confidence_size=13.392693554175437, confidence_max=109.99295110428278, new_horizon=8
            reward_single_sum=78.05750554033852, confidence_size=12.427775139215619, confidence_max=107.48280335517555, new_horizon=8
        episode=4, horizon=8, effective_score=95.06, baseline_lowerbound=88.27 baseline_stdev=0.30, new_epsilon=2.9926, bad=False, gap_average=1.1725479286502085
            reward_single_sum=35.05067386092251, 
            reward_single_sum=92.7064044945602, confidence_size=182.01197831254237, confidence_max=245.89051749028363, new_horizon=6
            reward_single_sum=90.742206292884, confidence_size=55.18690405623015, confidence_max=128.01999893901905, new_horizon=6
            reward_single_sum=22.5029833674197, confidence_size=43.19686110178171, confidence_max=103.4474281057283, new_horizon=6
        episode=5, horizon=8, effective_score=60.25, baseline_lowerbound=88.27 baseline_stdev=0.30, new_epsilon=1.9951, bad=True, gap_average=2.07640498838445
            reward_single_sum=106.93470752666686, 
            reward_single_sum=103.19454147627117, confidence_size=11.807239533146394, confidence_max=116.8718640346154, new_horizon=8
            reward_single_sum=100.23343270203098, confidence_size=5.661396627926095, confidence_max=109.11562386291577, new_horizon=8
        episode=6, horizon=6, effective_score=103.45, baseline_lowerbound=88.27 baseline_stdev=0.30, new_epsilon=2.9926, bad=False, gap_average=1.15110083535855
            reward_single_sum=96.08552392285188, 
            reward_single_sum=22.166999939507466, confidence_size=233.35159638584503, confidence_max=292.4778583170246, new_horizon=8
            reward_single_sum=70.76978817840619, confidence_size=63.330208638981006, confidence_max=126.33764598590284, new_horizon=8
            reward_single_sum=21.717096830405918, confidence_size=43.50553910615797, confidence_max=96.19039132395082, new_horizon=8
        episode=7, horizon=8, effective_score=52.68, baseline_lowerbound=88.27 baseline_stdev=0.30, new_epsilon=1.9951, bad=True, gap_average=1.3538527887714795
            reward_single_sum=101.62926367609306, 
            reward_single_sum=104.03304090322995, confidence_size=7.588426054539752, confidence_max=110.41957834420126, new_horizon=8
            reward_single_sum=102.57943458250408, confidence_size=2.040968157811534, confidence_max=104.78821454508723, new_horizon=8
        episode=8, horizon=8, effective_score=102.75, baseline_lowerbound=88.27 baseline_stdev=0.30, new_epsilon=2.9926, bad=False, gap_average=1.1031086006844013
            reward_single_sum=60.4396745674672, 
            reward_single_sum=93.4751097540648, confidence_size=104.28876447574437, confidence_max=181.2461566365103, new_horizon=8
            reward_single_sum=87.32154572255246, confidence_size=29.617359708124425, confidence_max=110.02946972281923, new_horizon=8
            reward_single_sum=82.59929208068941, confidence_size=16.927706075780613, confidence_max=97.88661160697407, new_horizon=8
        episode=9, horizon=8, effective_score=80.96, baseline_lowerbound=88.27 baseline_stdev=0.30, new_epsilon=1.9951, bad=True, gap_average=1.5750292290088743
            reward_single_sum=22.96401993728797, 
            reward_single_sum=103.37962832390015, confidence_size=253.86208463230574, confidence_max=317.03390876289967, new_horizon=8
            reward_single_sum=23.403892013843432, confidence_size=78.05761624455359, confidence_max=127.9734630028974, new_horizon=8
            reward_single_sum=22.431310579218252, confidence_size=47.33230765613608, confidence_max=90.37702036969851, new_horizon=8
        episode=10, horizon=32, effective_score=43.04, baseline_lowerbound=88.27 baseline_stdev=0.30, new_epsilon=1.3300, bad=True, gap_average=1.3024556107660878
            reward_single_sum=102.29721533532957, 
            reward_single_sum=101.87575708095142, confidence_size=1.3304913460027024, confidence_max=103.41697755414319, new_horizon=8
        episode=11, horizon=8, effective_score=102.09, baseline_lowerbound=88.27 baseline_stdev=0.30, new_epsilon=1.9951, bad=False, gap_average=1.011309398517961
            reward_single_sum=103.83202607428926, 
            reward_single_sum=97.6638378522372, confidence_size=19.472203865279262, confidence_max=120.22013582854248, new_horizon=8
            reward_single_sum=104.54685697108746, confidence_size=6.380073139731678, confidence_max=108.39431343893632, new_horizon=8
        episode=12, horizon=8, effective_score=102.01, baseline_lowerbound=88.27 baseline_stdev=0.30, new_epsilon=2.9926, bad=False, gap_average=1.0072228117803832
            reward_single_sum=94.58657609378672, 
            reward_single_sum=99.87252180074418, confidence_size=16.687073857229095, confidence_max=113.91662280449452, new_horizon=8
            reward_single_sum=100.61655619784374, confidence_size=5.542653854699793, confidence_max=103.90120521882467, new_horizon=8
        episode=13, horizon=10, effective_score=98.36, baseline_lowerbound=88.27 baseline_stdev=0.30, new_epsilon=4.4889, bad=False, gap_average=1.239810698772701
            reward_single_sum=22.04685473712637, 
            reward_single_sum=22.53604965128533, confidence_size=1.5443275651520274, confidence_max=23.83577975935788, new_horizon=10
        episode=14, horizon=8, effective_score=22.29, baseline_lowerbound=88.27 baseline_stdev=0.30, new_epsilon=2.9926, bad=True, gap_average=5.582842375102796
optimal_epsilon = 2.9925925925925925
optimal_horizon = 8
    scaled_epsilon: 2.9926, forecast_average: 3.8739, episode_reward:888.71, max_timestep_reward: 4.70, min_timestep_reward: -0.51
    scaled_epsilon: 2.9926, forecast_average: 3.6422, episode_reward:1138.79, max_timestep_reward: 4.82, min_timestep_reward: -0.34
    scaled_epsilon: 2.9926, forecast_average: 3.5816, episode_reward:334.75, max_timestep_reward: 5.04, min_timestep_reward: -0.42
    scaled_epsilon: 2.9926, forecast_average: 2.6862, episode_reward:24.11, max_timestep_reward: 3.43, min_timestep_reward: -1.25
    scaled_epsilon: 2.9926, forecast_average: 2.7131, episode_reward:308.39, max_timestep_reward: 5.07, min_timestep_reward: -0.71
    scaled_epsilon: 2.9926, forecast_average: 2.7308, episode_reward:148.89, max_timestep_reward: 3.20, min_timestep_reward: -0.51
    scaled_epsilon: 2.9926, forecast_average: 2.4835, episode_reward:24.12, max_timestep_reward: 3.56, min_timestep_reward: -1.81
    scaled_epsilon: 2.9926, forecast_average: 2.5496, episode_reward:363.08, max_timestep_reward: 4.02, min_timestep_reward: -0.66
    scaled_epsilon: 2.9926, forecast_average: 2.6360, episode_reward:463.99, max_timestep_reward: 5.04, min_timestep_reward: -0.61
    scaled_epsilon: 2.9926, forecast_average: 2.8391, episode_reward:24.37, max_timestep_reward: 3.10, min_timestep_reward: -2.11
self.recorder = {
    number_of_records: 0,
    records: [ ... ],
    local_data: {},
    parent_data:    {
    }
}
/home/jeffhykin/repos/AFRL/.venv/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(
argv[0]=
argv[0]=


-------------------------------------------------------

 Environment: HalfCheetahBulletEnv-v0

-------------------------------------------------------




-----------------------------------------------------------------------------------------------------
 Agent Model Exists, loading: subrepos/rl-trained-agents/sac/HalfCheetahBulletEnv-v0_1/HalfCheetahBulletEnv-v0.zip
-----------------------------------------------------------------------------------------------------


pybullet build time: Oct 11 2021 20:52:37
Wrapping the env with a `Monitor` wrapper
Wrapping the env in a DummyVecEnv.


-----------------------------------------------------------------------------------------------------
 Training Coach Model from scrach
-----------------------------------------------------------------------------------------------------


coach settings = {
    "force_retrain": False, 
    "learning_rate": 0.0001, 
    "hidden_sizes": [64, 64, 64, 64, ], 
    "loss_function": "state_prediction_loss", 
    "consistent_coach_loss": {"scale_future_state_loss": 1000000, }, 
    "delayed_coach_loss": {"epochs_of_delay": 50, }, 
    "value_plus_state_loss": {"value_proportion": 0.5, }, 
    "number_of_episodes": 100, 
    "number_of_epochs": 100, 
    "train_test_split": 0.7, 
    "batch_size": 128, 
    "with_card": True, 
}
Starting Experience Recording
    Episode: 0, Reward: 2800.877, Average Reward: 2800.877
    Episode: 1, Reward: 2794.938, Average Reward: 2797.907
    Episode: 2, Reward: 2801.315, Average Reward: 2799.043
    Episode: 3, Reward: 2793.928, Average Reward: 2797.764
    Episode: 4, Reward: 2782.309, Average Reward: 2794.673
    Episode: 5, Reward: 2802.035, Average Reward: 2795.900
    Episode: 6, Reward: 2786.139, Average Reward: 2794.506
    Episode: 7, Reward: 2794.462, Average Reward: 2794.500
    Episode: 8, Reward: 2809.752, Average Reward: 2796.195
    Episode: 9, Reward: 2799.965, Average Reward: 2796.572
    Episode: 10, Reward: 2786.799, Average Reward: 2795.684
    Episode: 11, Reward: 2799.654, Average Reward: 2796.014
    Episode: 12, Reward: 2795.359, Average Reward: 2795.964
    Episode: 13, Reward: 2788.051, Average Reward: 2795.399
    Episode: 14, Reward: 2796.050, Average Reward: 2795.442
    Episode: 15, Reward: 2782.960, Average Reward: 2794.662
    Episode: 16, Reward: 2784.927, Average Reward: 2794.089
    Episode: 17, Reward: 2806.308, Average Reward: 2794.768
    Episode: 18, Reward: 2787.232, Average Reward: 2794.372
    Episode: 19, Reward: 2795.599, Average Reward: 2794.433
    Episode: 20, Reward: 2789.233, Average Reward: 2794.185
    Episode: 21, Reward: 2813.016, Average Reward: 2795.041
    Episode: 22, Reward: 2782.607, Average Reward: 2794.501
    Episode: 23, Reward: 2769.708, Average Reward: 2793.468
    Episode: 24, Reward: 2795.198, Average Reward: 2793.537
    Episode: 25, Reward: 2796.143, Average Reward: 2793.637
    Episode: 26, Reward: 2796.601, Average Reward: 2793.747
    Episode: 27, Reward: 2821.138, Average Reward: 2794.725
    Episode: 28, Reward: 2770.258, Average Reward: 2793.881
    Episode: 29, Reward: 2778.227, Average Reward: 2793.360
    Episode: 30, Reward: 2802.484, Average Reward: 2793.654
    Episode: 31, Reward: 2787.076, Average Reward: 2793.448
    Episode: 32, Reward: 2786.143, Average Reward: 2793.227
    Episode: 33, Reward: 2786.698, Average Reward: 2793.035
    Episode: 34, Reward: 2782.595, Average Reward: 2792.737
    Episode: 35, Reward: 2781.472, Average Reward: 2792.424
    Episode: 36, Reward: 2791.221, Average Reward: 2792.391
    Episode: 37, Reward: 2791.083, Average Reward: 2792.357
    Episode: 38, Reward: 2781.596, Average Reward: 2792.081
    Episode: 39, Reward: 2772.411, Average Reward: 2791.589
    Episode: 40, Reward: 2797.850, Average Reward: 2791.742
    Episode: 41, Reward: 2782.794, Average Reward: 2791.529
    Episode: 42, Reward: 2808.221, Average Reward: 2791.917
    Episode: 43, Reward: 2797.880, Average Reward: 2792.053
    Episode: 44, Reward: 2794.433, Average Reward: 2792.105
    Episode: 45, Reward: 2796.905, Average Reward: 2792.210
    Episode: 46, Reward: 2786.461, Average Reward: 2792.088
    Episode: 47, Reward: 2800.501, Average Reward: 2792.263
    Episode: 48, Reward: 2791.972, Average Reward: 2792.257
    Episode: 49, Reward: 2793.779, Average Reward: 2792.287
    Episode: 50, Reward: 2798.252, Average Reward: 2792.404
    Episode: 51, Reward: 2753.745, Average Reward: 2791.661
    Episode: 52, Reward: 2817.543, Average Reward: 2792.149
    Episode: 53, Reward: 2783.123, Average Reward: 2791.982
    Episode: 54, Reward: 2813.888, Average Reward: 2792.380
    Episode: 55, Reward: 2813.345, Average Reward: 2792.755
    Episode: 56, Reward: 2779.214, Average Reward: 2792.517
    Episode: 57, Reward: 2812.568, Average Reward: 2792.863
    Episode: 58, Reward: 2800.896, Average Reward: 2792.999
    Episode: 59, Reward: 2813.279, Average Reward: 2793.337
    Episode: 60, Reward: 2788.081, Average Reward: 2793.251
    Episode: 61, Reward: 2781.997, Average Reward: 2793.069
    Episode: 62, Reward: 2813.227, Average Reward: 2793.389
    Episode: 63, Reward: 2784.233, Average Reward: 2793.246
    Episode: 64, Reward: 2792.883, Average Reward: 2793.241
    Episode: 65, Reward: 2782.694, Average Reward: 2793.081
    Episode: 66, Reward: 2794.442, Average Reward: 2793.101
    Episode: 67, Reward: 2790.353, Average Reward: 2793.061
    Episode: 68, Reward: 2799.802, Average Reward: 2793.158
    Episode: 69, Reward: 2797.983, Average Reward: 2793.227
    Episode: 70, Reward: 2804.742, Average Reward: 2793.389
    Episode: 71, Reward: 2786.199, Average Reward: 2793.290
    Episode: 72, Reward: 2799.439, Average Reward: 2793.374
    Episode: 73, Reward: 2784.444, Average Reward: 2793.253
    Episode: 74, Reward: 2787.872, Average Reward: 2793.181
    Episode: 75, Reward: 2785.619, Average Reward: 2793.082
    Episode: 76, Reward: 2786.513, Average Reward: 2792.997
    Episode: 77, Reward: 2778.062, Average Reward: 2792.805
    Episode: 78, Reward: 2796.428, Average Reward: 2792.851
    Episode: 79, Reward: 2793.740, Average Reward: 2792.862
    Episode: 80, Reward: 2798.962, Average Reward: 2792.937
    Episode: 81, Reward: 2787.206, Average Reward: 2792.868
    Episode: 82, Reward: 2808.373, Average Reward: 2793.054
    Episode: 83, Reward: 2787.567, Average Reward: 2792.989
    Episode: 84, Reward: 2816.421, Average Reward: 2793.265
    Episode: 85, Reward: 2789.829, Average Reward: 2793.225
    Episode: 86, Reward: 2812.880, Average Reward: 2793.451
    Episode: 87, Reward: 2790.045, Average Reward: 2793.412
    Episode: 88, Reward: 2784.253, Average Reward: 2793.309
    Episode: 89, Reward: 2802.716, Average Reward: 2793.414
    Episode: 90, Reward: 2787.364, Average Reward: 2793.347
    Episode: 91, Reward: 2778.694, Average Reward: 2793.188
    Episode: 92, Reward: 2792.820, Average Reward: 2793.184
    Episode: 93, Reward: 2785.793, Average Reward: 2793.105
    Episode: 94, Reward: 2811.869, Average Reward: 2793.303
    Episode: 95, Reward: 2779.425, Average Reward: 2793.158
    Episode: 96, Reward: 2808.453, Average Reward: 2793.316
    Episode: 97, Reward: 2781.477, Average Reward: 2793.195
    Episode: 98, Reward: 2780.061, Average Reward: 2793.062
    Episode: 99, Reward: 2776.592, Average Reward: 2792.898

    Max Episode Reward: 2821.138169778618
    Min Episode Reward: 2753.744925740726
    Max Timestep Reward: 3.409786983921397
    Min Timestep Reward: 0.40728848909365567
Starting Train/Test
    Epoch: 0
    Epoch: 1
    Epoch: 2
    Epoch: 3
    Epoch: 4
    Epoch: 5
    Epoch: 6
    Epoch: 7
    Epoch: 8
    Epoch: 9
    Epoch: 10
    Epoch: 11
    Epoch: 12
    Epoch: 13
    Epoch: 14
    Epoch: 15
    Epoch: 16
    Epoch: 17
    Epoch: 18
    Epoch: 19
    Epoch: 20
    Epoch: 21
    Epoch: 22
    Epoch: 23
    Epoch: 24
    Epoch: 25
    Epoch: 26
    Epoch: 27
    Epoch: 28
    Epoch: 29
    Epoch: 30
    Epoch: 31
    Epoch: 32
    Epoch: 33
    Epoch: 34
    Epoch: 35
    Epoch: 36
    Epoch: 37
    Epoch: 38
    Epoch: 39
    Epoch: 40
    Epoch: 41
    Epoch: 42
    Epoch: 43
    Epoch: 44
    Epoch: 45
    Epoch: 46
    Epoch: 47
    Epoch: 48
    Epoch: 49
    Epoch: 50
    Epoch: 51
    Epoch: 52
    Epoch: 53
    Epoch: 54
    Epoch: 55
    Epoch: 56
    Epoch: 57
    Epoch: 58
    Epoch: 59
    Epoch: 60
    Epoch: 61
    Epoch: 62
    Epoch: 63
    Epoch: 64
    Epoch: 65
    Epoch: 66
    Epoch: 67
    Epoch: 68
    Epoch: 69
    Epoch: 70
    Epoch: 71
    Epoch: 72
    Epoch: 73
    Epoch: 74
    Epoch: 75
    Epoch: 76
    Epoch: 77
    Epoch: 78
    Epoch: 79
    Epoch: 80
    Epoch: 81
    Epoch: 82
    Epoch: 83
    Epoch: 84
    Epoch: 85
    Epoch: 86
    Epoch: 87
    Epoch: 88
    Epoch: 89
    Epoch: 90
    Epoch: 91
    Epoch: 92
    Epoch: 93
    Epoch: 94
    Epoch: 95
    Epoch: 96
    Epoch: 97
    Epoch: 98
    Epoch: 99
Saving Coach to: models.ignore/coach/HalfCheetahBulletEnv-v0/final_1_80%/


-----------------------------------------------------------------------------------------------------
 Testing Agent+Coach
-----------------------------------------------------------------------------------------------------


test settings = {
    "force_recompute": True, 
    "graph_smoothing": 0, 
    "number_of_episodes_for_baseline": 30, 
    "number_of_epochs_for_optimal_parameters": 15, 
    "number_of_episodes_for_testing": 10, 
    "initial_epsilon": 10.1, 
    "initial_horizon": 16, 
    "reward_discount": 0.98, 
    "acceptable_performance_loss": 1, 
    "acceptable_performance_level": 0.8, 
    "confidence_interval_for_convergence": 90, 
    "min_reward_single_timestep": -60, 
    "max_reward_single_timestep": 100, 
    "horizons": {0: 1, 0.001: 11, 0.01: 13, 0.1: 26, }, 
}
----- getting a reward baseline -----------------------------------------------------------------------------------------------------------------------------------
  episode_index=0, episode_discounted_reward_sum=102.80245137928826
  episode_index=1, episode_discounted_reward_sum=104.26396516638142
  episode_index=2, episode_discounted_reward_sum=103.0372356432131
  episode_index=3, episode_discounted_reward_sum=105.05633543485206
  episode_index=4, episode_discounted_reward_sum=106.66327396181029
  episode_index=5, episode_discounted_reward_sum=105.17418454380265
  episode_index=6, episode_discounted_reward_sum=106.02684256743157
  episode_index=7, episode_discounted_reward_sum=103.07890531350354
  episode_index=8, episode_discounted_reward_sum=106.2120333595421
  episode_index=9, episode_discounted_reward_sum=104.17843112486922
  episode_index=10, episode_discounted_reward_sum=106.19628829280472
  episode_index=11, episode_discounted_reward_sum=104.98772085114483
  episode_index=12, episode_discounted_reward_sum=102.59625495750481
  episode_index=13, episode_discounted_reward_sum=105.72357712021217
  episode_index=14, episode_discounted_reward_sum=105.21772629204052
  episode_index=15, episode_discounted_reward_sum=101.58280311862438
  episode_index=16, episode_discounted_reward_sum=104.11553399241787
  episode_index=17, episode_discounted_reward_sum=103.94440793889824
  episode_index=18, episode_discounted_reward_sum=106.48789674998864
  episode_index=19, episode_discounted_reward_sum=102.58141179434438
  episode_index=20, episode_discounted_reward_sum=104.10215153566708
  episode_index=21, episode_discounted_reward_sum=101.96516513998574
  episode_index=22, episode_discounted_reward_sum=103.53362878060453
  episode_index=23, episode_discounted_reward_sum=100.52790459383701
  episode_index=24, episode_discounted_reward_sum=104.53305645795744
  episode_index=25, episode_discounted_reward_sum=105.30584916806865
  episode_index=26, episode_discounted_reward_sum=106.51922485599641
  episode_index=27, episode_discounted_reward_sum=102.8631900730118
  episode_index=28, episode_discounted_reward_sum=104.32794365865023
  episode_index=29, episode_discounted_reward_sum=106.35677271089435
baseline = {
    "max": 106.66327396181029, 
    "min": 100.52790459383701, 
    "range": 6.135369367973283, 
    "count": 30, 
    "sum": 3129.962166577348, 
    "average": 104.33207221924494, 
    "stdev": 1.6251292758092355, 
    "median": 104.29595441251583, 
}
baseline_min = 103.82792992378867, baseline_max = 104.8362145147012,
baseline_confidence_size = 0.504142295456262
----- finding optimal epsilon -------------------------------------------------------------------------------------------------------------------------------------
            reward_single_sum=13.325812404793213, 
            reward_single_sum=9.759794922768194, confidence_size=11.257474139471043, confidence_max=22.80027780325174, new_horizon=18
        episode=0, horizon=32, effective_score=11.54, baseline_lowerbound=83.47 baseline_stdev=0.30, new_epsilon=6.7333, bad=True, gap_average=3.4740590296983718
            reward_single_sum=31.530873934453282, 
            reward_single_sum=34.176734665525544, confidence_size=8.352653599379902, confidence_max=41.20645789936931, new_horizon=13
        episode=1, horizon=32, effective_score=32.85, baseline_lowerbound=83.47 baseline_stdev=0.30, new_epsilon=4.4889, bad=True, gap_average=0.7785141429901123
            reward_single_sum=27.649422462275208, 
            reward_single_sum=21.926075740038048, confidence_size=18.067894518627917, confidence_max=42.85564361978454, new_horizon=8
        episode=2, horizon=32, effective_score=24.79, baseline_lowerbound=83.47 baseline_stdev=0.30, new_epsilon=2.9926, bad=True, gap_average=1.05906409740448
            reward_single_sum=16.929040278312478, 
            reward_single_sum=47.0631932173423, confidence_size=95.12977688302131, confidence_max=127.12589363084864, new_horizon=4
            reward_single_sum=59.05712720164047, confidence_size=36.59182825250771, confidence_max=77.60828181827279, new_horizon=4
        episode=3, horizon=32, effective_score=41.02, baseline_lowerbound=83.47 baseline_stdev=0.30, new_epsilon=1.9951, bad=True, gap_average=1.1187014115651448
            reward_single_sum=59.992967382319485, 
            reward_single_sum=71.92906445408545, confidence_size=37.68077548383668, confidence_max=103.64179140203913, new_horizon=4
        episode=4, horizon=32, effective_score=65.96, baseline_lowerbound=83.47 baseline_stdev=0.30, new_epsilon=1.3300, bad=True, gap_average=1.4100427469611168
            reward_single_sum=77.78665852657869, 
            reward_single_sum=83.49137353079391, confidence_size=18.009076499685726, confidence_max=98.64809252837202, new_horizon=6
        episode=5, horizon=32, effective_score=80.64, baseline_lowerbound=83.47 baseline_stdev=0.30, new_epsilon=0.8867, bad=True, gap_average=0.9344144730567933
            reward_single_sum=89.75841522972077, 
            reward_single_sum=86.82511513932864, confidence_size=9.260063944539496, confidence_max=97.5518291290642, new_horizon=6
        episode=6, horizon=6, effective_score=88.29, baseline_lowerbound=83.47 baseline_stdev=0.30, new_epsilon=1.3300, bad=False, gap_average=0.6713818655014038
            reward_single_sum=73.11895818180814, 
            reward_single_sum=89.14216786356629, confidence_size=50.58328220008674, confidence_max=131.71384522277393, new_horizon=6
            reward_single_sum=88.75285816857962, confidence_size=15.409878503005935, confidence_max=99.08120657432394, new_horizon=6
        episode=7, horizon=4, effective_score=83.67, baseline_lowerbound=83.47 baseline_stdev=0.30, new_epsilon=1.9951, bad=False, gap_average=0.6720797011057535
            reward_single_sum=67.75832667882077, 
            reward_single_sum=61.88520577405401, confidence_size=18.540713004540077, confidence_max=83.36247923097746, new_horizon=4
        episode=8, horizon=6, effective_score=64.82, baseline_lowerbound=83.47 baseline_stdev=0.30, new_epsilon=1.3300, bad=True, gap_average=0.9298158540725708
            reward_single_sum=84.24335328803345, 
            reward_single_sum=91.23537348128752, confidence_size=22.072939043338337, confidence_max=109.81230242799882, new_horizon=6
            reward_single_sum=76.83604983110894, confidence_size=12.13926450607223, confidence_max=96.24419003954885, new_horizon=4
        episode=9, horizon=4, effective_score=84.10, baseline_lowerbound=83.47 baseline_stdev=0.30, new_epsilon=1.9951, bad=False, gap_average=0.9410837290684382
            reward_single_sum=68.0122733372646, 
            reward_single_sum=58.99979074532184, confidence_size=28.45128780849784, confidence_max=91.95731984979105, new_horizon=4
        episode=10, horizon=4, effective_score=63.51, baseline_lowerbound=83.47 baseline_stdev=0.30, new_epsilon=1.3300, bad=True, gap_average=1.4080606985092163
            reward_single_sum=55.16374914811796, 
            reward_single_sum=74.46467982710072, confidence_size=60.93064015574765, confidence_max=125.74485464335697, new_horizon=4
            reward_single_sum=74.95245769355638, confidence_size=19.027973236060326, confidence_max=87.22160212565201, new_horizon=4
        episode=11, horizon=6, effective_score=68.19, baseline_lowerbound=83.47 baseline_stdev=0.30, new_epsilon=0.8867, bad=True, gap_average=0.9377150839567184
            reward_single_sum=92.6847327992812, 
            reward_single_sum=85.42776076831906, confidence_size=22.90935907667763, confidence_max=111.96560586047775, new_horizon=6
            reward_single_sum=89.99029028594319, confidence_size=6.184288462821122, confidence_max=95.55188308066893, new_horizon=6
        episode=12, horizon=4, effective_score=89.37, baseline_lowerbound=83.47 baseline_stdev=0.30, new_epsilon=1.3300, bad=False, gap_average=0.7688590574264527
            reward_single_sum=88.38094905863447, 
            reward_single_sum=84.49244623703547, confidence_size=12.275520290089212, confidence_max=98.71221793792417, new_horizon=6
        episode=13, horizon=4, effective_score=86.44, baseline_lowerbound=83.47 baseline_stdev=0.30, new_epsilon=1.9951, bad=False, gap_average=0.8968851981759072
            reward_single_sum=66.61072581693553, 
            reward_single_sum=79.03644775608964, confidence_size=39.22646035791475, confidence_max=112.05004714442731, new_horizon=4
            reward_single_sum=73.13234443308184, confidence_size=10.47828983684613, confidence_max=83.4047958388818, new_horizon=4
        episode=14, horizon=6, effective_score=72.93, baseline_lowerbound=83.47 baseline_stdev=0.30, new_epsilon=1.3300, bad=True, gap_average=1.0525048243204753
optimal_epsilon = 1.3300411522633744
optimal_horizon = 6
    scaled_epsilon: 1.3300, forecast_average: 2.2596, episode_reward:1954.99, max_timestep_reward: 2.94, min_timestep_reward: -0.70
    scaled_epsilon: 1.3300, forecast_average: 2.1962, episode_reward:1012.95, max_timestep_reward: 2.90, min_timestep_reward: -2.82
    scaled_epsilon: 1.3300, forecast_average: 2.1878, episode_reward:2014.46, max_timestep_reward: 3.02, min_timestep_reward: -0.74
    scaled_epsilon: 1.3300, forecast_average: 2.2012, episode_reward:1908.32, max_timestep_reward: 3.01, min_timestep_reward: -1.89
    scaled_epsilon: 1.3300, forecast_average: 2.2286, episode_reward:1913.83, max_timestep_reward: 2.98, min_timestep_reward: -2.23
    scaled_epsilon: 1.3300, forecast_average: 2.2408, episode_reward:1985.08, max_timestep_reward: 3.02, min_timestep_reward: -1.74
    scaled_epsilon: 1.3300, forecast_average: 2.2404, episode_reward:1794.36, max_timestep_reward: 2.78, min_timestep_reward: -1.98
    scaled_epsilon: 1.3300, forecast_average: 2.1834, episode_reward:1124.94, max_timestep_reward: 2.89, min_timestep_reward: -1.81
    scaled_epsilon: 1.3300, forecast_average: 2.1828, episode_reward:1983.88, max_timestep_reward: 2.90, min_timestep_reward: -1.11
    scaled_epsilon: 1.3300, forecast_average: 2.1943, episode_reward:1877.45, max_timestep_reward: 2.83, min_timestep_reward: -2.05
self.recorder = {
    number_of_records: 0,
    records: [ ... ],
    local_data: {},
    parent_data:    {
    }
}
/home/jeffhykin/repos/AFRL/.venv/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(
argv[0]=
argv[0]=
argv[0]=
argv[0]=


-------------------------------------------------------

 Environment: BipedalWalker-v3

-------------------------------------------------------




-----------------------------------------------------------------------------------------------------
 Agent Model Exists, loading: ./subrepos/rl-trained-agents/sac/Walker2d-v3_1/Walker2d-v3.zip
-----------------------------------------------------------------------------------------------------


/home/jeffhykin/repos/AFRL/.venv/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(
Wrapping the env with a `Monitor` wrapper
Wrapping the env in a DummyVecEnv.
Traceback (most recent call last):
  File "./main/run/full.py", line 50, in <module>
    full_run(
  File "./main/run/full.py", line 17, in full_run
    agent = Agent.smart_load(
  File "/home/jeffhykin/repos/AFRL/main/agent.py", line 28, in smart_load
    agent = Agent.load(
  File "/home/jeffhykin/repos/AFRL/.venv/lib/python3.8/site-packages/stable_baselines3/common/base_class.py", line 709, in load
    check_for_correct_spaces(env, data["observation_space"], data["action_space"])
  File "/home/jeffhykin/repos/AFRL/.venv/lib/python3.8/site-packages/stable_baselines3/common/utils.py", line 223, in check_for_correct_spaces
    raise ValueError(f"Observation spaces do not match: {observation_space} != {env.observation_space}")
ValueError: Observation spaces do not match: Box([-inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf
 -inf -inf -inf], [inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf], (17,), float64) != Box([-inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf
 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf   0.], [inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf
 inf inf inf inf inf inf  1.], (25,), float32)


-------------------------------------------------------

 Environment: AntBulletEnv-v0

-------------------------------------------------------




-----------------------------------------------------------------------------------------------------
 Agent Model Exists, loading: subrepos/rl-trained-agents/sac/AntBulletEnv-v0_1/AntBulletEnv-v0.zip
-----------------------------------------------------------------------------------------------------


pybullet build time: Oct 11 2021 20:52:37
Wrapping the env with a `Monitor` wrapper
Wrapping the env in a DummyVecEnv.


-----------------------------------------------------------------------------------------------------
 Training Coach Model from scrach
-----------------------------------------------------------------------------------------------------


coach settings = {
    "force_retrain": False, 
    "learning_rate": 0.0001, 
    "hidden_sizes": [64, 64, 64, 64, ], 
    "loss_function": "state_prediction_loss", 
    "consistent_coach_loss": {"scale_future_state_loss": 1000000, }, 
    "delayed_coach_loss": {"epochs_of_delay": 50, }, 
    "value_plus_state_loss": {"value_proportion": 0.5, }, 
    "number_of_episodes": 100, 
    "number_of_epochs": 100, 
    "train_test_split": 0.7, 
    "batch_size": 128, 
    "with_card": True, 
}
Starting Experience Recording
    Episode: 0, Reward: 3111.323, Average Reward: 3111.323
    Episode: 1, Reward: 3076.581, Average Reward: 3093.952
    Episode: 2, Reward: 3116.691, Average Reward: 3101.532
    Episode: 3, Reward: 3106.670, Average Reward: 3102.817
    Episode: 4, Reward: 3041.861, Average Reward: 3090.626
    Episode: 5, Reward: 3103.561, Average Reward: 3092.781
    Episode: 6, Reward: 3115.573, Average Reward: 3096.037
    Episode: 7, Reward: 3073.373, Average Reward: 3093.204
    Episode: 8, Reward: 3109.906, Average Reward: 3095.060
    Episode: 9, Reward: 3109.130, Average Reward: 3096.467
    Episode: 10, Reward: 3129.581, Average Reward: 3099.477
    Episode: 11, Reward: 3080.760, Average Reward: 3097.918
    Episode: 12, Reward: 3089.701, Average Reward: 3097.286
    Episode: 13, Reward: 3008.823, Average Reward: 3090.967
    Episode: 14, Reward: 3051.923, Average Reward: 3088.364
    Episode: 15, Reward: 3097.610, Average Reward: 3088.942
    Episode: 16, Reward: 3093.707, Average Reward: 3089.222
    Episode: 17, Reward: 3120.431, Average Reward: 3090.956
    Episode: 18, Reward: 3062.153, Average Reward: 3089.440
    Episode: 19, Reward: 3101.539, Average Reward: 3090.045
    Episode: 20, Reward: 3099.568, Average Reward: 3090.498
    Episode: 21, Reward: 3084.191, Average Reward: 3090.212
    Episode: 22, Reward: 3128.409, Average Reward: 3091.872
    Episode: 23, Reward: 3137.926, Average Reward: 3093.791
    Episode: 24, Reward: 3085.223, Average Reward: 3093.449
    Episode: 25, Reward: 3096.605, Average Reward: 3093.570
    Episode: 26, Reward: 3059.417, Average Reward: 3092.305
    Episode: 27, Reward: 3076.790, Average Reward: 3091.751
    Episode: 28, Reward: 3123.408, Average Reward: 3092.843
    Episode: 29, Reward: 3094.786, Average Reward: 3092.907
    Episode: 30, Reward: 3086.128, Average Reward: 3092.689
    Episode: 31, Reward: 3097.693, Average Reward: 3092.845
    Episode: 32, Reward: 3078.688, Average Reward: 3092.416
    Episode: 33, Reward: 3090.465, Average Reward: 3092.359
    Episode: 34, Reward: 3081.987, Average Reward: 3092.062
    Episode: 35, Reward: 3085.792, Average Reward: 3091.888
    Episode: 36, Reward: 3119.547, Average Reward: 3092.636
    Episode: 37, Reward: 3124.255, Average Reward: 3093.468
    Episode: 38, Reward: 3110.175, Average Reward: 3093.896
    Episode: 39, Reward: 3105.932, Average Reward: 3094.197
    Episode: 40, Reward: 2985.663, Average Reward: 3091.550
    Episode: 41, Reward: 3040.997, Average Reward: 3090.346
    Episode: 42, Reward: 3104.110, Average Reward: 3090.666
    Episode: 43, Reward: 3094.766, Average Reward: 3090.759
    Episode: 44, Reward: 3147.943, Average Reward: 3092.030
    Episode: 45, Reward: 3092.941, Average Reward: 3092.050
    Episode: 46, Reward: 3059.588, Average Reward: 3091.359
    Episode: 47, Reward: 3097.615, Average Reward: 3091.490
    Episode: 48, Reward: 3121.807, Average Reward: 3092.108
    Episode: 49, Reward: 3045.576, Average Reward: 3091.178
    Episode: 50, Reward: 3108.146, Average Reward: 3091.510
    Episode: 51, Reward: 3117.403, Average Reward: 3092.008
    Episode: 52, Reward: 3053.992, Average Reward: 3091.291
    Episode: 53, Reward: 3064.248, Average Reward: 3090.790
    Episode: 54, Reward: 3161.477, Average Reward: 3092.075
    Episode: 55, Reward: 3126.298, Average Reward: 3092.687
    Episode: 56, Reward: 3080.440, Average Reward: 3092.472
    Episode: 57, Reward: 3088.140, Average Reward: 3092.397
    Episode: 58, Reward: 3142.011, Average Reward: 3093.238
    Episode: 59, Reward: 3148.537, Average Reward: 3094.160
    Episode: 60, Reward: 3110.995, Average Reward: 3094.436
    Episode: 61, Reward: 3054.855, Average Reward: 3093.797
    Episode: 62, Reward: 3105.844, Average Reward: 3093.988
    Episode: 63, Reward: 3116.061, Average Reward: 3094.333
    Episode: 64, Reward: 3086.586, Average Reward: 3094.214
    Episode: 65, Reward: 3127.724, Average Reward: 3094.722
    Episode: 66, Reward: 3099.267, Average Reward: 3094.790
    Episode: 67, Reward: 3129.257, Average Reward: 3095.297
    Episode: 68, Reward: 3121.647, Average Reward: 3095.678
    Episode: 69, Reward: 3091.429, Average Reward: 3095.618
    Episode: 70, Reward: 3073.697, Average Reward: 3095.309
    Episode: 71, Reward: 3103.149, Average Reward: 3095.418
    Episode: 72, Reward: 3125.608, Average Reward: 3095.831
    Episode: 73, Reward: 3080.551, Average Reward: 3095.625
    Episode: 74, Reward: 3134.475, Average Reward: 3096.143
    Episode: 75, Reward: 3142.562, Average Reward: 3096.754
    Episode: 76, Reward: 3092.116, Average Reward: 3096.694
    Episode: 77, Reward: 3079.786, Average Reward: 3096.477
    Episode: 78, Reward: 3075.006, Average Reward: 3096.205
    Episode: 79, Reward: 3116.440, Average Reward: 3096.458
    Episode: 80, Reward: 3111.900, Average Reward: 3096.649
    Episode: 81, Reward: 3117.574, Average Reward: 3096.904
    Episode: 82, Reward: 3130.077, Average Reward: 3097.303
    Episode: 83, Reward: 3112.680, Average Reward: 3097.486
    Episode: 84, Reward: 3131.921, Average Reward: 3097.892
    Episode: 85, Reward: 3109.852, Average Reward: 3098.031
    Episode: 86, Reward: 3116.557, Average Reward: 3098.244
    Episode: 87, Reward: 3021.466, Average Reward: 3097.371
    Episode: 88, Reward: 3079.061, Average Reward: 3097.165
    Episode: 89, Reward: 3076.269, Average Reward: 3096.933
    Episode: 90, Reward: 3068.717, Average Reward: 3096.623
    Episode: 91, Reward: 3092.637, Average Reward: 3096.580
    Episode: 92, Reward: 3101.849, Average Reward: 3096.636
    Episode: 93, Reward: 3106.041, Average Reward: 3096.737
    Episode: 94, Reward: 3106.972, Average Reward: 3096.844
    Episode: 95, Reward: 3133.293, Average Reward: 3097.224
    Episode: 96, Reward: 3092.807, Average Reward: 3097.178
    Episode: 97, Reward: 3017.440, Average Reward: 3096.365
    Episode: 98, Reward: 3069.643, Average Reward: 3096.095
    Episode: 99, Reward: 3134.860, Average Reward: 3096.482

    Max Episode Reward: 3161.477351761374
    Min Episode Reward: 2985.6627405182862
    Max Timestep Reward: 3.873265055824568
    Min Timestep Reward: -0.3588479670757818
Starting Train/Test
    Epoch: 0
    Epoch: 1
    Epoch: 2
    Epoch: 3
    Epoch: 4
    Epoch: 5
    Epoch: 6
    Epoch: 7
    Epoch: 8
    Epoch: 9
    Epoch: 10
    Epoch: 11
    Epoch: 12
    Epoch: 13
    Epoch: 14
    Epoch: 15
    Epoch: 16
    Epoch: 17
    Epoch: 18
    Epoch: 19
    Epoch: 20
    Epoch: 21
    Epoch: 22
    Epoch: 23
    Epoch: 24
    Epoch: 25
    Epoch: 26
    Epoch: 27
    Epoch: 28
    Epoch: 29
    Epoch: 30
    Epoch: 31
    Epoch: 32
    Epoch: 33
    Epoch: 34
    Epoch: 35
    Epoch: 36
    Epoch: 37
    Epoch: 38
    Epoch: 39
    Epoch: 40
    Epoch: 41
    Epoch: 42
    Epoch: 43
    Epoch: 44
    Epoch: 45
    Epoch: 46
    Epoch: 47
    Epoch: 48
    Epoch: 49
    Epoch: 50
    Epoch: 51
    Epoch: 52
    Epoch: 53
    Epoch: 54
    Epoch: 55
    Epoch: 56
    Epoch: 57
    Epoch: 58
    Epoch: 59
    Epoch: 60
    Epoch: 61
    Epoch: 62
    Epoch: 63
    Epoch: 64
    Epoch: 65
    Epoch: 66
    Epoch: 67
    Epoch: 68
    Epoch: 69
    Epoch: 70
    Epoch: 71
    Epoch: 72
    Epoch: 73
    Epoch: 74
    Epoch: 75
    Epoch: 76
    Epoch: 77
    Epoch: 78
    Epoch: 79
    Epoch: 80
    Epoch: 81
    Epoch: 82
    Epoch: 83
    Epoch: 84
    Epoch: 85
    Epoch: 86
    Epoch: 87
    Epoch: 88
    Epoch: 89
    Epoch: 90
    Epoch: 91
    Epoch: 92
    Epoch: 93
    Epoch: 94
    Epoch: 95
    Epoch: 96
    Epoch: 97
    Epoch: 98
    Epoch: 99
Saving Coach to: models.ignore/coach/AntBulletEnv-v0/final_1_80%/


-----------------------------------------------------------------------------------------------------
 Testing Agent+Coach
-----------------------------------------------------------------------------------------------------


test settings = {
    "force_recompute": True, 
    "graph_smoothing": 0, 
    "number_of_episodes_for_baseline": 30, 
    "number_of_epochs_for_optimal_parameters": 15, 
    "number_of_episodes_for_testing": 10, 
    "initial_epsilon": 10.1, 
    "initial_horizon": 16, 
    "reward_discount": 0.98, 
    "acceptable_performance_loss": 1, 
    "acceptable_performance_level": 0.8, 
    "confidence_interval_for_convergence": 90, 
    "min_reward_single_timestep": 19, 
    "max_reward_single_timestep": 105, 
    "horizons": {0: 1, 0.0007: 4, 0.0015: 16, 0.002: 26, }, 
}
----- getting a reward baseline -----------------------------------------------------------------------------------------------------------------------------------
  episode_index=0, episode_discounted_reward_sum=103.91215052474561
  episode_index=1, episode_discounted_reward_sum=104.90401747079912
  episode_index=2, episode_discounted_reward_sum=106.78994313873307
  episode_index=3, episode_discounted_reward_sum=102.27590102814041
  episode_index=4, episode_discounted_reward_sum=102.11691695647527
  episode_index=5, episode_discounted_reward_sum=102.14992360991728
  episode_index=6, episode_discounted_reward_sum=99.29898209858057
  episode_index=7, episode_discounted_reward_sum=102.1386197717213
  episode_index=8, episode_discounted_reward_sum=102.99004265634343
  episode_index=9, episode_discounted_reward_sum=102.00561453106954
  episode_index=10, episode_discounted_reward_sum=103.34479094167229
  episode_index=11, episode_discounted_reward_sum=103.5537586686248
  episode_index=12, episode_discounted_reward_sum=102.24882783722319
  episode_index=13, episode_discounted_reward_sum=99.8868981836472
  episode_index=14, episode_discounted_reward_sum=101.41369426691955
  episode_index=15, episode_discounted_reward_sum=101.42780521847143
  episode_index=16, episode_discounted_reward_sum=100.22375888942986
  episode_index=17, episode_discounted_reward_sum=105.89586703831006
  episode_index=18, episode_discounted_reward_sum=101.7762522357355
  episode_index=19, episode_discounted_reward_sum=101.67719125029545
  episode_index=20, episode_discounted_reward_sum=103.4198317738007
  episode_index=21, episode_discounted_reward_sum=101.80097904212052
  episode_index=22, episode_discounted_reward_sum=101.02071470695418
  episode_index=23, episode_discounted_reward_sum=104.96427336624542
  episode_index=24, episode_discounted_reward_sum=99.36758611143887
  episode_index=25, episode_discounted_reward_sum=102.77364330195661
  episode_index=26, episode_discounted_reward_sum=102.2300794836445
  episode_index=27, episode_discounted_reward_sum=103.24298813162238
  episode_index=28, episode_discounted_reward_sum=100.6310702540393
  episode_index=29, episode_discounted_reward_sum=102.48120757127847
baseline = {
    "max": 106.78994313873307, 
    "min": 99.29898209858057, 
    "range": 7.490961040152499, 
    "count": 30, 
    "sum": 3071.963330059956, 
    "average": 102.3987776686652, 
    "stdev": 1.7578467076894742, 
    "median": 102.19000154678089, 
}
baseline_min = 101.85346420501257, baseline_max = 102.94409113231781,
baseline_confidence_size = 0.5453134636526187
----- finding optimal epsilon -------------------------------------------------------------------------------------------------------------------------------------
            reward_single_sum=31.754624042856403, 
            reward_single_sum=17.765154389061742, confidence_size=44.16301760895388, confidence_max=68.92290682491293, new_horizon=2
        episode=0, horizon=32, effective_score=24.76, baseline_lowerbound=81.92 baseline_stdev=0.32, new_epsilon=6.7333, bad=True, gap_average=0.5706174783706665
            reward_single_sum=31.740771331044407, 
            reward_single_sum=32.49460714942976, confidence_size=2.3797660201208686, confidence_max=34.49745526035795, new_horizon=45
        episode=1, horizon=32, effective_score=32.12, baseline_lowerbound=81.92 baseline_stdev=0.32, new_epsilon=4.4889, bad=True, gap_average=0.5601553955078125
            reward_single_sum=46.53374578593435, 
            reward_single_sum=45.66987281633689, confidence_size=2.727139635195769, confidence_max=48.828948936331386, new_horizon=41
        episode=2, horizon=32, effective_score=46.10, baseline_lowerbound=81.92 baseline_stdev=0.32, new_epsilon=2.9926, bad=True, gap_average=0.7266728668212891
            reward_single_sum=18.29430047131475, 
            reward_single_sum=51.436027302729244, confidence_size=104.624313992481, confidence_max=139.48947787950294, new_horizon=6
            reward_single_sum=23.380239445319642, confidence_size=30.089614036337547, confidence_max=61.12646977612542, new_horizon=8
        episode=3, horizon=32, effective_score=31.04, baseline_lowerbound=81.92 baseline_stdev=0.32, new_epsilon=1.9951, bad=True, gap_average=0.46537994956970213
            reward_single_sum=53.37761475974006, 
            reward_single_sum=66.8669548216261, confidence_size=42.58417062469894, confidence_max=102.706455415382, new_horizon=2
            reward_single_sum=62.544913680000455, confidence_size=11.612460050035601, confidence_max=72.54228780382446, new_horizon=2
        episode=4, horizon=32, effective_score=60.93, baseline_lowerbound=81.92 baseline_stdev=0.32, new_epsilon=1.3300, bad=True, gap_average=1.7098264821370444
            reward_single_sum=75.98069166701443, 
            reward_single_sum=59.23691191246684, confidence_size=52.85803239438404, confidence_max=120.46683418412464, new_horizon=2
            reward_single_sum=54.00078074874081, confidence_size=19.355387426378215, confidence_max=82.42818220245223, new_horizon=2
        episode=5, horizon=32, effective_score=63.07, baseline_lowerbound=81.92 baseline_stdev=0.32, new_epsilon=0.8867, bad=True, gap_average=1.6456014982859293
            reward_single_sum=74.53812095787443, 
            reward_single_sum=71.11925223544081, confidence_size=10.79294378758539, confidence_max=83.621630384243, new_horizon=10
        episode=6, horizon=32, effective_score=72.83, baseline_lowerbound=81.92 baseline_stdev=0.32, new_epsilon=0.5911, bad=True, gap_average=0.5083354659080506
            reward_single_sum=94.49413601896568, 
            reward_single_sum=86.88605041031747, confidence_size=24.017781018168918, confidence_max=114.70787423281047, new_horizon=8
            reward_single_sum=86.15554130180394, confidence_size=7.785069882356744, confidence_max=96.96364579271909, new_horizon=6
        episode=7, horizon=10, effective_score=89.18, baseline_lowerbound=81.92 baseline_stdev=0.32, new_epsilon=0.8867, bad=False, gap_average=0.4480544321537018
            reward_single_sum=67.76946513065663, 
            reward_single_sum=93.58143132306294, confidence_size=81.48517032364796, confidence_max=162.1606185505077, new_horizon=8
            reward_single_sum=89.85080256064843, confidence_size=23.519136521268486, confidence_max=107.25303619272448, new_horizon=8
            reward_single_sum=75.08302507428407, confidence_size=14.33720842735972, confidence_max=95.90838944952273, new_horizon=8
        episode=8, horizon=6, effective_score=81.57, baseline_lowerbound=81.92 baseline_stdev=0.32, new_epsilon=0.5911, bad=True, gap_average=0.5350716648101806
            reward_single_sum=78.30191686025664, 
            reward_single_sum=89.55216635343012, confidence_size=35.51563988970627, confidence_max=119.44268149654962, new_horizon=6
            reward_single_sum=96.16943370096848, confidence_size=15.228851582216755, confidence_max=103.2366905537685, new_horizon=6
            reward_single_sum=90.66454280353574, confidence_size=8.818443963515477, confidence_max=97.49045889306322, new_horizon=6
        episode=9, horizon=8, effective_score=88.67, baseline_lowerbound=81.92 baseline_stdev=0.32, new_epsilon=0.8867, bad=False, gap_average=0.3942138855457306
            reward_single_sum=90.90102901223466, 
            reward_single_sum=93.78891045425448, confidence_size=9.116682914559071, confidence_max=101.46165264780363, new_horizon=8
        episode=10, horizon=2, effective_score=92.34, baseline_lowerbound=81.92 baseline_stdev=0.32, new_epsilon=1.3300, bad=False, gap_average=0.5169257674217224
            reward_single_sum=73.55882465897352, 
            reward_single_sum=67.71931019096927, confidence_size=18.434621659031915, confidence_max=89.07368908400329, new_horizon=2
        episode=11, horizon=8, effective_score=70.64, baseline_lowerbound=81.92 baseline_stdev=0.32, new_epsilon=0.8867, bad=True, gap_average=1.118807508945465
            reward_single_sum=83.11975050127457, 
            reward_single_sum=70.64896359891425, confidence_size=39.36872484776856, confidence_max=116.25308189786296, new_horizon=8
            reward_single_sum=79.45190090441082, confidence_size=10.804940796025988, confidence_max=88.5451457975592, new_horizon=6
        episode=12, horizon=6, effective_score=77.74, baseline_lowerbound=81.92 baseline_stdev=0.32, new_epsilon=0.5911, bad=True, gap_average=0.5813831650416056
            reward_single_sum=87.52934007182061, 
            reward_single_sum=61.45710222520941, confidence_size=82.30681559914589, confidence_max=156.80003674766084, new_horizon=6
            reward_single_sum=83.99917808301228, confidence_size=23.845245941057804, confidence_max=101.5071194010719, new_horizon=6
        episode=13, horizon=32, effective_score=77.66, baseline_lowerbound=81.92 baseline_stdev=0.32, new_epsilon=0.3941, bad=True, gap_average=0.429815064907074
            reward_single_sum=90.88925082040413, 
            reward_single_sum=95.15917962985887, confidence_size=13.479634744393515, confidence_max=106.503849969525, new_horizon=6
            reward_single_sum=85.68506034026203, confidence_size=7.998925724403016, confidence_max=98.57675598791135, new_horizon=6
        episode=14, horizon=6, effective_score=90.58, baseline_lowerbound=81.92 baseline_stdev=0.32, new_epsilon=0.5911, bad=False, gap_average=0.31918132972717284
optimal_epsilon = 0.8866941015089163
optimal_horizon = 6
    scaled_epsilon: 0.8867, forecast_average: 2.8340, episode_reward:2417.73, max_timestep_reward: 3.26, min_timestep_reward: -0.35
    scaled_epsilon: 0.8867, forecast_average: 2.8224, episode_reward:2497.48, max_timestep_reward: 3.42, min_timestep_reward: -0.22
    scaled_epsilon: 0.8867, forecast_average: 2.5832, episode_reward:1854.46, max_timestep_reward: 3.43, min_timestep_reward: -0.14
    scaled_epsilon: 0.8867, forecast_average: 2.6962, episode_reward:2654.46, max_timestep_reward: 3.47, min_timestep_reward: -0.20
