

-------------------------------------------------------

 Environment: LunarLanderContinuous-v2

-------------------------------------------------------




-----------------------------------------------------------------------------------------------------
 Agent Model Exists, loading: ./subrepos/rl-trained-agents/sac/LunarLanderContinuous-v2_1/LunarLanderContinuous-v2.zip
-----------------------------------------------------------------------------------------------------


Wrapping the env with a `Monitor` wrapper
Wrapping the env in a DummyVecEnv.


-----------------------------------------------------------------------------------------------------
 Coach Model Exists, loading: models.ignore/coach/LunarLanderContinuous-v2/final_1_80%/
-----------------------------------------------------------------------------------------------------


coach settings = {
    "force_retrain": False, 
    "learning_rate": 0.0001, 
    "hidden_sizes": [64, 64, 64, 64, ], 
    "loss_function": "state_prediction_loss", 
    "consistent_coach_loss": {"scale_future_state_loss": 1000000, }, 
    "delayed_coach_loss": {"epochs_of_delay": 50, }, 
    "value_plus_state_loss": {"value_proportion": 0.5, }, 
    "number_of_episodes": 100, 
    "number_of_epochs": 100, 
    "train_test_split": 0.7, 
    "batch_size": 128, 
    "with_card": True, 
}


-----------------------------------------------------------------------------------------------------
 Testing Agent+Coach
-----------------------------------------------------------------------------------------------------


test settings = {
    "force_recompute": True, 
    "graph_smoothing": 0, 
    "number_of_episodes_for_baseline": 30, 
    "number_of_epochs_for_optimal_parameters": 15, 
    "number_of_episodes_for_testing": 10, 
    "initial_epsilon": 10.1, 
    "initial_horizon": 16, 
    "reward_discount": 0.98, 
    "acceptable_performance_loss": 1, 
    "acceptable_performance_level": 0.8, 
    "confidence_interval_for_convergence": 90, 
    "min_reward_single_timestep": -100, 
    "max_reward_single_timestep": 70, 
    "horizons": {0.0: 1, 0.005: 10, 0.01: 20, }, 
}
    scaled_epsilon: 1.9951, forecast_average: 5.6503, episode_reward:89.31, max_timestep_reward: 20.75, min_timestep_reward: -100.00
    scaled_epsilon: 1.9951, forecast_average: 5.9546, episode_reward:277.31, max_timestep_reward: 100.00, min_timestep_reward: -2.38
    scaled_epsilon: 1.9951, forecast_average: 6.0173, episode_reward:298.21, max_timestep_reward: 100.00, min_timestep_reward: -3.05
    scaled_epsilon: 1.9951, forecast_average: 6.1015, episode_reward:289.18, max_timestep_reward: 100.00, min_timestep_reward: -9.87
    scaled_epsilon: 1.9951, forecast_average: 6.0789, episode_reward:301.21, max_timestep_reward: 100.00, min_timestep_reward: -4.69
    scaled_epsilon: 1.9951, forecast_average: 6.0172, episode_reward:259.90, max_timestep_reward: 100.00, min_timestep_reward: -9.44
    scaled_epsilon: 1.9951, forecast_average: 6.0015, episode_reward:42.64, max_timestep_reward: 9.64, min_timestep_reward: -100.00
    scaled_epsilon: 1.9951, forecast_average: 5.8947, episode_reward:3.44, max_timestep_reward: 13.11, min_timestep_reward: -100.00
    scaled_epsilon: 1.9951, forecast_average: 5.8760, episode_reward:258.02, max_timestep_reward: 100.00, min_timestep_reward: -7.20
    scaled_epsilon: 1.9951, forecast_average: 5.8756, episode_reward:257.09, max_timestep_reward: 100.00, min_timestep_reward: -9.63
self.recorder = {
    number_of_records: 0,
    records: [ ... ],
    local_data: {},
    parent_data:    {
    }
}


-------------------------------------------------------

 Environment: HopperBulletEnv-v0

-------------------------------------------------------




-----------------------------------------------------------------------------------------------------
 Agent Model Exists, loading: ./subrepos/rl-trained-agents/sac/HopperBulletEnv-v0_1/HopperBulletEnv-v0.zip
-----------------------------------------------------------------------------------------------------


pybullet build time: Oct 11 2021 20:52:37
Wrapping the env with a `Monitor` wrapper
Wrapping the env in a DummyVecEnv.


-----------------------------------------------------------------------------------------------------
 Coach Model Exists, loading: models.ignore/coach/HopperBulletEnv-v0/final_1_80%/
-----------------------------------------------------------------------------------------------------


coach settings = {
    "force_retrain": False, 
    "learning_rate": 0.0001, 
    "hidden_sizes": [64, 64, 64, 64, ], 
    "loss_function": "state_prediction_loss", 
    "consistent_coach_loss": {"scale_future_state_loss": 1000000, }, 
    "delayed_coach_loss": {"epochs_of_delay": 50, }, 
    "value_plus_state_loss": {"value_proportion": 0.5, }, 
    "number_of_episodes": 100, 
    "number_of_epochs": 100, 
    "train_test_split": 0.7, 
    "batch_size": 128, 
    "with_card": True, 
}


-----------------------------------------------------------------------------------------------------
 Testing Agent+Coach
-----------------------------------------------------------------------------------------------------


test settings = {
    "force_recompute": True, 
    "graph_smoothing": 0, 
    "number_of_episodes_for_baseline": 30, 
    "number_of_epochs_for_optimal_parameters": 15, 
    "number_of_episodes_for_testing": 10, 
    "initial_epsilon": 10.1, 
    "initial_horizon": 16, 
    "reward_discount": 0.98, 
    "acceptable_performance_loss": 1, 
    "acceptable_performance_level": 0.8, 
    "confidence_interval_for_convergence": 90, 
    "min_reward_single_timestep": 18, 
    "max_reward_single_timestep": 110, 
    "horizons": {0.1: 26, 0.01: 16, 0.001: 11, }, 
}
----- getting a reward baseline -----------------------------------------------------------------------------------------------------------------------------------
  episode_index=0, episode_discounted_reward_sum=109.9969396219113
  episode_index=1, episode_discounted_reward_sum=109.10104547243334
  episode_index=2, episode_discounted_reward_sum=112.4374018031179
  episode_index=3, episode_discounted_reward_sum=110.9502333147074
  episode_index=4, episode_discounted_reward_sum=110.41940564040175
  episode_index=5, episode_discounted_reward_sum=111.32043786558123
  episode_index=6, episode_discounted_reward_sum=111.9963352517592
  episode_index=7, episode_discounted_reward_sum=108.90152457119423
  episode_index=8, episode_discounted_reward_sum=111.59672602461853
  episode_index=9, episode_discounted_reward_sum=107.0778736046189
  episode_index=10, episode_discounted_reward_sum=111.40758339624064
  episode_index=11, episode_discounted_reward_sum=112.7322078348281
  episode_index=12, episode_discounted_reward_sum=111.81315138915986
  episode_index=13, episode_discounted_reward_sum=112.76325785689613
  episode_index=14, episode_discounted_reward_sum=110.4894849625362
  episode_index=15, episode_discounted_reward_sum=110.29650629089976
  episode_index=16, episode_discounted_reward_sum=111.03068553041196
  episode_index=17, episode_discounted_reward_sum=111.49187376525647
  episode_index=18, episode_discounted_reward_sum=107.68725850962996
  episode_index=19, episode_discounted_reward_sum=111.9041771789836
  episode_index=20, episode_discounted_reward_sum=107.89013898853536
  episode_index=21, episode_discounted_reward_sum=110.0944670184727
  episode_index=22, episode_discounted_reward_sum=109.08352742276985
  episode_index=23, episode_discounted_reward_sum=110.39930398061928
  episode_index=24, episode_discounted_reward_sum=107.63108074199619
  episode_index=25, episode_discounted_reward_sum=110.63980292264837
  episode_index=26, episode_discounted_reward_sum=107.70534267597252
  episode_index=27, episode_discounted_reward_sum=111.71547068074186
  episode_index=28, episode_discounted_reward_sum=109.08653954123986
  episode_index=29, episode_discounted_reward_sum=110.30637025195641
baseline = {
    "max": 112.76325785689613, 
    "min": 107.0778736046189, 
    "range": 5.685384252277231, 
    "count": 30, 
    "sum": 3309.9661541101386, 
    "average": 110.33220513700462, 
    "stdev": 1.6201111137907513, 
    "median": 110.45444530146898, 
}
baseline_min = 109.8296195593779, baseline_max = 110.83479071463134,
baseline_confidence_size = 0.5025855776267178
----- finding optimal epsilon -------------------------------------------------------------------------------------------------------------------------------------
            reward_single_sum=58.02526740758567, 
            reward_single_sum=22.33846637357914, confidence_size=112.65879704342916, confidence_max=152.8406639340115, new_horizon=7
            reward_single_sum=21.655066194966295, confidence_size=35.072299056340704, confidence_max=69.07856571505107, new_horizon=7
        episode=0, horizon=32, effective_score=34.01, baseline_lowerbound=88.27 baseline_stdev=0.30, new_epsilon=6.7333, bad=True, gap_average=4.966516186209286
            reward_single_sum=21.409202859361372, 
            reward_single_sum=60.299499415156006, confidence_size=122.77183439510299, confidence_max=163.6261855323616, new_horizon=12
            reward_single_sum=85.28108731247882, confidence_size=54.263185500719295, confidence_max=109.92644869638468, new_horizon=14
            reward_single_sum=61.75679107387877, confidence_size=31.131324436050463, confidence_max=88.3179696012692, new_horizon=16
        episode=1, horizon=32, effective_score=57.19, baseline_lowerbound=88.27 baseline_stdev=0.30, new_epsilon=4.4889, bad=True, gap_average=2.441789935771427
            reward_single_sum=22.89603616149221, 
            reward_single_sum=86.48725737128866, confidence_size=200.74958462069688, confidence_max=255.4412313870872, new_horizon=10
            reward_single_sum=91.04035126304554, confidence_size=64.22575144978352, confidence_max=131.0336330483923, new_horizon=10
            reward_single_sum=96.05307282799912, confidence_size=40.44432396465962, confidence_max=114.56350337061599, new_horizon=10
            reward_single_sum=100.28876014636232, confidence_size=30.493901853959343, confidence_max=109.8469974079969, new_horizon=10
            reward_single_sum=22.848546925244854, confidence_size=30.23180536186607, confidence_max=100.16747614443818, new_horizon=10
        episode=2, horizon=32, effective_score=69.94, baseline_lowerbound=88.27 baseline_stdev=0.30, new_epsilon=2.9926, bad=True, gap_average=2.00711519347955
            reward_single_sum=58.79001957796211, 
            reward_single_sum=96.81843851745245, confidence_size=120.05099384234586, confidence_max=197.85522289005308, new_horizon=8
            reward_single_sum=37.89530398424957, confidence_size=50.36298250544748, confidence_max=114.86423653200217, new_horizon=8
            reward_single_sum=21.269070303157495, confidence_size=38.35008608373626, confidence_max=92.04329417944166, new_horizon=8
        episode=3, horizon=32, effective_score=53.69, baseline_lowerbound=88.27 baseline_stdev=0.30, new_epsilon=1.9951, bad=True, gap_average=1.8809662472587938
            reward_single_sum=101.83366358268596, 
            reward_single_sum=98.63800475807938, confidence_size=10.088297872323388, confidence_max=110.32413204270605, new_horizon=8
            reward_single_sum=107.32727336173177, confidence_size=7.409305505306989, confidence_max=110.00895273947269, new_horizon=8
            reward_single_sum=23.154578004999223, confidence_size=46.93112121263731, confidence_max=129.66950113951137, new_horizon=8
            reward_single_sum=101.89096146671903, confidence_size=33.928286816220165, confidence_max=120.49718305106322, new_horizon=8
            reward_single_sum=106.43303374899482, confidence_size=27.02106018994416, confidence_max=116.90064601047918, new_horizon=8
            reward_single_sum=103.887167001173, confidence_size=22.363122603778223, confidence_max=114.24379145011868, new_horizon=8
            reward_single_sum=105.37076399731, confidence_size=19.15098923394872, confidence_max=112.71791997416037, new_horizon=8
            reward_single_sum=107.42803688054028, confidence_size=16.82286914388152, confidence_max=111.92992278857412, new_horizon=8
            reward_single_sum=104.53725840541689, confidence_size=14.93331911925955, confidence_max=110.98339324002458, new_horizon=8
            reward_single_sum=102.1020918435304, confidence_size=13.392693554175437, confidence_max=109.99295110428278, new_horizon=8
            reward_single_sum=78.05750554033852, confidence_size=12.427775139215619, confidence_max=107.48280335517555, new_horizon=8
        episode=4, horizon=8, effective_score=95.06, baseline_lowerbound=88.27 baseline_stdev=0.30, new_epsilon=2.9926, bad=False, gap_average=1.1725479286502085
            reward_single_sum=35.05067386092251, 
            reward_single_sum=92.7064044945602, confidence_size=182.01197831254237, confidence_max=245.89051749028363, new_horizon=6
            reward_single_sum=90.742206292884, confidence_size=55.18690405623015, confidence_max=128.01999893901905, new_horizon=6
            reward_single_sum=22.5029833674197, confidence_size=43.19686110178171, confidence_max=103.4474281057283, new_horizon=6
        episode=5, horizon=8, effective_score=60.25, baseline_lowerbound=88.27 baseline_stdev=0.30, new_epsilon=1.9951, bad=True, gap_average=2.07640498838445
            reward_single_sum=106.93470752666686, 
            reward_single_sum=103.19454147627117, confidence_size=11.807239533146394, confidence_max=116.8718640346154, new_horizon=8
            reward_single_sum=100.23343270203098, confidence_size=5.661396627926095, confidence_max=109.11562386291577, new_horizon=8
        episode=6, horizon=6, effective_score=103.45, baseline_lowerbound=88.27 baseline_stdev=0.30, new_epsilon=2.9926, bad=False, gap_average=1.15110083535855
            reward_single_sum=96.08552392285188, 
            reward_single_sum=22.166999939507466, confidence_size=233.35159638584503, confidence_max=292.4778583170246, new_horizon=8
            reward_single_sum=70.76978817840619, confidence_size=63.330208638981006, confidence_max=126.33764598590284, new_horizon=8
            reward_single_sum=21.717096830405918, confidence_size=43.50553910615797, confidence_max=96.19039132395082, new_horizon=8
        episode=7, horizon=8, effective_score=52.68, baseline_lowerbound=88.27 baseline_stdev=0.30, new_epsilon=1.9951, bad=True, gap_average=1.3538527887714795
            reward_single_sum=101.62926367609306, 
            reward_single_sum=104.03304090322995, confidence_size=7.588426054539752, confidence_max=110.41957834420126, new_horizon=8
            reward_single_sum=102.57943458250408, confidence_size=2.040968157811534, confidence_max=104.78821454508723, new_horizon=8
        episode=8, horizon=8, effective_score=102.75, baseline_lowerbound=88.27 baseline_stdev=0.30, new_epsilon=2.9926, bad=False, gap_average=1.1031086006844013
            reward_single_sum=60.4396745674672, 
            reward_single_sum=93.4751097540648, confidence_size=104.28876447574437, confidence_max=181.2461566365103, new_horizon=8
            reward_single_sum=87.32154572255246, confidence_size=29.617359708124425, confidence_max=110.02946972281923, new_horizon=8
            reward_single_sum=82.59929208068941, confidence_size=16.927706075780613, confidence_max=97.88661160697407, new_horizon=8
        episode=9, horizon=8, effective_score=80.96, baseline_lowerbound=88.27 baseline_stdev=0.30, new_epsilon=1.9951, bad=True, gap_average=1.5750292290088743
            reward_single_sum=22.96401993728797, 
            reward_single_sum=103.37962832390015, confidence_size=253.86208463230574, confidence_max=317.03390876289967, new_horizon=8
            reward_single_sum=23.403892013843432, confidence_size=78.05761624455359, confidence_max=127.9734630028974, new_horizon=8
            reward_single_sum=22.431310579218252, confidence_size=47.33230765613608, confidence_max=90.37702036969851, new_horizon=8
        episode=10, horizon=32, effective_score=43.04, baseline_lowerbound=88.27 baseline_stdev=0.30, new_epsilon=1.3300, bad=True, gap_average=1.3024556107660878
            reward_single_sum=102.29721533532957, 
            reward_single_sum=101.87575708095142, confidence_size=1.3304913460027024, confidence_max=103.41697755414319, new_horizon=8
        episode=11, horizon=8, effective_score=102.09, baseline_lowerbound=88.27 baseline_stdev=0.30, new_epsilon=1.9951, bad=False, gap_average=1.011309398517961
            reward_single_sum=103.83202607428926, 
            reward_single_sum=97.6638378522372, confidence_size=19.472203865279262, confidence_max=120.22013582854248, new_horizon=8
            reward_single_sum=104.54685697108746, confidence_size=6.380073139731678, confidence_max=108.39431343893632, new_horizon=8
        episode=12, horizon=8, effective_score=102.01, baseline_lowerbound=88.27 baseline_stdev=0.30, new_epsilon=2.9926, bad=False, gap_average=1.0072228117803832
            reward_single_sum=94.58657609378672, 
            reward_single_sum=99.87252180074418, confidence_size=16.687073857229095, confidence_max=113.91662280449452, new_horizon=8
            reward_single_sum=100.61655619784374, confidence_size=5.542653854699793, confidence_max=103.90120521882467, new_horizon=8
        episode=13, horizon=10, effective_score=98.36, baseline_lowerbound=88.27 baseline_stdev=0.30, new_epsilon=4.4889, bad=False, gap_average=1.239810698772701
            reward_single_sum=22.04685473712637, 
            reward_single_sum=22.53604965128533, confidence_size=1.5443275651520274, confidence_max=23.83577975935788, new_horizon=10
        episode=14, horizon=8, effective_score=22.29, baseline_lowerbound=88.27 baseline_stdev=0.30, new_epsilon=2.9926, bad=True, gap_average=5.582842375102796
optimal_epsilon = 2.9925925925925925
optimal_horizon = 8
    scaled_epsilon: 2.9926, forecast_average: 3.8739, episode_reward:888.71, max_timestep_reward: 4.70, min_timestep_reward: -0.51
    scaled_epsilon: 2.9926, forecast_average: 3.6422, episode_reward:1138.79, max_timestep_reward: 4.82, min_timestep_reward: -0.34
    scaled_epsilon: 2.9926, forecast_average: 3.5816, episode_reward:334.75, max_timestep_reward: 5.04, min_timestep_reward: -0.42
    scaled_epsilon: 2.9926, forecast_average: 2.6862, episode_reward:24.11, max_timestep_reward: 3.43, min_timestep_reward: -1.25
    scaled_epsilon: 2.9926, forecast_average: 2.7131, episode_reward:308.39, max_timestep_reward: 5.07, min_timestep_reward: -0.71
    scaled_epsilon: 2.9926, forecast_average: 2.7308, episode_reward:148.89, max_timestep_reward: 3.20, min_timestep_reward: -0.51
    scaled_epsilon: 2.9926, forecast_average: 2.4835, episode_reward:24.12, max_timestep_reward: 3.56, min_timestep_reward: -1.81
    scaled_epsilon: 2.9926, forecast_average: 2.5496, episode_reward:363.08, max_timestep_reward: 4.02, min_timestep_reward: -0.66
    scaled_epsilon: 2.9926, forecast_average: 2.6360, episode_reward:463.99, max_timestep_reward: 5.04, min_timestep_reward: -0.61
    scaled_epsilon: 2.9926, forecast_average: 2.8391, episode_reward:24.37, max_timestep_reward: 3.10, min_timestep_reward: -2.11
self.recorder = {
    number_of_records: 0,
    records: [ ... ],
    local_data: {},
    parent_data:    {
    }
}
/home/jeffhykin/repos/AFRL/.venv/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(
argv[0]=
argv[0]=


-------------------------------------------------------

 Environment: HalfCheetahBulletEnv-v0

-------------------------------------------------------




-----------------------------------------------------------------------------------------------------
 Agent Model Exists, loading: subrepos/rl-trained-agents/sac/HalfCheetahBulletEnv-v0_1/HalfCheetahBulletEnv-v0.zip
-----------------------------------------------------------------------------------------------------


pybullet build time: Oct 11 2021 20:52:37
Wrapping the env with a `Monitor` wrapper
Wrapping the env in a DummyVecEnv.


-----------------------------------------------------------------------------------------------------
 Training Coach Model from scrach
-----------------------------------------------------------------------------------------------------


coach settings = {
    "force_retrain": False, 
    "learning_rate": 0.0001, 
    "hidden_sizes": [64, 64, 64, 64, ], 
    "loss_function": "state_prediction_loss", 
    "consistent_coach_loss": {"scale_future_state_loss": 1000000, }, 
    "delayed_coach_loss": {"epochs_of_delay": 50, }, 
    "value_plus_state_loss": {"value_proportion": 0.5, }, 
    "number_of_episodes": 100, 
    "number_of_epochs": 100, 
    "train_test_split": 0.7, 
    "batch_size": 128, 
    "with_card": True, 
}
Starting Experience Recording
    Episode: 0, Reward: 2800.877, Average Reward: 2800.877
    Episode: 1, Reward: 2794.938, Average Reward: 2797.907
    Episode: 2, Reward: 2801.315, Average Reward: 2799.043
    Episode: 3, Reward: 2793.928, Average Reward: 2797.764
    Episode: 4, Reward: 2782.309, Average Reward: 2794.673
    Episode: 5, Reward: 2802.035, Average Reward: 2795.900
    Episode: 6, Reward: 2786.139, Average Reward: 2794.506
    Episode: 7, Reward: 2794.462, Average Reward: 2794.500
    Episode: 8, Reward: 2809.752, Average Reward: 2796.195
    Episode: 9, Reward: 2799.965, Average Reward: 2796.572
    Episode: 10, Reward: 2786.799, Average Reward: 2795.684
    Episode: 11, Reward: 2799.654, Average Reward: 2796.014
    Episode: 12, Reward: 2795.359, Average Reward: 2795.964
    Episode: 13, Reward: 2788.051, Average Reward: 2795.399
    Episode: 14, Reward: 2796.050, Average Reward: 2795.442
    Episode: 15, Reward: 2782.960, Average Reward: 2794.662
    Episode: 16, Reward: 2784.927, Average Reward: 2794.089
    Episode: 17, Reward: 2806.308, Average Reward: 2794.768
    Episode: 18, Reward: 2787.232, Average Reward: 2794.372
    Episode: 19, Reward: 2795.599, Average Reward: 2794.433
    Episode: 20, Reward: 2789.233, Average Reward: 2794.185
    Episode: 21, Reward: 2813.016, Average Reward: 2795.041
    Episode: 22, Reward: 2782.607, Average Reward: 2794.501
    Episode: 23, Reward: 2769.708, Average Reward: 2793.468
    Episode: 24, Reward: 2795.198, Average Reward: 2793.537
    Episode: 25, Reward: 2796.143, Average Reward: 2793.637
    Episode: 26, Reward: 2796.601, Average Reward: 2793.747
    Episode: 27, Reward: 2821.138, Average Reward: 2794.725
    Episode: 28, Reward: 2770.258, Average Reward: 2793.881
    Episode: 29, Reward: 2778.227, Average Reward: 2793.360
    Episode: 30, Reward: 2802.484, Average Reward: 2793.654
    Episode: 31, Reward: 2787.076, Average Reward: 2793.448
    Episode: 32, Reward: 2786.143, Average Reward: 2793.227
    Episode: 33, Reward: 2786.698, Average Reward: 2793.035
    Episode: 34, Reward: 2782.595, Average Reward: 2792.737
    Episode: 35, Reward: 2781.472, Average Reward: 2792.424
    Episode: 36, Reward: 2791.221, Average Reward: 2792.391
    Episode: 37, Reward: 2791.083, Average Reward: 2792.357
    Episode: 38, Reward: 2781.596, Average Reward: 2792.081
    Episode: 39, Reward: 2772.411, Average Reward: 2791.589
    Episode: 40, Reward: 2797.850, Average Reward: 2791.742
    Episode: 41, Reward: 2782.794, Average Reward: 2791.529
    Episode: 42, Reward: 2808.221, Average Reward: 2791.917
    Episode: 43, Reward: 2797.880, Average Reward: 2792.053
    Episode: 44, Reward: 2794.433, Average Reward: 2792.105
    Episode: 45, Reward: 2796.905, Average Reward: 2792.210
    Episode: 46, Reward: 2786.461, Average Reward: 2792.088
    Episode: 47, Reward: 2800.501, Average Reward: 2792.263
    Episode: 48, Reward: 2791.972, Average Reward: 2792.257
    Episode: 49, Reward: 2793.779, Average Reward: 2792.287
    Episode: 50, Reward: 2798.252, Average Reward: 2792.404
    Episode: 51, Reward: 2753.745, Average Reward: 2791.661
    Episode: 52, Reward: 2817.543, Average Reward: 2792.149
    Episode: 53, Reward: 2783.123, Average Reward: 2791.982
    Episode: 54, Reward: 2813.888, Average Reward: 2792.380
    Episode: 55, Reward: 2813.345, Average Reward: 2792.755
    Episode: 56, Reward: 2779.214, Average Reward: 2792.517
    Episode: 57, Reward: 2812.568, Average Reward: 2792.863
    Episode: 58, Reward: 2800.896, Average Reward: 2792.999
    Episode: 59, Reward: 2813.279, Average Reward: 2793.337
    Episode: 60, Reward: 2788.081, Average Reward: 2793.251
    Episode: 61, Reward: 2781.997, Average Reward: 2793.069
    Episode: 62, Reward: 2813.227, Average Reward: 2793.389
    Episode: 63, Reward: 2784.233, Average Reward: 2793.246
    Episode: 64, Reward: 2792.883, Average Reward: 2793.241
    Episode: 65, Reward: 2782.694, Average Reward: 2793.081
    Episode: 66, Reward: 2794.442, Average Reward: 2793.101
    Episode: 67, Reward: 2790.353, Average Reward: 2793.061
    Episode: 68, Reward: 2799.802, Average Reward: 2793.158
    Episode: 69, Reward: 2797.983, Average Reward: 2793.227
    Episode: 70, Reward: 2804.742, Average Reward: 2793.389
    Episode: 71, Reward: 2786.199, Average Reward: 2793.290
    Episode: 72, Reward: 2799.439, Average Reward: 2793.374
    Episode: 73, Reward: 2784.444, Average Reward: 2793.253
    Episode: 74, Reward: 2787.872, Average Reward: 2793.181
    Episode: 75, Reward: 2785.619, Average Reward: 2793.082
    Episode: 76, Reward: 2786.513, Average Reward: 2792.997
    Episode: 77, Reward: 2778.062, Average Reward: 2792.805
    Episode: 78, Reward: 2796.428, Average Reward: 2792.851
    Episode: 79, Reward: 2793.740, Average Reward: 2792.862
    Episode: 80, Reward: 2798.962, Average Reward: 2792.937
    Episode: 81, Reward: 2787.206, Average Reward: 2792.868
    Episode: 82, Reward: 2808.373, Average Reward: 2793.054
    Episode: 83, Reward: 2787.567, Average Reward: 2792.989
    Episode: 84, Reward: 2816.421, Average Reward: 2793.265
    Episode: 85, Reward: 2789.829, Average Reward: 2793.225
    Episode: 86, Reward: 2812.880, Average Reward: 2793.451
    Episode: 87, Reward: 2790.045, Average Reward: 2793.412
    Episode: 88, Reward: 2784.253, Average Reward: 2793.309
    Episode: 89, Reward: 2802.716, Average Reward: 2793.414
    Episode: 90, Reward: 2787.364, Average Reward: 2793.347
    Episode: 91, Reward: 2778.694, Average Reward: 2793.188
    Episode: 92, Reward: 2792.820, Average Reward: 2793.184
    Episode: 93, Reward: 2785.793, Average Reward: 2793.105
    Episode: 94, Reward: 2811.869, Average Reward: 2793.303
    Episode: 95, Reward: 2779.425, Average Reward: 2793.158
    Episode: 96, Reward: 2808.453, Average Reward: 2793.316
    Episode: 97, Reward: 2781.477, Average Reward: 2793.195
    Episode: 98, Reward: 2780.061, Average Reward: 2793.062
    Episode: 99, Reward: 2776.592, Average Reward: 2792.898

    Max Episode Reward: 2821.138169778618
    Min Episode Reward: 2753.744925740726
    Max Timestep Reward: 3.409786983921397
    Min Timestep Reward: 0.40728848909365567
Starting Train/Test
    Epoch: 0
    Epoch: 1
    Epoch: 2
    Epoch: 3
    Epoch: 4
    Epoch: 5
    Epoch: 6
    Epoch: 7
    Epoch: 8
    Epoch: 9
    Epoch: 10
    Epoch: 11
    Epoch: 12
    Epoch: 13
    Epoch: 14
    Epoch: 15
    Epoch: 16
    Epoch: 17
    Epoch: 18
    Epoch: 19
    Epoch: 20
    Epoch: 21
    Epoch: 22
    Epoch: 23
    Epoch: 24
    Epoch: 25
    Epoch: 26
    Epoch: 27
    Epoch: 28
    Epoch: 29
    Epoch: 30
    Epoch: 31
    Epoch: 32
    Epoch: 33
    Epoch: 34
    Epoch: 35
    Epoch: 36
    Epoch: 37
    Epoch: 38
    Epoch: 39
    Epoch: 40
    Epoch: 41
    Epoch: 42
    Epoch: 43
    Epoch: 44
    Epoch: 45
    Epoch: 46
    Epoch: 47
    Epoch: 48
    Epoch: 49
    Epoch: 50
    Epoch: 51
    Epoch: 52
    Epoch: 53
    Epoch: 54
    Epoch: 55
    Epoch: 56
    Epoch: 57
    Epoch: 58
    Epoch: 59
    Epoch: 60
    Epoch: 61
    Epoch: 62
    Epoch: 63
    Epoch: 64
    Epoch: 65
    Epoch: 66
    Epoch: 67
    Epoch: 68
    Epoch: 69
    Epoch: 70
    Epoch: 71
    Epoch: 72
    Epoch: 73
    Epoch: 74
    Epoch: 75
    Epoch: 76
    Epoch: 77
    Epoch: 78
    Epoch: 79
    Epoch: 80
    Epoch: 81
    Epoch: 82
    Epoch: 83
    Epoch: 84
    Epoch: 85
    Epoch: 86
    Epoch: 87
    Epoch: 88
    Epoch: 89
    Epoch: 90
    Epoch: 91
    Epoch: 92
    Epoch: 93
    Epoch: 94
    Epoch: 95
    Epoch: 96
    Epoch: 97
    Epoch: 98
    Epoch: 99
Saving Coach to: models.ignore/coach/HalfCheetahBulletEnv-v0/final_1_80%/


-----------------------------------------------------------------------------------------------------
 Testing Agent+Coach
-----------------------------------------------------------------------------------------------------


test settings = {
    "force_recompute": True, 
    "graph_smoothing": 0, 
    "number_of_episodes_for_baseline": 30, 
    "number_of_epochs_for_optimal_parameters": 15, 
    "number_of_episodes_for_testing": 10, 
    "initial_epsilon": 10.1, 
    "initial_horizon": 16, 
    "reward_discount": 0.98, 
    "acceptable_performance_loss": 1, 
    "acceptable_performance_level": 0.8, 
    "confidence_interval_for_convergence": 90, 
    "min_reward_single_timestep": -60, 
    "max_reward_single_timestep": 100, 
    "horizons": {0: 1, 0.001: 11, 0.01: 13, 0.1: 26, }, 
}
----- getting a reward baseline -----------------------------------------------------------------------------------------------------------------------------------
  episode_index=0, episode_discounted_reward_sum=102.80245137928826
  episode_index=1, episode_discounted_reward_sum=104.26396516638142
  episode_index=2, episode_discounted_reward_sum=103.0372356432131
  episode_index=3, episode_discounted_reward_sum=105.05633543485206
  episode_index=4, episode_discounted_reward_sum=106.66327396181029
  episode_index=5, episode_discounted_reward_sum=105.17418454380265
  episode_index=6, episode_discounted_reward_sum=106.02684256743157
  episode_index=7, episode_discounted_reward_sum=103.07890531350354
  episode_index=8, episode_discounted_reward_sum=106.2120333595421
  episode_index=9, episode_discounted_reward_sum=104.17843112486922
  episode_index=10, episode_discounted_reward_sum=106.19628829280472
  episode_index=11, episode_discounted_reward_sum=104.98772085114483
  episode_index=12, episode_discounted_reward_sum=102.59625495750481
  episode_index=13, episode_discounted_reward_sum=105.72357712021217
  episode_index=14, episode_discounted_reward_sum=105.21772629204052
  episode_index=15, episode_discounted_reward_sum=101.58280311862438
  episode_index=16, episode_discounted_reward_sum=104.11553399241787
  episode_index=17, episode_discounted_reward_sum=103.94440793889824
  episode_index=18, episode_discounted_reward_sum=106.48789674998864
  episode_index=19, episode_discounted_reward_sum=102.58141179434438
  episode_index=20, episode_discounted_reward_sum=104.10215153566708
  episode_index=21, episode_discounted_reward_sum=101.96516513998574
  episode_index=22, episode_discounted_reward_sum=103.53362878060453
  episode_index=23, episode_discounted_reward_sum=100.52790459383701
  episode_index=24, episode_discounted_reward_sum=104.53305645795744
  episode_index=25, episode_discounted_reward_sum=105.30584916806865
  episode_index=26, episode_discounted_reward_sum=106.51922485599641
  episode_index=27, episode_discounted_reward_sum=102.8631900730118
  episode_index=28, episode_discounted_reward_sum=104.32794365865023
  episode_index=29, episode_discounted_reward_sum=106.35677271089435
baseline = {
    "max": 106.66327396181029, 
    "min": 100.52790459383701, 
    "range": 6.135369367973283, 
    "count": 30, 
    "sum": 3129.962166577348, 
    "average": 104.33207221924494, 
    "stdev": 1.6251292758092355, 
    "median": 104.29595441251583, 
}
baseline_min = 103.82792992378867, baseline_max = 104.8362145147012,
baseline_confidence_size = 0.504142295456262
----- finding optimal epsilon -------------------------------------------------------------------------------------------------------------------------------------
            reward_single_sum=13.325812404793213, 
            reward_single_sum=9.759794922768194, confidence_size=11.257474139471043, confidence_max=22.80027780325174, new_horizon=18
        episode=0, horizon=32, effective_score=11.54, baseline_lowerbound=83.47 baseline_stdev=0.30, new_epsilon=6.7333, bad=True, gap_average=3.4740590296983718
            reward_single_sum=31.530873934453282, 
            reward_single_sum=34.176734665525544, confidence_size=8.352653599379902, confidence_max=41.20645789936931, new_horizon=13
        episode=1, horizon=32, effective_score=32.85, baseline_lowerbound=83.47 baseline_stdev=0.30, new_epsilon=4.4889, bad=True, gap_average=0.7785141429901123
            reward_single_sum=27.649422462275208, 
            reward_single_sum=21.926075740038048, confidence_size=18.067894518627917, confidence_max=42.85564361978454, new_horizon=8
        episode=2, horizon=32, effective_score=24.79, baseline_lowerbound=83.47 baseline_stdev=0.30, new_epsilon=2.9926, bad=True, gap_average=1.05906409740448
            reward_single_sum=16.929040278312478, 
            reward_single_sum=47.0631932173423, confidence_size=95.12977688302131, confidence_max=127.12589363084864, new_horizon=4
            reward_single_sum=59.05712720164047, confidence_size=36.59182825250771, confidence_max=77.60828181827279, new_horizon=4
        episode=3, horizon=32, effective_score=41.02, baseline_lowerbound=83.47 baseline_stdev=0.30, new_epsilon=1.9951, bad=True, gap_average=1.1187014115651448
            reward_single_sum=59.992967382319485, 
            reward_single_sum=71.92906445408545, confidence_size=37.68077548383668, confidence_max=103.64179140203913, new_horizon=4
        episode=4, horizon=32, effective_score=65.96, baseline_lowerbound=83.47 baseline_stdev=0.30, new_epsilon=1.3300, bad=True, gap_average=1.4100427469611168
            reward_single_sum=77.78665852657869, 
            reward_single_sum=83.49137353079391, confidence_size=18.009076499685726, confidence_max=98.64809252837202, new_horizon=6
        episode=5, horizon=32, effective_score=80.64, baseline_lowerbound=83.47 baseline_stdev=0.30, new_epsilon=0.8867, bad=True, gap_average=0.9344144730567933
            reward_single_sum=89.75841522972077, 
            reward_single_sum=86.82511513932864, confidence_size=9.260063944539496, confidence_max=97.5518291290642, new_horizon=6
        episode=6, horizon=6, effective_score=88.29, baseline_lowerbound=83.47 baseline_stdev=0.30, new_epsilon=1.3300, bad=False, gap_average=0.6713818655014038
            reward_single_sum=73.11895818180814, 
            reward_single_sum=89.14216786356629, confidence_size=50.58328220008674, confidence_max=131.71384522277393, new_horizon=6
            reward_single_sum=88.75285816857962, confidence_size=15.409878503005935, confidence_max=99.08120657432394, new_horizon=6
        episode=7, horizon=4, effective_score=83.67, baseline_lowerbound=83.47 baseline_stdev=0.30, new_epsilon=1.9951, bad=False, gap_average=0.6720797011057535
            reward_single_sum=67.75832667882077, 
            reward_single_sum=61.88520577405401, confidence_size=18.540713004540077, confidence_max=83.36247923097746, new_horizon=4
        episode=8, horizon=6, effective_score=64.82, baseline_lowerbound=83.47 baseline_stdev=0.30, new_epsilon=1.3300, bad=True, gap_average=0.9298158540725708
            reward_single_sum=84.24335328803345, 
            reward_single_sum=91.23537348128752, confidence_size=22.072939043338337, confidence_max=109.81230242799882, new_horizon=6
            reward_single_sum=76.83604983110894, confidence_size=12.13926450607223, confidence_max=96.24419003954885, new_horizon=4
        episode=9, horizon=4, effective_score=84.10, baseline_lowerbound=83.47 baseline_stdev=0.30, new_epsilon=1.9951, bad=False, gap_average=0.9410837290684382
            reward_single_sum=68.0122733372646, 
            reward_single_sum=58.99979074532184, confidence_size=28.45128780849784, confidence_max=91.95731984979105, new_horizon=4
        episode=10, horizon=4, effective_score=63.51, baseline_lowerbound=83.47 baseline_stdev=0.30, new_epsilon=1.3300, bad=True, gap_average=1.4080606985092163
            reward_single_sum=55.16374914811796, 
            reward_single_sum=74.46467982710072, confidence_size=60.93064015574765, confidence_max=125.74485464335697, new_horizon=4
            reward_single_sum=74.95245769355638, confidence_size=19.027973236060326, confidence_max=87.22160212565201, new_horizon=4
        episode=11, horizon=6, effective_score=68.19, baseline_lowerbound=83.47 baseline_stdev=0.30, new_epsilon=0.8867, bad=True, gap_average=0.9377150839567184
            reward_single_sum=92.6847327992812, 
            reward_single_sum=85.42776076831906, confidence_size=22.90935907667763, confidence_max=111.96560586047775, new_horizon=6
            reward_single_sum=89.99029028594319, confidence_size=6.184288462821122, confidence_max=95.55188308066893, new_horizon=6
        episode=12, horizon=4, effective_score=89.37, baseline_lowerbound=83.47 baseline_stdev=0.30, new_epsilon=1.3300, bad=False, gap_average=0.7688590574264527
            reward_single_sum=88.38094905863447, 
            reward_single_sum=84.49244623703547, confidence_size=12.275520290089212, confidence_max=98.71221793792417, new_horizon=6
        episode=13, horizon=4, effective_score=86.44, baseline_lowerbound=83.47 baseline_stdev=0.30, new_epsilon=1.9951, bad=False, gap_average=0.8968851981759072
            reward_single_sum=66.61072581693553, 
            reward_single_sum=79.03644775608964, confidence_size=39.22646035791475, confidence_max=112.05004714442731, new_horizon=4
            reward_single_sum=73.13234443308184, confidence_size=10.47828983684613, confidence_max=83.4047958388818, new_horizon=4
        episode=14, horizon=6, effective_score=72.93, baseline_lowerbound=83.47 baseline_stdev=0.30, new_epsilon=1.3300, bad=True, gap_average=1.0525048243204753
optimal_epsilon = 1.3300411522633744
optimal_horizon = 6
    scaled_epsilon: 1.3300, forecast_average: 2.2596, episode_reward:1954.99, max_timestep_reward: 2.94, min_timestep_reward: -0.70
    scaled_epsilon: 1.3300, forecast_average: 2.1962, episode_reward:1012.95, max_timestep_reward: 2.90, min_timestep_reward: -2.82
    scaled_epsilon: 1.3300, forecast_average: 2.1878, episode_reward:2014.46, max_timestep_reward: 3.02, min_timestep_reward: -0.74
    scaled_epsilon: 1.3300, forecast_average: 2.2012, episode_reward:1908.32, max_timestep_reward: 3.01, min_timestep_reward: -1.89
    scaled_epsilon: 1.3300, forecast_average: 2.2286, episode_reward:1913.83, max_timestep_reward: 2.98, min_timestep_reward: -2.23
    scaled_epsilon: 1.3300, forecast_average: 2.2408, episode_reward:1985.08, max_timestep_reward: 3.02, min_timestep_reward: -1.74
    scaled_epsilon: 1.3300, forecast_average: 2.2404, episode_reward:1794.36, max_timestep_reward: 2.78, min_timestep_reward: -1.98
    scaled_epsilon: 1.3300, forecast_average: 2.1834, episode_reward:1124.94, max_timestep_reward: 2.89, min_timestep_reward: -1.81
    scaled_epsilon: 1.3300, forecast_average: 2.1828, episode_reward:1983.88, max_timestep_reward: 2.90, min_timestep_reward: -1.11
    scaled_epsilon: 1.3300, forecast_average: 2.1943, episode_reward:1877.45, max_timestep_reward: 2.83, min_timestep_reward: -2.05
self.recorder = {
    number_of_records: 0,
    records: [ ... ],
    local_data: {},
    parent_data:    {
    }
}
/home/jeffhykin/repos/AFRL/.venv/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(
argv[0]=
argv[0]=
argv[0]=
argv[0]=


-------------------------------------------------------

 Environment: BipedalWalker-v3

-------------------------------------------------------




-----------------------------------------------------------------------------------------------------
 Agent Model Exists, loading: ./subrepos/rl-trained-agents/sac/Walker2d-v3_1/Walker2d-v3.zip
-----------------------------------------------------------------------------------------------------


/home/jeffhykin/repos/AFRL/.venv/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(
Wrapping the env with a `Monitor` wrapper
Wrapping the env in a DummyVecEnv.
Traceback (most recent call last):
  File "./main/run/full.py", line 50, in <module>
    full_run(
  File "./main/run/full.py", line 17, in full_run
    agent = Agent.smart_load(
  File "/home/jeffhykin/repos/AFRL/main/agent.py", line 28, in smart_load
    agent = Agent.load(
  File "/home/jeffhykin/repos/AFRL/.venv/lib/python3.8/site-packages/stable_baselines3/common/base_class.py", line 709, in load
    check_for_correct_spaces(env, data["observation_space"], data["action_space"])
  File "/home/jeffhykin/repos/AFRL/.venv/lib/python3.8/site-packages/stable_baselines3/common/utils.py", line 223, in check_for_correct_spaces
    raise ValueError(f"Observation spaces do not match: {observation_space} != {env.observation_space}")
ValueError: Observation spaces do not match: Box([-inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf
 -inf -inf -inf], [inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf], (17,), float64) != Box([-inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf
 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf   0.], [inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf
 inf inf inf inf inf inf  1.], (25,), float32)


-------------------------------------------------------

 Environment: AntBulletEnv-v0

-------------------------------------------------------




-----------------------------------------------------------------------------------------------------
 Agent Model Exists, loading: subrepos/rl-trained-agents/sac/AntBulletEnv-v0_1/AntBulletEnv-v0.zip
-----------------------------------------------------------------------------------------------------


pybullet build time: Oct 11 2021 20:52:37
Wrapping the env with a `Monitor` wrapper
Wrapping the env in a DummyVecEnv.


-----------------------------------------------------------------------------------------------------
 Training Coach Model from scrach
-----------------------------------------------------------------------------------------------------


coach settings = {
    "force_retrain": False, 
    "learning_rate": 0.0001, 
    "hidden_sizes": [64, 64, 64, 64, ], 
    "loss_function": "state_prediction_loss", 
    "consistent_coach_loss": {"scale_future_state_loss": 1000000, }, 
    "delayed_coach_loss": {"epochs_of_delay": 50, }, 
    "value_plus_state_loss": {"value_proportion": 0.5, }, 
    "number_of_episodes": 100, 
    "number_of_epochs": 100, 
    "train_test_split": 0.7, 
    "batch_size": 128, 
    "with_card": True, 
}
Starting Experience Recording
    Episode: 0, Reward: 3111.323, Average Reward: 3111.323
    Episode: 1, Reward: 3076.581, Average Reward: 3093.952
    Episode: 2, Reward: 3116.691, Average Reward: 3101.532
    Episode: 3, Reward: 3106.670, Average Reward: 3102.817
    Episode: 4, Reward: 3041.861, Average Reward: 3090.626
    Episode: 5, Reward: 3103.561, Average Reward: 3092.781
    Episode: 6, Reward: 3115.573, Average Reward: 3096.037
    Episode: 7, Reward: 3073.373, Average Reward: 3093.204
    Episode: 8, Reward: 3109.906, Average Reward: 3095.060
    Episode: 9, Reward: 3109.130, Average Reward: 3096.467
    Episode: 10, Reward: 3129.581, Average Reward: 3099.477
    Episode: 11, Reward: 3080.760, Average Reward: 3097.918
    Episode: 12, Reward: 3089.701, Average Reward: 3097.286
    Episode: 13, Reward: 3008.823, Average Reward: 3090.967
    Episode: 14, Reward: 3051.923, Average Reward: 3088.364
    Episode: 15, Reward: 3097.610, Average Reward: 3088.942
    Episode: 16, Reward: 3093.707, Average Reward: 3089.222
    Episode: 17, Reward: 3120.431, Average Reward: 3090.956
    Episode: 18, Reward: 3062.153, Average Reward: 3089.440
    Episode: 19, Reward: 3101.539, Average Reward: 3090.045
    Episode: 20, Reward: 3099.568, Average Reward: 3090.498
    Episode: 21, Reward: 3084.191, Average Reward: 3090.212
    Episode: 22, Reward: 3128.409, Average Reward: 3091.872
    Episode: 23, Reward: 3137.926, Average Reward: 3093.791
    Episode: 24, Reward: 3085.223, Average Reward: 3093.449
    Episode: 25, Reward: 3096.605, Average Reward: 3093.570
    Episode: 26, Reward: 3059.417, Average Reward: 3092.305
    Episode: 27, Reward: 3076.790, Average Reward: 3091.751
    Episode: 28, Reward: 3123.408, Average Reward: 3092.843
    Episode: 29, Reward: 3094.786, Average Reward: 3092.907
    Episode: 30, Reward: 3086.128, Average Reward: 3092.689
    Episode: 31, Reward: 3097.693, Average Reward: 3092.845
    Episode: 32, Reward: 3078.688, Average Reward: 3092.416
    Episode: 33, Reward: 3090.465, Average Reward: 3092.359
    Episode: 34, Reward: 3081.987, Average Reward: 3092.062
    Episode: 35, Reward: 3085.792, Average Reward: 3091.888
    Episode: 36, Reward: 3119.547, Average Reward: 3092.636
    Episode: 37, Reward: 3124.255, Average Reward: 3093.468
    Episode: 38, Reward: 3110.175, Average Reward: 3093.896
    Episode: 39, Reward: 3105.932, Average Reward: 3094.197
    Episode: 40, Reward: 2985.663, Average Reward: 3091.550
    Episode: 41, Reward: 3040.997, Average Reward: 3090.346
    Episode: 42, Reward: 3104.110, Average Reward: 3090.666
    Episode: 43, Reward: 3094.766, Average Reward: 3090.759
    Episode: 44, Reward: 3147.943, Average Reward: 3092.030
    Episode: 45, Reward: 3092.941, Average Reward: 3092.050
    Episode: 46, Reward: 3059.588, Average Reward: 3091.359
    Episode: 47, Reward: 3097.615, Average Reward: 3091.490
    Episode: 48, Reward: 3121.807, Average Reward: 3092.108
    Episode: 49, Reward: 3045.576, Average Reward: 3091.178
    Episode: 50, Reward: 3108.146, Average Reward: 3091.510
    Episode: 51, Reward: 3117.403, Average Reward: 3092.008
    Episode: 52, Reward: 3053.992, Average Reward: 3091.291
    Episode: 53, Reward: 3064.248, Average Reward: 3090.790
    Episode: 54, Reward: 3161.477, Average Reward: 3092.075
    Episode: 55, Reward: 3126.298, Average Reward: 3092.687
    Episode: 56, Reward: 3080.440, Average Reward: 3092.472
    Episode: 57, Reward: 3088.140, Average Reward: 3092.397
    Episode: 58, Reward: 3142.011, Average Reward: 3093.238
    Episode: 59, Reward: 3148.537, Average Reward: 3094.160
    Episode: 60, Reward: 3110.995, Average Reward: 3094.436
    Episode: 61, Reward: 3054.855, Average Reward: 3093.797
    Episode: 62, Reward: 3105.844, Average Reward: 3093.988
    Episode: 63, Reward: 3116.061, Average Reward: 3094.333
    Episode: 64, Reward: 3086.586, Average Reward: 3094.214
    Episode: 65, Reward: 3127.724, Average Reward: 3094.722
    Episode: 66, Reward: 3099.267, Average Reward: 3094.790
    Episode: 67, Reward: 3129.257, Average Reward: 3095.297
    Episode: 68, Reward: 3121.647, Average Reward: 3095.678
    Episode: 69, Reward: 3091.429, Average Reward: 3095.618
    Episode: 70, Reward: 3073.697, Average Reward: 3095.309
    Episode: 71, Reward: 3103.149, Average Reward: 3095.418
    Episode: 72, Reward: 3125.608, Average Reward: 3095.831
    Episode: 73, Reward: 3080.551, Average Reward: 3095.625
    Episode: 74, Reward: 3134.475, Average Reward: 3096.143
    Episode: 75, Reward: 3142.562, Average Reward: 3096.754
    Episode: 76, Reward: 3092.116, Average Reward: 3096.694
    Episode: 77, Reward: 3079.786, Average Reward: 3096.477
    Episode: 78, Reward: 3075.006, Average Reward: 3096.205
    Episode: 79, Reward: 3116.440, Average Reward: 3096.458
    Episode: 80, Reward: 3111.900, Average Reward: 3096.649
    Episode: 81, Reward: 3117.574, Average Reward: 3096.904
    Episode: 82, Reward: 3130.077, Average Reward: 3097.303
    Episode: 83, Reward: 3112.680, Average Reward: 3097.486
    Episode: 84, Reward: 3131.921, Average Reward: 3097.892
    Episode: 85, Reward: 3109.852, Average Reward: 3098.031
    Episode: 86, Reward: 3116.557, Average Reward: 3098.244
    Episode: 87, Reward: 3021.466, Average Reward: 3097.371
    Episode: 88, Reward: 3079.061, Average Reward: 3097.165
    Episode: 89, Reward: 3076.269, Average Reward: 3096.933
    Episode: 90, Reward: 3068.717, Average Reward: 3096.623
    Episode: 91, Reward: 3092.637, Average Reward: 3096.580
    Episode: 92, Reward: 3101.849, Average Reward: 3096.636
    Episode: 93, Reward: 3106.041, Average Reward: 3096.737
    Episode: 94, Reward: 3106.972, Average Reward: 3096.844
    Episode: 95, Reward: 3133.293, Average Reward: 3097.224
    Episode: 96, Reward: 3092.807, Average Reward: 3097.178
    Episode: 97, Reward: 3017.440, Average Reward: 3096.365
    Episode: 98, Reward: 3069.643, Average Reward: 3096.095
    Episode: 99, Reward: 3134.860, Average Reward: 3096.482

    Max Episode Reward: 3161.477351761374
    Min Episode Reward: 2985.6627405182862
    Max Timestep Reward: 3.873265055824568
    Min Timestep Reward: -0.3588479670757818
Starting Train/Test
    Epoch: 0
    Epoch: 1
    Epoch: 2
    Epoch: 3
    Epoch: 4
    Epoch: 5
    Epoch: 6
    Epoch: 7
    Epoch: 8
    Epoch: 9
    Epoch: 10
    Epoch: 11
    Epoch: 12
    Epoch: 13
    Epoch: 14
    Epoch: 15
    Epoch: 16
    Epoch: 17
    Epoch: 18
    Epoch: 19
    Epoch: 20
    Epoch: 21
    Epoch: 22
    Epoch: 23
    Epoch: 24
    Epoch: 25
    Epoch: 26
    Epoch: 27
    Epoch: 28
    Epoch: 29
    Epoch: 30
    Epoch: 31
    Epoch: 32
    Epoch: 33
    Epoch: 34
    Epoch: 35
    Epoch: 36
    Epoch: 37
    Epoch: 38
    Epoch: 39
    Epoch: 40
    Epoch: 41
    Epoch: 42
    Epoch: 43
    Epoch: 44
    Epoch: 45
    Epoch: 46
    Epoch: 47
    Epoch: 48
    Epoch: 49
    Epoch: 50
    Epoch: 51
    Epoch: 52
    Epoch: 53
    Epoch: 54
    Epoch: 55
    Epoch: 56
    Epoch: 57
    Epoch: 58
    Epoch: 59
    Epoch: 60
    Epoch: 61
    Epoch: 62
    Epoch: 63
    Epoch: 64
    Epoch: 65
    Epoch: 66
    Epoch: 67
    Epoch: 68
    Epoch: 69
    Epoch: 70
    Epoch: 71
    Epoch: 72
    Epoch: 73
    Epoch: 74
    Epoch: 75
    Epoch: 76
    Epoch: 77
    Epoch: 78
    Epoch: 79
    Epoch: 80
    Epoch: 81
    Epoch: 82
    Epoch: 83
    Epoch: 84
    Epoch: 85
    Epoch: 86
    Epoch: 87
    Epoch: 88
    Epoch: 89
    Epoch: 90
    Epoch: 91
    Epoch: 92
    Epoch: 93
    Epoch: 94
    Epoch: 95
    Epoch: 96
    Epoch: 97
    Epoch: 98
    Epoch: 99
Saving Coach to: models.ignore/coach/AntBulletEnv-v0/final_1_80%/


-----------------------------------------------------------------------------------------------------
 Testing Agent+Coach
-----------------------------------------------------------------------------------------------------


test settings = {
    "force_recompute": True, 
    "graph_smoothing": 0, 
    "number_of_episodes_for_baseline": 30, 
    "number_of_epochs_for_optimal_parameters": 15, 
    "number_of_episodes_for_testing": 10, 
    "initial_epsilon": 10.1, 
    "initial_horizon": 16, 
    "reward_discount": 0.98, 
    "acceptable_performance_loss": 1, 
    "acceptable_performance_level": 0.8, 
    "confidence_interval_for_convergence": 90, 
    "min_reward_single_timestep": 19, 
    "max_reward_single_timestep": 105, 
    "horizons": {0: 1, 0.0007: 4, 0.0015: 16, 0.002: 26, }, 
}
----- getting a reward baseline -----------------------------------------------------------------------------------------------------------------------------------
  episode_index=0, episode_discounted_reward_sum=103.91215052474561
  episode_index=1, episode_discounted_reward_sum=104.90401747079912
  episode_index=2, episode_discounted_reward_sum=106.78994313873307
  episode_index=3, episode_discounted_reward_sum=102.27590102814041
  episode_index=4, episode_discounted_reward_sum=102.11691695647527
  episode_index=5, episode_discounted_reward_sum=102.14992360991728
  episode_index=6, episode_discounted_reward_sum=99.29898209858057
  episode_index=7, episode_discounted_reward_sum=102.1386197717213
  episode_index=8, episode_discounted_reward_sum=102.99004265634343
  episode_index=9, episode_discounted_reward_sum=102.00561453106954
  episode_index=10, episode_discounted_reward_sum=103.34479094167229
  episode_index=11, episode_discounted_reward_sum=103.5537586686248
  episode_index=12, episode_discounted_reward_sum=102.24882783722319
  episode_index=13, episode_discounted_reward_sum=99.8868981836472
  episode_index=14, episode_discounted_reward_sum=101.41369426691955
  episode_index=15, episode_discounted_reward_sum=101.42780521847143
  episode_index=16, episode_discounted_reward_sum=100.22375888942986
  episode_index=17, episode_discounted_reward_sum=105.89586703831006
  episode_index=18, episode_discounted_reward_sum=101.7762522357355
  episode_index=19, episode_discounted_reward_sum=101.67719125029545
  episode_index=20, episode_discounted_reward_sum=103.4198317738007
  episode_index=21, episode_discounted_reward_sum=101.80097904212052
  episode_index=22, episode_discounted_reward_sum=101.02071470695418
  episode_index=23, episode_discounted_reward_sum=104.96427336624542
  episode_index=24, episode_discounted_reward_sum=99.36758611143887
  episode_index=25, episode_discounted_reward_sum=102.77364330195661
  episode_index=26, episode_discounted_reward_sum=102.2300794836445
  episode_index=27, episode_discounted_reward_sum=103.24298813162238
  episode_index=28, episode_discounted_reward_sum=100.6310702540393
  episode_index=29, episode_discounted_reward_sum=102.48120757127847
baseline = {
    "max": 106.78994313873307, 
    "min": 99.29898209858057, 
    "range": 7.490961040152499, 
    "count": 30, 
    "sum": 3071.963330059956, 
    "average": 102.3987776686652, 
    "stdev": 1.7578467076894742, 
    "median": 102.19000154678089, 
}
baseline_min = 101.85346420501257, baseline_max = 102.94409113231781,
baseline_confidence_size = 0.5453134636526187
----- finding optimal epsilon -------------------------------------------------------------------------------------------------------------------------------------
            reward_single_sum=31.754624042856403, 
            reward_single_sum=17.765154389061742, confidence_size=44.16301760895388, confidence_max=68.92290682491293, new_horizon=2
        episode=0, horizon=32, effective_score=24.76, baseline_lowerbound=81.92 baseline_stdev=0.32, new_epsilon=6.7333, bad=True, gap_average=0.5706174783706665
            reward_single_sum=31.740771331044407, 
            reward_single_sum=32.49460714942976, confidence_size=2.3797660201208686, confidence_max=34.49745526035795, new_horizon=45
        episode=1, horizon=32, effective_score=32.12, baseline_lowerbound=81.92 baseline_stdev=0.32, new_epsilon=4.4889, bad=True, gap_average=0.5601553955078125
            reward_single_sum=46.53374578593435, 
            reward_single_sum=45.66987281633689, confidence_size=2.727139635195769, confidence_max=48.828948936331386, new_horizon=41
        episode=2, horizon=32, effective_score=46.10, baseline_lowerbound=81.92 baseline_stdev=0.32, new_epsilon=2.9926, bad=True, gap_average=0.7266728668212891
            reward_single_sum=18.29430047131475, 
            reward_single_sum=51.436027302729244, confidence_size=104.624313992481, confidence_max=139.48947787950294, new_horizon=6
            reward_single_sum=23.380239445319642, confidence_size=30.089614036337547, confidence_max=61.12646977612542, new_horizon=8
        episode=3, horizon=32, effective_score=31.04, baseline_lowerbound=81.92 baseline_stdev=0.32, new_epsilon=1.9951, bad=True, gap_average=0.46537994956970213
            reward_single_sum=53.37761475974006, 
            reward_single_sum=66.8669548216261, confidence_size=42.58417062469894, confidence_max=102.706455415382, new_horizon=2
            reward_single_sum=62.544913680000455, confidence_size=11.612460050035601, confidence_max=72.54228780382446, new_horizon=2
        episode=4, horizon=32, effective_score=60.93, baseline_lowerbound=81.92 baseline_stdev=0.32, new_epsilon=1.3300, bad=True, gap_average=1.7098264821370444
            reward_single_sum=75.98069166701443, 
            reward_single_sum=59.23691191246684, confidence_size=52.85803239438404, confidence_max=120.46683418412464, new_horizon=2
            reward_single_sum=54.00078074874081, confidence_size=19.355387426378215, confidence_max=82.42818220245223, new_horizon=2
        episode=5, horizon=32, effective_score=63.07, baseline_lowerbound=81.92 baseline_stdev=0.32, new_epsilon=0.8867, bad=True, gap_average=1.6456014982859293
            reward_single_sum=74.53812095787443, 
            reward_single_sum=71.11925223544081, confidence_size=10.79294378758539, confidence_max=83.621630384243, new_horizon=10
        episode=6, horizon=32, effective_score=72.83, baseline_lowerbound=81.92 baseline_stdev=0.32, new_epsilon=0.5911, bad=True, gap_average=0.5083354659080506
            reward_single_sum=94.49413601896568, 
            reward_single_sum=86.88605041031747, confidence_size=24.017781018168918, confidence_max=114.70787423281047, new_horizon=8
            reward_single_sum=86.15554130180394, confidence_size=7.785069882356744, confidence_max=96.96364579271909, new_horizon=6
        episode=7, horizon=10, effective_score=89.18, baseline_lowerbound=81.92 baseline_stdev=0.32, new_epsilon=0.8867, bad=False, gap_average=0.4480544321537018
            reward_single_sum=67.76946513065663, 
            reward_single_sum=93.58143132306294, confidence_size=81.48517032364796, confidence_max=162.1606185505077, new_horizon=8
            reward_single_sum=89.85080256064843, confidence_size=23.519136521268486, confidence_max=107.25303619272448, new_horizon=8
            reward_single_sum=75.08302507428407, confidence_size=14.33720842735972, confidence_max=95.90838944952273, new_horizon=8
        episode=8, horizon=6, effective_score=81.57, baseline_lowerbound=81.92 baseline_stdev=0.32, new_epsilon=0.5911, bad=True, gap_average=0.5350716648101806
            reward_single_sum=78.30191686025664, 
            reward_single_sum=89.55216635343012, confidence_size=35.51563988970627, confidence_max=119.44268149654962, new_horizon=6
            reward_single_sum=96.16943370096848, confidence_size=15.228851582216755, confidence_max=103.2366905537685, new_horizon=6
            reward_single_sum=90.66454280353574, confidence_size=8.818443963515477, confidence_max=97.49045889306322, new_horizon=6
        episode=9, horizon=8, effective_score=88.67, baseline_lowerbound=81.92 baseline_stdev=0.32, new_epsilon=0.8867, bad=False, gap_average=0.3942138855457306
            reward_single_sum=90.90102901223466, 
            reward_single_sum=93.78891045425448, confidence_size=9.116682914559071, confidence_max=101.46165264780363, new_horizon=8
        episode=10, horizon=2, effective_score=92.34, baseline_lowerbound=81.92 baseline_stdev=0.32, new_epsilon=1.3300, bad=False, gap_average=0.5169257674217224
            reward_single_sum=73.55882465897352, 
            reward_single_sum=67.71931019096927, confidence_size=18.434621659031915, confidence_max=89.07368908400329, new_horizon=2
        episode=11, horizon=8, effective_score=70.64, baseline_lowerbound=81.92 baseline_stdev=0.32, new_epsilon=0.8867, bad=True, gap_average=1.118807508945465
            reward_single_sum=83.11975050127457, 
            reward_single_sum=70.64896359891425, confidence_size=39.36872484776856, confidence_max=116.25308189786296, new_horizon=8
            reward_single_sum=79.45190090441082, confidence_size=10.804940796025988, confidence_max=88.5451457975592, new_horizon=6
        episode=12, horizon=6, effective_score=77.74, baseline_lowerbound=81.92 baseline_stdev=0.32, new_epsilon=0.5911, bad=True, gap_average=0.5813831650416056
            reward_single_sum=87.52934007182061, 
            reward_single_sum=61.45710222520941, confidence_size=82.30681559914589, confidence_max=156.80003674766084, new_horizon=6
            reward_single_sum=83.99917808301228, confidence_size=23.845245941057804, confidence_max=101.5071194010719, new_horizon=6
        episode=13, horizon=32, effective_score=77.66, baseline_lowerbound=81.92 baseline_stdev=0.32, new_epsilon=0.3941, bad=True, gap_average=0.429815064907074
            reward_single_sum=90.88925082040413, 
            reward_single_sum=95.15917962985887, confidence_size=13.479634744393515, confidence_max=106.503849969525, new_horizon=6
            reward_single_sum=85.68506034026203, confidence_size=7.998925724403016, confidence_max=98.57675598791135, new_horizon=6
        episode=14, horizon=6, effective_score=90.58, baseline_lowerbound=81.92 baseline_stdev=0.32, new_epsilon=0.5911, bad=False, gap_average=0.31918132972717284
optimal_epsilon = 0.8866941015089163
optimal_horizon = 6
    scaled_epsilon: 0.8867, forecast_average: 2.8340, episode_reward:2417.73, max_timestep_reward: 3.26, min_timestep_reward: -0.35
    scaled_epsilon: 0.8867, forecast_average: 2.8224, episode_reward:2497.48, max_timestep_reward: 3.42, min_timestep_reward: -0.22
    scaled_epsilon: 0.8867, forecast_average: 2.5832, episode_reward:1854.46, max_timestep_reward: 3.43, min_timestep_reward: -0.14
    scaled_epsilon: 0.8867, forecast_average: 2.6962, episode_reward:2654.46, max_timestep_reward: 3.47, min_timestep_reward: -0.20
    scaled_epsilon: 0.8867, forecast_average: 2.6994, episode_reward:2431.59, max_timestep_reward: 3.52, min_timestep_reward: -0.17
    scaled_epsilon: 0.8867, forecast_average: 2.7577, episode_reward:2735.60, max_timestep_reward: 3.50, min_timestep_reward: -0.26
    scaled_epsilon: 0.8867, forecast_average: 2.7713, episode_reward:2505.88, max_timestep_reward: 3.38, min_timestep_reward: -0.23
    scaled_epsilon: 0.8867, forecast_average: 2.7619, episode_reward:2414.10, max_timestep_reward: 3.33, min_timestep_reward: -0.30
    scaled_epsilon: 0.8867, forecast_average: 2.7181, episode_reward:2347.32, max_timestep_reward: 3.40, min_timestep_reward: -0.26
    scaled_epsilon: 0.8867, forecast_average: 2.7215, episode_reward:2510.78, max_timestep_reward: 3.37, min_timestep_reward: -0.20
self.recorder = {
    number_of_records: 0,
    records: [ ... ],
    local_data: {},
    parent_data:    {
    }
}
/home/jeffhykin/repos/AFRL/.venv/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(
argv[0]=
argv[0]=
argv[0]=
argv[0]=


-------------------------------------------------------

 Environment: ReacherBulletEnv-v0

-------------------------------------------------------




-----------------------------------------------------------------------------------------------------
 Agent Model Exists, loading: ./subrepos/rl-trained-agents/sac/ReacherBulletEnv-v0_1/ReacherBulletEnv-v0.zip
-----------------------------------------------------------------------------------------------------


pybullet build time: Oct 11 2021 20:52:37
Wrapping the env with a `Monitor` wrapper
Wrapping the env in a DummyVecEnv.


-----------------------------------------------------------------------------------------------------
 Coach Model Exists, loading: models.ignore/coach/ReacherBulletEnv-v0/final_1_80%/
-----------------------------------------------------------------------------------------------------


coach settings = {
    "force_retrain": False, 
    "learning_rate": 0.0001, 
    "hidden_sizes": [64, 64, 64, 64, ], 
    "loss_function": "state_prediction_loss", 
    "consistent_coach_loss": {"scale_future_state_loss": 1000000, }, 
    "delayed_coach_loss": {"epochs_of_delay": 50, }, 
    "value_plus_state_loss": {"value_proportion": 0.5, }, 
    "number_of_episodes": 100, 
    "number_of_epochs": 100, 
    "train_test_split": 0.7, 
    "batch_size": 128, 
    "with_card": True, 
}


-----------------------------------------------------------------------------------------------------
 Testing Agent+Coach
-----------------------------------------------------------------------------------------------------


test settings = {
    "force_recompute": True, 
    "graph_smoothing": 0, 
    "number_of_episodes_for_baseline": 30, 
    "number_of_epochs_for_optimal_parameters": 15, 
    "number_of_episodes_for_testing": 10, 
    "initial_epsilon": 10.1, 
    "initial_horizon": 16, 
    "reward_discount": 0.98, 
    "acceptable_performance_loss": 1, 
    "acceptable_performance_level": 0.8, 
    "confidence_interval_for_convergence": 90, 
}
----- getting a reward baseline -----------------------------------------------------------------------------------------------------------------------------------
argv[0]=
argv[0]=
  episode_index=0, episode_discounted_reward_sum=14.803850842664142
  episode_index=1, episode_discounted_reward_sum=8.67710297272466
  episode_index=2, episode_discounted_reward_sum=25.842017543141473
  episode_index=3, episode_discounted_reward_sum=8.664663072046254
  episode_index=4, episode_discounted_reward_sum=3.3982188562282274
  episode_index=5, episode_discounted_reward_sum=5.999554355456983
  episode_index=6, episode_discounted_reward_sum=20.761382727030036
  episode_index=7, episode_discounted_reward_sum=16.208572410106616
  episode_index=8, episode_discounted_reward_sum=13.795809773164686
  episode_index=9, episode_discounted_reward_sum=16.45301248311772
  episode_index=10, episode_discounted_reward_sum=9.971182261073299
  episode_index=11, episode_discounted_reward_sum=6.234918821260175
  episode_index=12, episode_discounted_reward_sum=14.942134725729922
  episode_index=13, episode_discounted_reward_sum=20.37324919053046
  episode_index=14, episode_discounted_reward_sum=24.303996522251616
  episode_index=15, episode_discounted_reward_sum=6.0420670186839285
  episode_index=16, episode_discounted_reward_sum=4.733517044898386
  episode_index=17, episode_discounted_reward_sum=20.209552335388256
  episode_index=18, episode_discounted_reward_sum=16.623863732397258
  episode_index=19, episode_discounted_reward_sum=-0.026951967968909276
  episode_index=20, episode_discounted_reward_sum=9.37630738511291
  episode_index=21, episode_discounted_reward_sum=4.389928320667925
  episode_index=22, episode_discounted_reward_sum=12.315845837562701
  episode_index=23, episode_discounted_reward_sum=18.790536285667738
  episode_index=24, episode_discounted_reward_sum=28.474230749393342
  episode_index=25, episode_discounted_reward_sum=12.893075096018705
  episode_index=26, episode_discounted_reward_sum=23.33850302320432
  episode_index=27, episode_discounted_reward_sum=10.42889637762907
  episode_index=28, episode_discounted_reward_sum=18.62138046960204
  episode_index=29, episode_discounted_reward_sum=8.896873522397392
baseline = {
    "max": 28.474230749393342, 
    "min": -0.026951967968909276, 
    "range": 28.50118271736225, 
    "count": 30, 
    "sum": 405.53729178718135, 
    "average": 13.517909726239377, 
    "stdev": 7.2800413875813526, 
    "median": 13.344442434591695, 
}
baseline_min = 11.259519067241325, baseline_max = 15.776300385237429,
baseline_confidence_size = 2.258390658998052
----- finding optimal epsilon -------------------------------------------------------------------------------------------------------------------------------------
            reward_single_sum=-7.91143200079689, 
            reward_single_sum=0.30053335352282823, confidence_size=25.924154347664462, confidence_max=22.118705024027417, new_horizon=2
            reward_single_sum=-6.355829352447148, confidence_size=7.353735003323546, confidence_max=2.6981590034164737, new_horizon=2
        episode=0, horizon=32, effective_score=-4.66, baseline_lowerbound=10.81 baseline_stdev=1.33, new_epsilon=6.7333, bad=True, gap_average=0.5987032158507242
            reward_single_sum=-2.7022100087204644, 
            reward_single_sum=7.5588454667209355, confidence_size=32.392877275762274, confidence_max=34.821195004762494, new_horizon=2
            reward_single_sum=2.9860763714057805, confidence_size=8.666343572771734, confidence_max=11.280580849240481, new_horizon=2
            reward_single_sum=-0.3722621828766932, confidence_size=5.242131818700565, confidence_max=7.109744230332953, new_horizon=2
        episode=1, horizon=32, effective_score=1.87, baseline_lowerbound=10.81 baseline_stdev=1.33, new_epsilon=4.4889, bad=True, gap_average=0.4798487629989783
            reward_single_sum=-10.359240945631077, 
            reward_single_sum=10.670718559918521, confidence_size=66.38896934218306, confidence_max=66.54470814932674, new_horizon=2
            reward_single_sum=-8.621115645085881, confidence_size=19.677794445086533, confidence_max=16.90791510148705, new_horizon=2
            reward_single_sum=7.0691436702636326, confidence_size=12.620148274369967, confidence_max=12.310024684236263, new_horizon=2
            reward_single_sum=-0.936701178763897, confidence_size=8.859406185996352, confidence_max=8.42396707813661, new_horizon=2
        episode=2, horizon=32, effective_score=-0.44, baseline_lowerbound=10.81 baseline_stdev=1.33, new_epsilon=2.9926, bad=True, gap_average=0.47889355619748436
            reward_single_sum=4.926802458436758, 
            reward_single_sum=-0.9630717478197981, confidence_size=18.593601095869644, confidence_max=20.575466451178112, new_horizon=2
            reward_single_sum=3.168532863250913, confidence_size=5.097319115680342, confidence_max=7.474740306969632, new_horizon=2
        episode=3, horizon=32, effective_score=2.38, baseline_lowerbound=10.81 baseline_stdev=1.33, new_epsilon=1.9951, bad=True, gap_average=0.44749332884947457
            reward_single_sum=-1.35920232836592, 
            reward_single_sum=4.823194235339933, confidence_size=19.517057834598965, confidence_max=21.24905378808596, new_horizon=3
            reward_single_sum=-5.898918224648592, confidence_size=9.0732478072183, confidence_max=8.26160570132677, new_horizon=4
        episode=4, horizon=32, effective_score=-0.81, baseline_lowerbound=10.81 baseline_stdev=1.33, new_epsilon=1.3300, bad=True, gap_average=0.5769310510820813
            reward_single_sum=3.332920447581857, 
            reward_single_sum=10.70123959676193, confidence_size=23.260868094836212, confidence_max=30.27794811700809, new_horizon=26
            reward_single_sum=9.849120917519999, confidence_size=6.795168240936682, confidence_max=14.756261894891274, new_horizon=22
            reward_single_sum=3.981992030016855, confidence_size=4.525151917226131, confidence_max=11.49147016519629, new_horizon=10
            reward_single_sum=-2.9332979101621923, confidence_size=5.281860490554497, confidence_max=10.268255506898186, new_horizon=4
        episode=5, horizon=32, effective_score=4.99, baseline_lowerbound=10.81 baseline_stdev=1.33, new_epsilon=0.8867, bad=True, gap_average=0.4782302269538244
            reward_single_sum=0.030081827976688305, 
            reward_single_sum=6.770644312098068, confidence_size=21.279118297365862, confidence_max=24.679481367403227, new_horizon=8
            reward_single_sum=2.0611552924106205, confidence_size=5.829406215235288, confidence_max=8.78336669273041, new_horizon=8
        episode=6, horizon=32, effective_score=2.95, baseline_lowerbound=10.81 baseline_stdev=1.33, new_epsilon=0.5911, bad=True, gap_average=0.3763163506984711
            reward_single_sum=6.628565219293014, 
            reward_single_sum=3.923165687721213, confidence_size=8.5406101953016, confidence_max=13.816475648808709, new_horizon=8
            reward_single_sum=17.931006146827645, confidence_size=12.526930078973269, confidence_max=22.021175763587223, new_horizon=6
            reward_single_sum=15.093845579306157, confidence_size=7.862506559051045, confidence_max=18.75665221733805, new_horizon=6
            reward_single_sum=7.750783918234186, confidence_size=5.677464641880807, confidence_max=15.942937952157248, new_horizon=6
            reward_single_sum=2.328497639353517, confidence_size=5.128756925347312, confidence_max=14.0714009571366, new_horizon=6
            reward_single_sum=5.430811213372959, confidence_size=4.292171433990163, confidence_max=12.733125063148545, new_horizon=4
            reward_single_sum=17.24282747025, confidence_size=4.18085939171729, confidence_max=13.722047251012125, new_horizon=5
            reward_single_sum=17.85218249608124, confidence_size=4.005729295541686, confidence_max=14.470361003368343, new_horizon=6
            reward_single_sum=4.760927588943585, confidence_size=3.683408760875036, confidence_max=13.577670056813387, new_horizon=6
            reward_single_sum=3.2125424547955843, confidence_size=3.4733330910927043, confidence_max=12.760165401472621, new_horizon=6
            reward_single_sum=4.7239557026735675, confidence_size=3.215062331288499, confidence_max=12.121654924359554, new_horizon=6
            reward_single_sum=8.93519286431573, confidence_size=2.935038123141478, confidence_max=11.843830737077509, new_horizon=6
            reward_single_sum=1.9838242671963384, confidence_size=2.838550353118866, confidence_max=11.252702370859204, new_horizon=4
        episode=7, horizon=32, effective_score=8.41, baseline_lowerbound=10.81 baseline_stdev=1.33, new_epsilon=0.3941, bad=True, gap_average=0.2763364315600622
            reward_single_sum=4.331617937326026, 
            reward_single_sum=16.792979373196104, confidence_size=39.33896982110333, confidence_max=49.90126847636437, new_horizon=15
            reward_single_sum=5.8209615874271, confidence_size=11.473079769882212, confidence_max=20.454932735865285, new_horizon=14
            reward_single_sum=15.822816517245748, confidence_size=7.677903136007496, confidence_max=18.36999698980624, new_horizon=14
            reward_single_sum=9.66993972709646, confidence_size=5.405073370345775, confidence_max=15.892736398804061, new_horizon=12
            reward_single_sum=13.152284997934999, confidence_size=4.266344620388447, confidence_max=15.198111310426187, new_horizon=11
            reward_single_sum=15.676395520652955, confidence_size=3.7182114283619017, confidence_max=15.327782237058956, new_horizon=12
            reward_single_sum=20.59247019587072, confidence_size=3.7923941928615434, confidence_max=16.52482742495531, new_horizon=11
            reward_single_sum=5.424585540375588, confidence_size=3.6133393633005992, confidence_max=15.533789518536787, new_horizon=12
            reward_single_sum=13.670945931274826, confidence_size=3.202044114846002, confidence_max=15.297543847686054, new_horizon=10
            reward_single_sum=0.5290570985134706, confidence_size=3.4399104901902655, confidence_max=14.483915438091538, new_horizon=10
            reward_single_sum=5.516922963380483, confidence_size=3.2195466998844506, confidence_max=13.80296148240899, new_horizon=10
            reward_single_sum=6.630678795944787, confidence_size=2.988670990264609, confidence_max=13.26802915843686, new_horizon=10
            reward_single_sum=-2.519804093215175, confidence_size=3.1906368710102395, confidence_max=12.555769163369103, new_horizon=10
            reward_single_sum=23.003411023731136, confidence_size=3.3603147990126225, confidence_max=13.63466567346297, new_horizon=8
            reward_single_sum=-1.4441690728171526, confidence_size=3.381762954928844, confidence_max=12.923706332674973, new_horizon=8
            reward_single_sum=6.259286515512478, confidence_size=3.1815386536166574, confidence_max=12.530384568878455, new_horizon=8
            reward_single_sum=1.518657404891086, confidence_size=3.083113037615571, confidence_max=11.996948480078995, new_horizon=10
            reward_single_sum=0.13853153048807462, confidence_size=3.0153457943423065, confidence_max=11.467323136175448, new_horizon=10
            reward_single_sum=15.430262499366007, confidence_size=2.915579237740496, confidence_max=11.71647083745028, new_horizon=10
            reward_single_sum=2.7027044260513757, confidence_size=2.811163723414316, confidence_max=11.321665457711797, new_horizon=10
            reward_single_sum=-4.425515260172324, confidence_size=2.8591738456148406, confidence_max=10.78167480743642, new_horizon=10
        episode=8, horizon=32, effective_score=7.92, baseline_lowerbound=10.81 baseline_stdev=1.33, new_epsilon=0.2627, bad=True, gap_average=0.16213711907466252
            reward_single_sum=9.991552294305277, 
            reward_single_sum=6.772309367258047, confidence_size=10.162749953578325, confidence_max=18.544680784359983, new_horizon=8
            reward_single_sum=9.737602092269096, confidence_size=3.0173950952973354, confidence_max=11.851216346574809, new_horizon=10
            reward_single_sum=11.34421578244618, confidence_size=2.266810751890191, confidence_max=11.728230635959841, new_horizon=8
            reward_single_sum=15.274377194334262, confidence_size=2.9449567439560047, confidence_max=13.568968090078576, new_horizon=9
            reward_single_sum=23.728924547604468, confidence_size=4.9533801427884985, confidence_max=17.761543689158053, new_horizon=8
            reward_single_sum=3.9112464402806695, confidence_size=4.732606864159052, confidence_max=16.26978225251591, new_horizon=8
            reward_single_sum=11.794097986116421, confidence_size=3.9965102640655172, confidence_max=15.565800977142318, new_horizon=8
            reward_single_sum=2.0849061304856167, confidence_size=3.9758955984975843, confidence_max=14.491365802397588, new_horizon=6
            reward_single_sum=14.48395741478252, confidence_size=3.5802806104993694, confidence_max=14.492599535487624, new_horizon=6
            reward_single_sum=9.342167100957049, confidence_size=3.2124355787147527, confidence_max=13.98201342879108, new_horizon=6
            reward_single_sum=14.314985066193252, confidence_size=2.9537659144573114, confidence_max=14.018794365876715, new_horizon=6
            reward_single_sum=-2.7388016916525704, confidence_size=3.294332230065466, confidence_max=13.297527593556257, new_horizon=6
            reward_single_sum=7.683152188731678, confidence_size=3.044705839411311, confidence_max=12.882183833276452, new_horizon=6
            reward_single_sum=23.782270495431735, confidence_size=3.260095933110014, confidence_max=14.027226760412928, new_horizon=6
            reward_single_sum=10.570177186328285, confidence_size=3.0353161953951506, confidence_max=13.79013742013715, new_horizon=6
            reward_single_sum=16.933632932199036, confidence_size=2.90956666315547, confidence_max=14.027847400100825, new_horizon=6
            reward_single_sum=15.982106463335882, confidence_size=2.773428678700764, confidence_max=14.161921956001148, new_horizon=6
            reward_single_sum=20.538226645993685, confidence_size=2.745136600544723, confidence_max=14.615194791986859, new_horizon=6
            reward_single_sum=12.850777954013093, confidence_size=2.5982438983032408, confidence_max=14.517338077873925, new_horizon=6
            reward_single_sum=1.8666221685781303, confidence_size=2.599693267349828, confidence_max=14.040098303539914, new_horizon=6
            reward_single_sum=26.821746449875263, confidence_size=2.750103501610325, confidence_max=14.889660420240645, new_horizon=6
            reward_single_sum=3.2472096325311726, confidence_size=2.7050522414569853, confidence_max=14.457985365039518, new_horizon=6
            reward_single_sum=6.602488787337958, confidence_size=2.6109891466392288, confidence_max=14.149320422878237, new_horizon=6
            reward_single_sum=1.5270584104253255, confidence_size=2.5921840005295786, confidence_max=13.730064362136039, new_horizon=6
            reward_single_sum=14.400226616904277, confidence_size=2.495719507074708, confidence_max=13.75907472465416, new_horizon=6
            reward_single_sum=13.42389891018447, confidence_size=2.4018412617794827, confidence_max=13.745216616122086, new_horizon=6
            reward_single_sum=4.92016926988385, confidence_size=2.3441059614289665, confidence_max=13.45808109846947, new_horizon=6
            reward_single_sum=16.010253270507768, confidence_size=2.277151432237421, confidence_max=13.559963746294038, new_horizon=6
            reward_single_sum=11.918129331509025, confidence_size=2.1976403930179558, confidence_max=13.501629940989654, new_horizon=6
        episode=9, horizon=10, effective_score=11.30, baseline_lowerbound=10.81 baseline_stdev=1.33, new_epsilon=0.3941, bad=False, gap_average=0.12252598975102107
            reward_single_sum=16.588108468330688, 
            reward_single_sum=21.240758955069854, confidence_size=14.687839529244359, confidence_max=33.60227324094462, new_horizon=10
            reward_single_sum=12.503801448634023, confidence_size=7.369811543940356, confidence_max=24.147367834618542, new_horizon=10
            reward_single_sum=20.968005842557254, confidence_size=4.870142842619452, confidence_max=22.695311521267406, new_horizon=10
            reward_single_sum=21.79893426656308, confidence_size=3.8142665143433456, confidence_max=22.434188310574324, new_horizon=10
            reward_single_sum=12.058079673641524, confidence_size=3.6772116498549696, confidence_max=21.20349309232104, new_horizon=10
            reward_single_sum=16.481372683069765, confidence_size=3.0109731333280862, confidence_max=20.387981895880397, new_horizon=10
            reward_single_sum=0.15147491922237066, confidence_size=4.806768681050474, confidence_max=20.030585713186543, new_horizon=8
            reward_single_sum=25.398330694363032, confidence_size=4.661706491918196, confidence_max=21.016025042079484, new_horizon=8
            reward_single_sum=17.71596144264146, confidence_size=4.11785500859531, confidence_max=20.608337848004616, new_horizon=8
            reward_single_sum=23.418679203185842, confidence_size=3.8556441252030123, confidence_max=20.97596299768291, new_horizon=8
            reward_single_sum=20.401241925964218, confidence_size=3.52191080921161, confidence_max=20.915639936148533, new_horizon=8
            reward_single_sum=-7.98624677840426, confidence_size=4.737578593259433, confidence_max=20.17900188132396, new_horizon=8
            reward_single_sum=15.201055831123464, confidence_size=4.358309369181766, confidence_max=19.782563553179074, new_horizon=8
            reward_single_sum=12.334268369256424, confidence_size=4.051596158109344, confidence_max=19.269851287790594, new_horizon=8
            reward_single_sum=8.93970572270369, confidence_size=3.834361217474008, confidence_max=18.66020700921916, new_horizon=8
            reward_single_sum=25.00590556205623, confidence_size=3.7362846934085434, confidence_max=19.160957530466113, new_horizon=8
            reward_single_sum=25.67403273625299, confidence_size=3.6470331444908695, confidence_max=19.64111486483707, new_horizon=8
            reward_single_sum=1.8511724020398848, confidence_size=3.6730284041147687, confidence_max=18.92274647612906, new_horizon=8
            reward_single_sum=-0.31487505526816434, confidence_size=3.7261076162426185, confidence_max=18.197596031892786, new_horizon=8
            reward_single_sum=7.614423106926598, confidence_size=3.579762821171009, confidence_max=17.72472431735815, new_horizon=8
            reward_single_sum=7.71259854544202, confidence_size=3.442267878799698, confidence_max=17.29484924086206, new_horizon=8
            reward_single_sum=1.0456721584679114, confidence_size=3.4187498848053686, confidence_max=16.714509107581016, new_horizon=8
            reward_single_sum=16.51014715886607, confidence_size=3.2750193220893244, confidence_max=16.704711375535407, new_horizon=8
            reward_single_sum=18.521879293008684, confidence_size=3.1551132606359165, confidence_max=16.7884928036645, new_horizon=8
            reward_single_sum=18.680432108072825, confidence_size=3.0445874154181247, confidence_max=16.872084364794567, new_horizon=8
            reward_single_sum=12.107810926106277, confidence_size=2.9273450617467844, confidence_max=16.691149936187294, new_horizon=8
            reward_single_sum=1.0412118244438495, confidence_size=2.9213885610992305, confidence_max=16.230815112325573, new_horizon=8
            reward_single_sum=5.087589409996153, confidence_size=2.8562932504739766, confidence_max=15.882208176140658, new_horizon=8
            reward_single_sum=21.27241873493033, confidence_size=2.795484729808167, confidence_max=16.096283115783635, new_horizon=8
        hit cap of: 30 iterations
        episode=10, horizon=4, effective_score=13.30, baseline_lowerbound=10.81 baseline_stdev=1.33, new_epsilon=0.5911, bad=False, gap_average=0.17249580992261568
            reward_single_sum=17.55782818840898, 
            reward_single_sum=16.1328728304117, confidence_size=4.498407025039519, confidence_max=21.343757534449853, new_horizon=4
            reward_single_sum=1.838313328488775, confidence_size=14.656079552298923, confidence_max=26.499084334735404, new_horizon=4
            reward_single_sum=13.518696964438021, confidence_size=8.410379630895235, confidence_max=20.6723074588321, new_horizon=6
            reward_single_sum=0.8336575846900096, confidence_size=7.65309645281624, confidence_max=17.629370232103735, new_horizon=6
            reward_single_sum=3.328785850257594, confidence_size=6.314216788456871, confidence_max=15.18257591290605, new_horizon=6
            reward_single_sum=3.952957769246268, confidence_size=5.3239837922366195, confidence_max=13.490142723085384, new_horizon=6
            reward_single_sum=6.24905321326089, confidence_size=4.518253461611205, confidence_max=12.444774177761484, new_horizon=6
            reward_single_sum=10.692641882353241, confidence_size=3.9525861593287943, confidence_max=12.186453671723847, new_horizon=6
            reward_single_sum=21.78628318327153, confidence_size=4.2798745377491425, confidence_max=13.86898361723184, new_horizon=6
            reward_single_sum=4.911126146549859, confidence_size=3.9045146175485312, confidence_max=13.068352521310064, new_horizon=6
            reward_single_sum=11.346595033435888, confidence_size=3.5467947540846594, confidence_max=12.892529085319055, new_horizon=6
            reward_single_sum=4.483881560546864, confidence_size=3.305771878048266, confidence_max=12.27751753461439, new_horizon=6
            reward_single_sum=16.403010914132928, confidence_size=3.1830234458691367, confidence_max=12.685573763690032, new_horizon=6
            reward_single_sum=-1.09775472746555, confidence_size=3.1991950124970843, confidence_max=11.995058327298883, new_horizon=6
            reward_single_sum=4.482648967187143, confidence_size=3.0157962150503312, confidence_max=11.542083633126214, new_horizon=6
            reward_single_sum=15.969425619430453, confidence_size=2.9229874160537275, confidence_max=11.887106493032821, new_horizon=6
            reward_single_sum=3.0484780991329763, confidence_size=2.804797052165073, confidence_max=11.440269408152716, new_horizon=6
            reward_single_sum=-0.17760251234431038, confidence_size=2.7642300981014083, confidence_max=10.935856408387368, new_horizon=6
        episode=11, horizon=8, effective_score=8.17, baseline_lowerbound=10.81 baseline_stdev=1.33, new_epsilon=0.3941, bad=True, gap_average=0.25229130443250924
            reward_single_sum=5.485552400164459, 
            reward_single_sum=7.582085662293621, confidence_size=6.61849502979927, confidence_max=13.152314061028306, new_horizon=8
            reward_single_sum=3.2873373240915074, confidence_size=3.620498517256758, confidence_max=9.072156979439953, new_horizon=8
        episode=12, horizon=6, effective_score=5.45, baseline_lowerbound=10.81 baseline_stdev=1.33, new_epsilon=0.2627, bad=True, gap_average=0.16862362715933057
            reward_single_sum=24.01515138852713, 
            reward_single_sum=9.281735439389672, confidence_size=46.51156363352945, confidence_max=63.160007047487824, new_horizon=6
            reward_single_sum=1.4300728614847458, confidence_size=19.329921318720913, confidence_max=30.905574548521425, new_horizon=6
            reward_single_sum=14.829372799057072, confidence_size=11.181076945038967, confidence_max=23.57016006715362, new_horizon=6
            reward_single_sum=23.592407416546546, confidence_size=9.185360312987498, confidence_max=23.81510829398853, new_horizon=6
            reward_single_sum=17.071309953493632, confidence_size=7.136185634701148, confidence_max=22.172860611117613, new_horizon=6
            reward_single_sum=14.694904581254695, confidence_size=5.816845301496707, confidence_max=20.804695935747205, new_horizon=6
            reward_single_sum=19.665639901480667, confidence_size=5.034923665418063, confidence_max=20.607497958072333, new_horizon=6
            reward_single_sum=12.337641468770842, confidence_size=4.409237634050676, confidence_max=19.622374946273453, new_horizon=6
            reward_single_sum=4.337823000694056, confidence_size=4.3690222619069115, confidence_max=18.494628142976815, new_horizon=6
            reward_single_sum=1.7998519149513394, confidence_size=4.40368056791891, confidence_max=17.408763361159853, new_horizon=6
            reward_single_sum=12.605564017031291, confidence_size=3.9836748814646654, confidence_max=16.955464443354806, new_horizon=6
            reward_single_sum=3.424469604388062, confidence_size=3.8650877369963705, confidence_max=16.10246807138635, new_horizon=6
            reward_single_sum=10.71854528137032, confidence_size=3.5607666318397486, confidence_max=15.689658748156896, new_horizon=6
            reward_single_sum=3.239049017507715, confidence_size=3.458185171114044, confidence_max=14.994421080843896, new_horizon=6
            reward_single_sum=13.21790887545222, confidence_size=3.2249339264145442, confidence_max=14.866274396502044, new_horizon=6
            reward_single_sum=9.645347171085415, confidence_size=3.0238697798181153, confidence_max=14.547798879376082, new_horizon=6
            reward_single_sum=18.000801720711017, confidence_size=2.9088304417072797, confidence_max=14.79258579799597, new_horizon=6
            reward_single_sum=19.232426574520357, confidence_size=2.823523359363488, confidence_max=15.094050885032793, new_horizon=6
            reward_single_sum=17.30266554801007, confidence_size=2.706212863644046, confidence_max=15.228347290430388, new_horizon=6
            reward_single_sum=17.97745412330858, confidence_size=2.606348984958487, confidence_max=15.38826054015065, new_horizon=6
            reward_single_sum=25.432142232780958, confidence_size=2.669470191471617, confidence_max=16.026392232008725, new_horizon=6
            reward_single_sum=-1.577016758928132, confidence_size=2.7789080073470434, confidence_max=15.486528360950881, new_horizon=6
            reward_single_sum=20.60587877820872, confidence_size=2.7147685024889308, confidence_max=15.751482957117972, new_horizon=6
            reward_single_sum=13.6206289035821, confidence_size=2.599679691659733, confidence_max=15.659750724246896, new_horizon=6
            reward_single_sum=16.29786782642703, confidence_size=2.5027455119931217, confidence_max=15.687347190497203, new_horizon=6
            reward_single_sum=10.198399753861565, confidence_size=2.4120988905182186, confidence_max=15.486100497739242, new_horizon=6
            reward_single_sum=6.7304249333740005, confidence_size=2.3530401871606594, confidence_max=15.200485484601433, new_horizon=6
            reward_single_sum=26.26361936346814, confidence_size=2.4002603989809588, confidence_max=15.710332388353711, new_horizon=6
            reward_single_sum=16.931504563295178, confidence_size=2.3252043821351975, confidence_max=15.755990790638698, new_horizon=6
        hit cap of: 30 iterations
        episode=13, horizon=8, effective_score=13.43, baseline_lowerbound=10.81 baseline_stdev=1.33, new_epsilon=0.3941, bad=False, gap_average=0.13046957247455915
            reward_single_sum=22.887599576113143, 
            reward_single_sum=5.95807770195918, confidence_size=53.44439718889758, confidence_max=67.86723582793371, new_horizon=8
            reward_single_sum=22.29208802552124, confidence_size=16.195953268620297, confidence_max=33.241875036484814, new_horizon=8
            reward_single_sum=13.422334506777615, confidence_size=9.472970775936163, confidence_max=25.612995728528958, new_horizon=8
            reward_single_sum=0.08316364134786094, confidence_size=9.542172992076871, confidence_max=22.470825682420674, new_horizon=8
            reward_single_sum=10.066283686987918, confidence_size=7.4267714474702835, confidence_max=19.878362637254774, new_horizon=8
            reward_single_sum=6.65438597088311, confidence_size=6.263180730248633, confidence_max=17.886599745904356, new_horizon=8
            reward_single_sum=9.132714510084853, confidence_size=5.3212035795043136, confidence_max=16.633284531963678, new_horizon=8
            reward_single_sum=6.873081759746137, confidence_size=4.696516339486357, confidence_max=15.51537515942203, new_horizon=8
            reward_single_sum=13.940840360420093, confidence_size=4.18033470323244, confidence_max=15.311391677216555, new_horizon=8
            reward_single_sum=20.72793911431818, confidence_size=4.059305512336488, confidence_max=16.062806317260062, new_horizon=8
            reward_single_sum=9.570888688701867, confidence_size=3.689735610479363, confidence_max=15.490518739051128, new_horizon=8
            reward_single_sum=9.167494153651331, confidence_size=3.387657383669705, confidence_max=14.985879821862977, new_horizon=8
            reward_single_sum=15.56050486752747, confidence_size=3.156428596562444, confidence_max=15.037671208279587, new_horizon=8
            reward_single_sum=9.986741321077533, confidence_size=2.9309614858904807, confidence_max=14.685904011564983, new_horizon=8
            reward_single_sum=6.233394454087381, confidence_size=2.795062512567352, confidence_max=14.20490828376766, new_horizon=8
            reward_single_sum=3.387388616043962, confidence_size=2.741502274558016, confidence_max=13.679438801337362, new_horizon=8
            reward_single_sum=24.76593596995857, confidence_size=2.901511148549692, confidence_max=14.60766986661677, new_horizon=8
            reward_single_sum=19.586134611763885, confidence_size=2.828757727846037, confidence_max=14.949652019265582, new_horizon=8
            reward_single_sum=11.975538781577466, confidence_size=2.6759939390037353, confidence_max=14.789620454931175, new_horizon=8
            reward_single_sum=-6.179535860122209, confidence_size=2.9501078468059463, confidence_max=14.192631392445307, new_horizon=8
            reward_single_sum=-4.564742834088408, confidence_size=3.0666159686309893, confidence_max=13.590627406100907, new_horizon=8
            reward_single_sum=20.90304240377625, confidence_size=3.0250537538386775, confidence_max=14.000327407234956, new_horizon=8
            reward_single_sum=10.862145669677888, confidence_size=2.8907595185833976, confidence_max=13.86131950599141, new_horizon=8
            reward_single_sum=3.9464711553756557, confidence_size=2.8093136483554995, confidence_max=13.498910082482217, new_horizon=8
            reward_single_sum=23.10628241272761, confidence_size=2.815539959878481, confidence_max=13.982701239336002, new_horizon=8
            reward_single_sum=17.136248397106918, confidence_size=2.7314060418918276, confidence_max=14.11964462200303, new_horizon=8
            reward_single_sum=7.080569516348627, confidence_size=2.641483812400673, confidence_max=13.87587706880607, new_horizon=8
            reward_single_sum=19.532728929348508, confidence_size=2.5916674170546923, confidence_max=14.112210179423645, new_horizon=8
            reward_single_sum=2.1436843842321007, confidence_size=2.5566081277120736, confidence_max=13.76458894414313, new_horizon=8
        hit cap of: 30 iterations
        episode=14, horizon=6, effective_score=11.21, baseline_lowerbound=10.81 baseline_stdev=1.33, new_epsilon=0.5911, bad=False, gap_average=0.1722811370458868
optimal_epsilon = 0.5911294010059441
optimal_horizon = 6
/home/jeffhykin/repos/AFRL/.venv/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(
Traceback (most recent call last):
  File "./main/run/full.py", line 50, in <module>
    full_run(
  File "./main/run/full.py", line 26, in full_run
    results = Tester.smart_load(
  File "/home/jeffhykin/repos/AFRL/main/test_prediction.py", line 675, in smart_load
    return Tester(
  File "/home/jeffhykin/repos/AFRL/main/test_prediction.py", line 451, in run_all_episodes
    normalized_rewards = normalize(rewards, min=settings.min_reward_single_timestep, max=settings.max_reward_single_timestep)
AttributeError: 'LazyDict' object has no attribute 'min_reward_single_timestep'

[silver_spectacle] There were unviewed results
[silver_spectacle] So the display server is still running at: http://localhost:9900
[silver_spectacle] (use the stop server button to kill it now)



-------------------------------------------------------

 Environment: LunarLanderContinuous-v2

-------------------------------------------------------




-----------------------------------------------------------------------------------------------------
 Agent Model Exists, loading: ./subrepos/rl-trained-agents/sac/LunarLanderContinuous-v2_1/LunarLanderContinuous-v2.zip
-----------------------------------------------------------------------------------------------------


Wrapping the env with a `Monitor` wrapper
Wrapping the env in a DummyVecEnv.


-----------------------------------------------------------------------------------------------------
 Coach Model Exists, loading: models.ignore/coach/LunarLanderContinuous-v2/final_1_90%/
-----------------------------------------------------------------------------------------------------


coach settings = {
    "force_retrain": False, 
    "learning_rate": 0.0001, 
    "hidden_sizes": [64, 64, 64, 64, ], 
    "loss_function": "state_prediction_loss", 
    "consistent_coach_loss": {"scale_future_state_loss": 1000000, }, 
    "delayed_coach_loss": {"epochs_of_delay": 50, }, 
    "value_plus_state_loss": {"value_proportion": 0.5, }, 
    "number_of_episodes": 100, 
    "number_of_epochs": 100, 
    "train_test_split": 0.7, 
    "batch_size": 128, 
    "with_card": True, 
}


-----------------------------------------------------------------------------------------------------
 Testing Agent+Coach
-----------------------------------------------------------------------------------------------------


test settings = {
    "force_recompute": True, 
    "graph_smoothing": 0, 
    "number_of_episodes_for_baseline": 30, 
    "number_of_epochs_for_optimal_parameters": 15, 
    "number_of_episodes_for_testing": 10, 
    "initial_epsilon": 10.1, 
    "initial_horizon": 16, 
    "reward_discount": 0.98, 
    "acceptable_performance_loss": 1, 
    "acceptable_performance_level": 0.8, 
    "confidence_interval_for_convergence": 90, 
    "min_reward_single_timestep": -100, 
    "max_reward_single_timestep": 70, 
    "horizons": {0.0: 1, 0.005: 10, 0.01: 20, }, 
}
----- getting a reward baseline -----------------------------------------------------------------------------------------------------------------------------------
  episode_index=0, episode_discounted_reward_sum=19.84299123340949
  episode_index=1, episode_discounted_reward_sum=31.291248461969175
  episode_index=2, episode_discounted_reward_sum=29.661479649115456
  episode_index=3, episode_discounted_reward_sum=19.59626479067228
  episode_index=4, episode_discounted_reward_sum=18.50190687247029
  episode_index=5, episode_discounted_reward_sum=16.740529884020077
  episode_index=6, episode_discounted_reward_sum=52.51701193121679
  episode_index=7, episode_discounted_reward_sum=39.602383464991824
  episode_index=8, episode_discounted_reward_sum=20.279719044829026
  episode_index=9, episode_discounted_reward_sum=17.69536203538733
  episode_index=10, episode_discounted_reward_sum=17.775030229880173
  episode_index=11, episode_discounted_reward_sum=44.38435988007778
  episode_index=12, episode_discounted_reward_sum=23.088207772652336
  episode_index=13, episode_discounted_reward_sum=13.451263693878259
  episode_index=14, episode_discounted_reward_sum=17.198394530476687
  episode_index=15, episode_discounted_reward_sum=18.787656710193676
  episode_index=16, episode_discounted_reward_sum=25.508893559377583
  episode_index=17, episode_discounted_reward_sum=13.592598210383887
  episode_index=18, episode_discounted_reward_sum=23.099833587478013
  episode_index=19, episode_discounted_reward_sum=3.414076352922982
  episode_index=20, episode_discounted_reward_sum=20.81413596296459
  episode_index=21, episode_discounted_reward_sum=34.360930224132076
  episode_index=22, episode_discounted_reward_sum=26.893389138486427
  episode_index=23, episode_discounted_reward_sum=19.635628313875323
  episode_index=24, episode_discounted_reward_sum=53.36059722796291
  episode_index=25, episode_discounted_reward_sum=30.515100486577563
  episode_index=26, episode_discounted_reward_sum=13.959646538890219
  episode_index=27, episode_discounted_reward_sum=12.772667564739297
  episode_index=28, episode_discounted_reward_sum=43.15133024995809
  episode_index=29, episode_discounted_reward_sum=17.98051940436296
baseline = {
    "max": 53.36059722796291, 
    "min": 3.414076352922982, 
    "range": 49.94652087503993, 
    "count": 30, 
    "sum": 739.4731570073527, 
    "average": 24.649105233578425, 
    "stdev": 11.972223827144095, 
    "median": 20.061355139119257, 
}
baseline_min = 20.935121065194597, baseline_max = 28.363089401962235,
baseline_confidence_size = 3.713984168383819
----- finding optimal epsilon -------------------------------------------------------------------------------------------------------------------------------------
            reward_single_sum=-16.874839173144515, 
            reward_single_sum=-15.173022353601421, confidence_size=5.372424261151959, confidence_max=-10.651506502221011, new_horizon=4
        episode=0, horizon=32, effective_score=-16.02, baseline_lowerbound=19.72 baseline_stdev=2.19, new_epsilon=6.7333, bad=True, gap_average=2.168319792974563
            reward_single_sum=-12.856926275318788, 
            reward_single_sum=53.24417593574852, confidence_size=208.67296710756887, confidence_max=228.86659193778362, new_horizon=18
            reward_single_sum=-18.387014949272128, confidence_size=67.19127681356272, confidence_max=74.52468838394857, new_horizon=18
            reward_single_sum=-9.658833768168561, confidence_size=39.575363032760606, confidence_max=42.66071326850786, new_horizon=16
            reward_single_sum=-25.495964726571312, confidence_size=30.325675652452965, confidence_max=27.694762895736503, new_horizon=20
            reward_single_sum=4.596365324240779, confidence_size=23.529754382472213, confidence_max=22.10338797258196, new_horizon=10
            reward_single_sum=-2.067519961347447, confidence_size=19.177840311203507, confidence_max=17.65988053681937, new_horizon=6
        episode=1, horizon=32, effective_score=-1.52, baseline_lowerbound=19.72 baseline_stdev=2.19, new_epsilon=4.4889, bad=True, gap_average=1.4668785922075744
            reward_single_sum=22.580049186857803, 
            reward_single_sum=1.879739143519343, confidence_size=65.34830694648862, confidence_max=77.57820111167715, new_horizon=2
            reward_single_sum=18.14614580514398, confidence_size=18.374503053700316, confidence_max=32.57648109887402, new_horizon=2
            reward_single_sum=2.9206079952117, confidence_size=12.397820141654854, confidence_max=23.77945567433806, new_horizon=4
            reward_single_sum=-4.930060834219458, confidence_size=11.137703292759188, confidence_max=19.256999552061856, new_horizon=4
        episode=2, horizon=32, effective_score=8.12, baseline_lowerbound=19.72 baseline_stdev=2.19, new_epsilon=2.9926, bad=True, gap_average=1.388467491062638
            reward_single_sum=22.469342321159054, 
            reward_single_sum=-11.04031491781613, confidence_size=105.78582457651984, confidence_max=111.50033827819125, new_horizon=2
            reward_single_sum=22.174775937689144, confidence_size=32.47349966027414, confidence_max=43.674767440618155, new_horizon=29
            reward_single_sum=20.49168191641842, confidence_size=19.296733894674365, confidence_max=32.820605209036984, new_horizon=6
            reward_single_sum=21.84822175917234, confidence_size=13.997689411095351, confidence_max=29.18643081441991, new_horizon=6
            reward_single_sum=-8.661788061608608, confidence_size=13.44851635594526, confidence_max=24.662169515114293, new_horizon=6
            reward_single_sum=19.073722289265437, confidence_size=11.1757595735437, confidence_max=23.512279751297935, new_horizon=8
            reward_single_sum=15.404795037432958, confidence_size=9.46435434745091, confidence_max=22.184408882664986, new_horizon=6
            reward_single_sum=-11.37301980685238, confidence_size=9.586286629257604, confidence_max=19.629332904242073, new_horizon=6
        episode=3, horizon=32, effective_score=10.04, baseline_lowerbound=19.72 baseline_stdev=2.19, new_epsilon=1.9951, bad=True, gap_average=1.1636643246379963
            reward_single_sum=28.08924665866907, 
            reward_single_sum=13.879456174925132, confidence_size=44.8585430958711, confidence_max=65.84289451266817, new_horizon=30
            reward_single_sum=26.284235271192898, confidence_size=13.041417434175962, confidence_max=35.792396802438326, new_horizon=29
            reward_single_sum=37.21181298602965, confidence_size=11.296996331634258, confidence_max=37.66318410433844, new_horizon=20
            reward_single_sum=35.09417796125499, confidence_size=8.75698656679174, confidence_max=36.868772377206085, new_horizon=18
            reward_single_sum=25.02571448971183, confidence_size=6.8373264774262275, confidence_max=34.434767067723485, new_horizon=12
            reward_single_sum=4.8306563859965586, confidence_size=8.425857364767639, confidence_max=32.77090021159337, new_horizon=14
            reward_single_sum=14.002602245560569, confidence_size=7.5243109994608925, confidence_max=30.57654877112848, new_horizon=13
            reward_single_sum=5.557442630550828, confidence_size=7.448954057995811, confidence_max=28.557325702872646, new_horizon=14
            reward_single_sum=19.35160231391611, confidence_size=6.575723436692984, confidence_max=27.508418148473744, new_horizon=12
            reward_single_sum=18.754101829650686, confidence_size=5.891901160280126, confidence_max=26.626541973685427, new_horizon=10
            reward_single_sum=4.888823242417053, confidence_size=5.833158693717239, confidence_max=25.247314709540184, new_horizon=10
            reward_single_sum=10.467486752999402, confidence_size=5.464538970486984, confidence_max=24.19048965840043, new_horizon=11
            reward_single_sum=8.000896515075683, confidence_size=5.206801091732947, confidence_max=23.16667648158655, new_horizon=12
            reward_single_sum=12.814191540237625, confidence_size=4.858643387690517, confidence_max=22.475473187569722, new_horizon=4
            reward_single_sum=-8.572747860728832, confidence_size=5.35688718415871, confidence_max=21.336868380249914, new_horizon=4
            reward_single_sum=35.557938620077664, confidence_size=5.399654985584753, confidence_max=22.5312807360281, new_horizon=4
            reward_single_sum=25.556349425658183, confidence_size=5.137470040049335, confidence_max=22.737135994671284, new_horizon=4
            reward_single_sum=14.004687281309877, confidence_size=4.855174383211191, confidence_max=22.26563093397461, new_horizon=4
            reward_single_sum=17.299716588457244, confidence_size=4.592935638707976, confidence_max=21.997855191356084, new_horizon=4
            reward_single_sum=2.463577650524602, confidence_size=4.527085961584697, confidence_max=21.220513042703118, new_horizon=4
            reward_single_sum=16.664869594193295, confidence_size=4.306458702526687, confidence_max=20.9985877160576, new_horizon=4
            reward_single_sum=-18.02511909579031, confidence_size=4.855956190266027, confidence_max=20.038639633826453, new_horizon=4
        episode=4, horizon=32, effective_score=15.18, baseline_lowerbound=19.72 baseline_stdev=2.19, new_epsilon=1.3300, bad=True, gap_average=0.7250636459209766
            reward_single_sum=25.91231176433921, 
            reward_single_sum=28.191465008069354, confidence_size=7.1950036225323295, confidence_max=34.24689200873661, new_horizon=13
            reward_single_sum=35.65355696484493, confidence_size=8.589843836051006, confidence_max=38.50895508180217, new_horizon=14
            reward_single_sum=32.1509020687802, confidence_size=5.068332049990538, confidence_max=35.54539100149896, new_horizon=14
            reward_single_sum=17.99185091339476, confidence_size=6.401987260453568, confidence_max=34.38200460433926, new_horizon=14
            reward_single_sum=15.607685613587185, confidence_size=6.455761680613804, confidence_max=32.37372373611641, new_horizon=14
            reward_single_sum=22.534529332492916, confidence_size=5.344691859074089, confidence_max=30.77930638271817, new_horizon=14
            reward_single_sum=29.391841881504426, confidence_size=4.609150185481177, confidence_max=30.5384181288578, new_horizon=14
            reward_single_sum=2.4132849517604384, confidence_size=6.2869521254263905, confidence_max=29.603333069734546, new_horizon=14
            reward_single_sum=7.299391783587428, confidence_size=6.272848494693581, confidence_max=27.987530522929664, new_horizon=14
            reward_single_sum=20.18710483723926, confidence_size=5.615725136806916, confidence_max=27.191536511316016, new_horizon=14
            reward_single_sum=23.347518965847023, confidence_size=5.08646276138276, confidence_max=26.80991643517002, new_horizon=14
            reward_single_sum=23.176727991206036, confidence_size=4.647712547999083, confidence_max=26.48295655389548, new_horizon=12
            reward_single_sum=14.864602315106179, confidence_size=4.365509761022704, confidence_max=25.702850789005513, new_horizon=12
            reward_single_sum=22.60502978627095, confidence_size=4.044724198016979, confidence_max=25.466577809885667, new_horizon=12
            reward_single_sum=16.0811709102634, confidence_size=3.8109422233799215, confidence_max=24.899003166398277, new_horizon=12
            reward_single_sum=34.4189624488592, confidence_size=3.818959627976799, confidence_max=25.69119124780932, new_horizon=12
            reward_single_sum=5.548260040344012, confidence_size=3.9191599867168616, confidence_max=24.8845042965778, new_horizon=12
            reward_single_sum=23.021507992440725, confidence_size=3.700103490369827, confidence_max=24.773666941419176, new_horizon=12
        episode=5, horizon=4, effective_score=21.07, baseline_lowerbound=19.72 baseline_stdev=2.19, new_epsilon=1.9951, bad=False, gap_average=0.5356307721634486
            reward_single_sum=15.894365266777125, 
            reward_single_sum=17.661558481641286, confidence_size=5.578809418647267, confidence_max=22.356771292856468, new_horizon=6
            reward_single_sum=14.40064166574771, confidence_size=2.751935632816922, confidence_max=18.73745743753896, new_horizon=7
        episode=6, horizon=12, effective_score=15.99, baseline_lowerbound=19.72 baseline_stdev=2.19, new_epsilon=1.3300, bad=True, gap_average=0.7208631470548584
            reward_single_sum=16.34810577693944, 
            reward_single_sum=17.009049786829944, confidence_size=2.086518121822385, confidence_max=18.765095903707074, new_horizon=12
        episode=7, horizon=32, effective_score=16.68, baseline_lowerbound=19.72 baseline_stdev=2.19, new_epsilon=0.8867, bad=True, gap_average=0.5524028906455407
            reward_single_sum=32.41527394515359, 
            reward_single_sum=48.361797460068104, confidence_size=50.34119349905014, confidence_max=90.72972920166096, new_horizon=14
            reward_single_sum=25.536234526942117, confidence_size=19.73985024832175, confidence_max=55.177618892376344, new_horizon=12
            reward_single_sum=29.337251064013913, confidence_size=11.8082928845656, confidence_max=45.72093213361003, new_horizon=12
            reward_single_sum=47.01482669839586, confidence_size=9.993024246333466, confidence_max=46.52610098524818, new_horizon=12
            reward_single_sum=17.121958239144593, confidence_size=10.098357479961836, confidence_max=43.3962478022482, new_horizon=12
            reward_single_sum=26.072544755675704, confidence_size=8.471151046795669, confidence_max=40.73684914528051, new_horizon=10
            reward_single_sum=26.66645704554155, confidence_size=7.274618100411196, confidence_max=38.840411067278126, new_horizon=8
            reward_single_sum=36.793937669981275, confidence_size=6.388967929010178, confidence_max=38.535665862889815, new_horizon=6
            reward_single_sum=19.727541499570002, confidence_size=6.075859124192593, confidence_max=36.98064141464126, new_horizon=6
            reward_single_sum=46.0881864507129, confidence_size=5.982150029224721, confidence_max=38.26724178878832, new_horizon=6
            reward_single_sum=19.200567668920314, confidence_size=5.754414811286047, confidence_max=36.94912956329604, new_horizon=6
            reward_single_sum=29.414361923009455, confidence_size=5.258881914302446, confidence_max=36.316646448697014, new_horizon=6
            reward_single_sum=24.313730805846482, confidence_size=4.912402497343114, confidence_max=35.48845033684139, new_horizon=6
            reward_single_sum=28.686475602220426, confidence_size=4.553755536168392, confidence_max=35.00383189318148, new_horizon=6
            reward_single_sum=28.68600544440062, confidence_size=4.244075808908358, confidence_max=34.58389773388316, new_horizon=6
            reward_single_sum=26.27163556049652, confidence_size=3.9922393758872214, confidence_max=34.0927562205986, new_horizon=6
            reward_single_sum=35.94152351506897, confidence_size=3.7926331790659162, confidence_max=34.21765039435272, new_horizon=6
            reward_single_sum=5.991062020192806, confidence_size=4.2143770932126134, confidence_max=33.35339666665236, new_horizon=6
            reward_single_sum=28.802939928653267, confidence_size=3.986846453955053, confidence_max=33.109062045155476, new_horizon=6
            reward_single_sum=47.02494090010721, confidence_size=4.058288380364319, confidence_max=34.0330147005603, new_horizon=6
            reward_single_sum=27.09124213251105, confidence_size=3.8670897178128634, confidence_max=33.71074857493226, new_horizon=6
            reward_single_sum=18.681568148218922, confidence_size=3.7803989361843584, confidence_max=33.13874950161243, new_horizon=6
            reward_single_sum=20.05575392360663, confidence_size=3.6731297954377364, confidence_max=32.64387216745658, new_horizon=6
        episode=8, horizon=12, effective_score=28.97, baseline_lowerbound=19.72 baseline_stdev=2.19, new_epsilon=1.3300, bad=False, gap_average=0.4499177702938218
            reward_single_sum=18.61647371191996, 
            reward_single_sum=33.79863872533758, confidence_size=47.92820867571165, confidence_max=74.1357648943404, new_horizon=12
            reward_single_sum=64.35062633263257, confidence_size=39.269522918513985, confidence_max=78.19143584181067, new_horizon=12
            reward_single_sum=19.213802584095394, confidence_size=25.2048545104171, confidence_max=59.19973984891347, new_horizon=12
            reward_single_sum=18.684199606008878, confidence_size=18.85219937088942, confidence_max=49.78494756288829, new_horizon=12
            reward_single_sum=22.853887460716287, confidence_size=14.800248617273354, confidence_max=44.38652002072513, new_horizon=12
            reward_single_sum=18.761877341451108, confidence_size=12.430997464175913, confidence_max=40.470926858770454, new_horizon=12
            reward_single_sum=31.687055275598976, confidence_size=10.531774827438777, confidence_max=39.02759495715887, new_horizon=12
            reward_single_sum=13.446732602869096, confidence_size=9.632098885088432, confidence_max=36.45579817849175, new_horizon=12
            reward_single_sum=28.796119932486945, confidence_size=8.500431504124041, confidence_max=35.52137286143572, new_horizon=10
            reward_single_sum=21.724055596024364, confidence_size=7.652240008669821, confidence_max=34.19164629677356, new_horizon=10
            reward_single_sum=25.29506485044267, confidence_size=6.924124498364151, confidence_max=33.3598356666628, new_horizon=10
            reward_single_sum=37.26099349291314, confidence_size=6.492939204385337, confidence_max=33.76136439765433, new_horizon=10
            reward_single_sum=25.262573682917285, confidence_size=5.9783851112451, confidence_max=33.10353519663183, new_horizon=10
            reward_single_sum=19.49289511419317, confidence_size=5.607408334924942, confidence_max=32.2237414222321, new_horizon=10
            reward_single_sum=49.57908169121328, confidence_size=5.795265806819863, confidence_max=33.846770681871156, new_horizon=10
            reward_single_sum=24.365572703533424, confidence_size=5.434649201762651, confidence_max=33.26933453731289, new_horizon=10
            reward_single_sum=4.430065425887825, confidence_size=5.584049675689602, confidence_max=32.11847834959193, new_horizon=10
            reward_single_sum=20.08176209558009, confidence_size=5.297984418869076, confidence_max=31.492799062333393, new_horizon=10
            reward_single_sum=17.132898006330144, confidence_size=5.072684698351443, confidence_max=30.81440350995905, new_horizon=10
            reward_single_sum=45.745392073722755, confidence_size=5.085451240019456, confidence_max=31.779725921251597, new_horizon=10
            reward_single_sum=24.74082029873732, confidence_size=4.840023691254345, confidence_max=31.44550499146399, new_horizon=10
            reward_single_sum=10.104182067263778, confidence_size=4.776731513508533, confidence_max=30.664765020981406, new_horizon=10
            reward_single_sum=19.593991363775533, confidence_size=4.58673076314772, confidence_max=30.212512514633204, new_horizon=10
            reward_single_sum=26.27143748094403, confidence_size=4.391986077831573, confidence_max=30.043594058495398, new_horizon=10
            reward_single_sum=27.827286140783798, confidence_size=4.215346707656183, confidence_max=29.950634617555387, new_horizon=10
            reward_single_sum=10.249162651484891, confidence_size=4.166697894746072, confidence_max=29.328425609889194, new_horizon=10
            reward_single_sum=19.417739329411532, confidence_size=4.024842825978979, confidence_max=28.981428098774543, new_horizon=10
            reward_single_sum=11.304646629281274, confidence_size=3.9604657012215974, confidence_max=28.446294469068395, new_horizon=10
            reward_single_sum=21.899088084652988, confidence_size=3.824473390106988, confidence_max=28.22407746851399, new_horizon=10
        hit cap of: 30 iterations
        episode=9, horizon=7, effective_score=24.40, baseline_lowerbound=19.72 baseline_stdev=2.19, new_epsilon=1.9951, bad=False, gap_average=0.49246530188496107
            reward_single_sum=49.93764788559753, 
            reward_single_sum=16.936757594887965, confidence_size=104.1797105313735, confidence_max=137.6169132716162, new_horizon=8
            reward_single_sum=7.96021020009674, confidence_size=37.265535043710805, confidence_max=62.210406937238204, new_horizon=8
            reward_single_sum=1.8518217041300105, confidence_size=25.2115255131983, confidence_max=44.383134859376355, new_horizon=8
            reward_single_sum=24.25795517133422, confidence_size=17.822997307060042, confidence_max=38.01187581826933, new_horizon=8
            reward_single_sum=16.903081405998314, confidence_size=13.799320337508576, confidence_max=33.44056599784937, new_horizon=8
            reward_single_sum=36.805037240432014, confidence_size=12.214239436337166, confidence_max=34.307455322405275, new_horizon=8
            reward_single_sum=-15.420516243480977, confidence_size=13.61215200104403, confidence_max=31.016151370918507, new_horizon=8
            reward_single_sum=23.180620206293703, confidence_size=11.84311830223504, confidence_max=29.88896443171165, new_horizon=8
            reward_single_sum=21.604815222512354, confidence_size=10.46258114383854, confidence_max=28.864324182618724, new_horizon=8
            reward_single_sum=12.948434268524695, confidence_size=9.400185078653951, confidence_max=27.306172774683635, new_horizon=6
            reward_single_sum=13.234549890789724, confidence_size=8.531367767589519, confidence_max=26.048068979849205, new_horizon=6
            reward_single_sum=19.49308163619838, confidence_size=7.793011180074296, confidence_max=25.461741655713887, new_horizon=8
            reward_single_sum=18.76326389206944, confidence_size=7.170300148383442, confidence_max=24.917211582339448, new_horizon=8
            reward_single_sum=31.45590668172568, confidence_size=6.831278425109479, confidence_max=25.492122875583462, new_horizon=8
            reward_single_sum=38.24837629307655, confidence_size=6.712437873846107, confidence_max=26.59750306448275, new_horizon=8
            reward_single_sum=27.168703560634626, confidence_size=6.323856988994205, confidence_max=26.637371495513086, new_horizon=8
            reward_single_sum=54.47905147212969, confidence_size=6.796708393800811, confidence_max=29.008308287298068, new_horizon=8
            reward_single_sum=10.833490954403835, confidence_size=6.4921459757388345, confidence_max=28.104898030336436, new_horizon=8
            reward_single_sum=15.33314185553928, confidence_size=6.16542784394984, confidence_max=27.464199388594526, new_horizon=8
            reward_single_sum=19.22124460355811, confidence_size=5.8520072820907085, confidence_max=27.051848972397938, new_horizon=8
            reward_single_sum=57.54389267753132, confidence_size=6.25061163832763, confidence_max=29.102455646235956, new_horizon=8
            reward_single_sum=27.401708821135454, confidence_size=5.969847744054354, confidence_max=29.019511961233423, new_horizon=8
            reward_single_sum=47.54630881414413, confidence_size=5.966986301800054, confidence_max=30.037344043852666, new_horizon=8
            reward_single_sum=28.428911643058232, confidence_size=5.7211300649920105, confidence_max=29.96582996308485, new_horizon=8
            reward_single_sum=-12.316182338716533, confidence_size=5.990512061916553, confidence_max=28.82902418167057, new_horizon=8
            reward_single_sum=8.930108225378273, confidence_size=5.822531048554248, confidence_max=28.1459170981462, new_horizon=8
            reward_single_sum=17.5124240125334, confidence_size=5.610706238253494, confidence_max=27.762272215093354, new_horizon=8
            reward_single_sum=46.37978771685376, confidence_size=5.590585972058731, confidence_max=28.577607870968038, new_horizon=8
            reward_single_sum=22.421159719945226, confidence_size=5.394752046539949, confidence_max=28.362911872817122, new_horizon=8
        hit cap of: 30 iterations
        episode=10, horizon=6, effective_score=22.97, baseline_lowerbound=19.72 baseline_stdev=2.19, new_epsilon=2.9926, bad=False, gap_average=0.8072818887757731
            reward_single_sum=7.539228809351293, 
            reward_single_sum=3.527940366776377, confidence_size=12.66313924030543, confidence_max=18.196723828369258, new_horizon=6
        episode=11, horizon=8, effective_score=5.53, baseline_lowerbound=19.72 baseline_stdev=2.19, new_epsilon=1.9951, bad=True, gap_average=1.2040315920511881
            reward_single_sum=20.082651006419376, 
            reward_single_sum=-1.9409745302702728, confidence_size=69.52584954684141, confidence_max=78.59668778491593, new_horizon=8
            reward_single_sum=21.138803149876612, confidence_size=21.968260707894522, confidence_max=35.06175391656976, new_horizon=8
            reward_single_sum=35.91476896509993, confidence_size=18.357976214490957, confidence_max=37.156788362272366, new_horizon=8
            reward_single_sum=3.904552687880468, confidence_size=14.361827108610985, confidence_max=30.1817873644122, new_horizon=8
            reward_single_sum=11.523902036188094, confidence_size=11.177434139031037, confidence_max=26.281384691563403, new_horizon=8
            reward_single_sum=0.6813973174641887, confidence_size=9.950706375128703, confidence_max=22.994292179794186, new_horizon=8
            reward_single_sum=26.285929353875016, confidence_size=8.968225752165392, confidence_max=23.66710450048207, new_horizon=8
            reward_single_sum=4.489752093956811, confidence_size=8.044469058829009, confidence_max=21.6090004011057, new_horizon=8
            reward_single_sum=6.7291844905785165, confidence_size=7.202729610782134, confidence_max=20.083726267889006, new_horizon=8
        episode=12, horizon=10, effective_score=12.88, baseline_lowerbound=19.72 baseline_stdev=2.19, new_epsilon=1.3300, bad=True, gap_average=0.9121220439741939
            reward_single_sum=32.02173728573121, 
            reward_single_sum=37.07337159089293, confidence_size=15.947381873217589, confidence_max=50.494936311529656, new_horizon=10
            reward_single_sum=7.651584176871997, confidence_size=26.522666093124272, confidence_max=52.10489711095631, new_horizon=10
            reward_single_sum=10.93568787186465, confidence_size=17.398882480959564, confidence_max=39.31947771229976, new_horizon=10
            reward_single_sum=20.411415186221, confidence_size=12.225492314127798, confidence_max=33.844251536444155, new_horizon=10
            reward_single_sum=53.41186730205712, confidence_size=14.248873469430618, confidence_max=41.166484038370434, new_horizon=10
            reward_single_sum=11.823732559618087, confidence_size=12.34576188061742, confidence_max=37.10710416251128, new_horizon=12
            reward_single_sum=17.63076494368964, confidence_size=10.560220221343803, confidence_max=34.43024033596213, new_horizon=11
            reward_single_sum=24.071807592594823, confidence_size=9.141133064453943, confidence_max=33.03357400995855, new_horizon=10
            reward_single_sum=23.164228435986406, confidence_size=8.060953227460544, confidence_max=31.880572922013332, new_horizon=10
            reward_single_sum=18.85050312651296, confidence_size=7.255605971439458, confidence_max=30.623487796170437, new_horizon=10
            reward_single_sum=6.9009931928274, confidence_size=7.010297625301822, confidence_max=29.005938730707506, new_horizon=10
            reward_single_sum=38.878056467819064, confidence_size=6.805403240947458, confidence_max=30.099691681923403, new_horizon=10
            reward_single_sum=-16.163965900750224, confidence_size=8.006622955194524, confidence_max=28.48246465747574, new_horizon=10
            reward_single_sum=14.71446379716117, confidence_size=7.444060159268009, confidence_max=27.535810001207892, new_horizon=10
            reward_single_sum=20.64782242571823, confidence_size=6.930893700589143, confidence_max=27.057398079015172, new_horizon=10
            reward_single_sum=20.636981886044623, confidence_size=6.484037472583136, confidence_max=26.64056993969261, new_horizon=10
            reward_single_sum=27.16513355560886, confidence_size=6.128775104846061, confidence_max=26.67467429909439, new_horizon=10
            reward_single_sum=21.754773880319767, confidence_size=5.779821548416637, confidence_max=26.38934572614241, new_horizon=10
            reward_single_sum=26.638642489595373, confidence_size=5.492419554082284, confidence_max=26.403399647401535, new_horizon=10
            reward_single_sum=32.581893774674036, confidence_size=5.298418729709841, confidence_max=26.765156617379322, new_horizon=10
            reward_single_sum=15.239510982531066, confidence_size=5.063679383019075, confidence_max=26.247361502273176, new_horizon=10
            reward_single_sum=25.16672709921922, confidence_size=4.837542960408554, confidence_max=26.194400948356787, new_horizon=10
            reward_single_sum=50.453522590505955, confidence_size=5.068269609578742, confidence_max=27.637488622633548, new_horizon=10
            reward_single_sum=27.681189273537086, confidence_size=4.865427559287836, confidence_max=27.639125382761932, new_horizon=10
            reward_single_sum=21.679524069222385, confidence_size=4.667615208652027, confidence_max=27.399229426193365, new_horizon=10
            reward_single_sum=20.355846024763448, confidence_size=4.487291674078094, confidence_max=27.130914477072103, new_horizon=10
            reward_single_sum=8.704858640598447, confidence_size=4.4006187067438205, confidence_max=26.54642850393799, new_horizon=10
            reward_single_sum=27.26877999396838, confidence_size=4.251418275121814, confidence_max=26.573882217032335, new_horizon=10
            reward_single_sum=38.54848598998729, confidence_size=4.204096984635054, confidence_max=27.067428328148136, new_horizon=10
        hit cap of: 30 iterations
        episode=13, horizon=8, effective_score=22.86, baseline_lowerbound=19.72 baseline_stdev=2.19, new_epsilon=1.9951, bad=False, gap_average=0.5246470450424112
            reward_single_sum=0.030699388635022196, 
            reward_single_sum=23.37037380452786, confidence_size=73.68045234915202, confidence_max=85.38098894573342, new_horizon=8
            reward_single_sum=15.368878536871366, confidence_size=19.99502123751358, confidence_max=32.918338480858324, new_horizon=8
            reward_single_sum=10.44855744349536, confidence_size=11.487665915556889, confidence_max=23.792293208939284, new_horizon=8
            reward_single_sum=19.2252498850669, confidence_size=8.583836745342243, confidence_max=22.272588557061543, new_horizon=8
            reward_single_sum=18.755200003166845, confidence_size=6.839711042651466, confidence_max=21.37287088627869, new_horizon=8
            reward_single_sum=19.6169731438796, confidence_size=5.750306941915058, confidence_max=21.009725828435478, new_horizon=8
            reward_single_sum=33.98256531195191, confidence_size=6.575361150206075, confidence_max=24.17517333990543, new_horizon=8
            reward_single_sum=24.784378021515316, confidence_size=5.8820966793687735, confidence_max=24.280193961492124, new_horizon=8
            reward_single_sum=51.97029404026872, confidence_size=8.048080086713124, confidence_max=29.803397044651014, new_horizon=8
            reward_single_sum=15.379108984770394, confidence_size=7.2740189922151135, confidence_max=28.449680679865047, new_horizon=8
            reward_single_sum=38.44360542136439, confidence_size=7.068833676061944, confidence_max=29.68349067485475, new_horizon=8
            reward_single_sum=15.28869744770383, confidence_size=6.530843857786025, confidence_max=28.581965506495063, new_horizon=8
            reward_single_sum=22.538143721131945, confidence_size=6.008183530581357, confidence_max=28.094092470177745, new_horizon=8
            reward_single_sum=16.4942278560121, confidence_size=5.60153363156596, confidence_max=27.31466383225673, new_horizon=8
            reward_single_sum=21.0526036030899, confidence_size=5.215684867772225, confidence_max=26.88753215611294, new_horizon=8
            reward_single_sum=-8.209978070126018, confidence_size=5.764101516878586, confidence_max=25.678194372368317, new_horizon=8
            reward_single_sum=-23.178338556942947, confidence_size=6.83122464199913, confidence_max=24.35129353013149, new_horizon=8
            reward_single_sum=2.1294037747364816, confidence_size=6.592483760088354, confidence_max=23.30251764225251, new_horizon=8
            reward_single_sum=11.617944702501319, confidence_size=6.251915290983414, confidence_max=22.707344714164428, new_horizon=8
            reward_single_sum=20.72096491338824, confidence_size=5.941911569589031, confidence_max=22.600461730398962, new_horizon=8
            reward_single_sum=44.32485554402477, confidence_size=6.052394451953967, confidence_max=23.96850394836457, new_horizon=8
            reward_single_sum=28.39730858725117, confidence_size=5.8239772951364746, confidence_max=24.19579109984449, new_horizon=8
            reward_single_sum=57.849553936480255, confidence_size=6.2387010505650995, confidence_max=26.255420694096962, new_horizon=8
            reward_single_sum=18.14995553076896, confidence_size=5.974880162069284, confidence_max=25.91692924109063, new_horizon=8
            reward_single_sum=23.496499271748558, confidence_size=5.73603666299786, confidence_max=25.814795364816405, new_horizon=8
            reward_single_sum=11.472331859827737, confidence_size=5.538102865058311, confidence_max=25.298105017173494, new_horizon=8
            reward_single_sum=15.647509404514011, confidence_size=5.3352293333387895, confidence_max=24.948356744468217, new_horizon=8
            reward_single_sum=-32.86797105234061, confidence_size=5.992646418348608, confidence_max=23.79608077901355, new_horizon=8
            reward_single_sum=29.563143997636686, confidence_size=5.820857431022092, confidence_max=24.016282112919427, new_horizon=8
        hit cap of: 30 iterations
        episode=14, horizon=10, effective_score=18.20, baseline_lowerbound=19.72 baseline_stdev=2.19, new_epsilon=1.3300, bad=True, gap_average=0.8565868987894986
optimal_epsilon = 1.9950617283950616
optimal_horizon = 8
    scaled_epsilon: 1.9951, forecast_average: 5.4889, episode_reward:55.58, max_timestep_reward: 16.77, min_timestep_reward: -100.00
    scaled_epsilon: 1.9951, forecast_average: 5.5786, episode_reward:309.40, max_timestep_reward: 100.00, min_timestep_reward: -3.27
    scaled_epsilon: 1.9951, forecast_average: 5.7075, episode_reward:210.89, max_timestep_reward: 100.00, min_timestep_reward: -8.01
    scaled_epsilon: 1.9951, forecast_average: 5.6111, episode_reward:49.38, max_timestep_reward: 23.35, min_timestep_reward: -100.00
    scaled_epsilon: 1.9951, forecast_average: 5.6383, episode_reward:274.64, max_timestep_reward: 100.00, min_timestep_reward: -8.89
    scaled_epsilon: 1.9951, forecast_average: 5.6834, episode_reward:-17.08, max_timestep_reward: 10.30, min_timestep_reward: -100.00
    scaled_epsilon: 1.9951, forecast_average: 5.5929, episode_reward:315.15, max_timestep_reward: 100.00, min_timestep_reward: -9.79
    scaled_epsilon: 1.9951, forecast_average: 5.6254, episode_reward:72.07, max_timestep_reward: 19.64, min_timestep_reward: -100.00
    scaled_epsilon: 1.9951, forecast_average: 5.6635, episode_reward:250.06, max_timestep_reward: 100.00, min_timestep_reward: -2.85
    scaled_epsilon: 1.9951, forecast_average: 5.7108, episode_reward:273.04, max_timestep_reward: 100.00, min_timestep_reward: -9.23
self.recorder = {
    number_of_records: 0,
    records: [ ... ],
    local_data: {},
    parent_data:    {
    }
}


-------------------------------------------------------

 Environment: HopperBulletEnv-v0

-------------------------------------------------------




-----------------------------------------------------------------------------------------------------
 Agent Model Exists, loading: ./subrepos/rl-trained-agents/sac/HopperBulletEnv-v0_1/HopperBulletEnv-v0.zip
-----------------------------------------------------------------------------------------------------


pybullet build time: Oct 11 2021 20:52:37
Wrapping the env with a `Monitor` wrapper
Wrapping the env in a DummyVecEnv.


-----------------------------------------------------------------------------------------------------
 Coach Model Exists, loading: models.ignore/coach/HopperBulletEnv-v0/final_1_90%/
-----------------------------------------------------------------------------------------------------


coach settings = {
    "force_retrain": False, 
    "learning_rate": 0.0001, 
    "hidden_sizes": [64, 64, 64, 64, ], 
    "loss_function": "state_prediction_loss", 
    "consistent_coach_loss": {"scale_future_state_loss": 1000000, }, 
    "delayed_coach_loss": {"epochs_of_delay": 50, }, 
    "value_plus_state_loss": {"value_proportion": 0.5, }, 
    "number_of_episodes": 100, 
    "number_of_epochs": 100, 
    "train_test_split": 0.7, 
    "batch_size": 128, 
    "with_card": True, 
}


-----------------------------------------------------------------------------------------------------
 Testing Agent+Coach
-----------------------------------------------------------------------------------------------------


test settings = {
    "force_recompute": True, 
    "graph_smoothing": 0, 
    "number_of_episodes_for_baseline": 30, 
    "number_of_epochs_for_optimal_parameters": 15, 
    "number_of_episodes_for_testing": 10, 
    "initial_epsilon": 10.1, 
    "initial_horizon": 16, 
    "reward_discount": 0.98, 
    "acceptable_performance_loss": 1, 
    "acceptable_performance_level": 0.8, 
    "confidence_interval_for_convergence": 90, 
    "min_reward_single_timestep": 18, 
    "max_reward_single_timestep": 110, 
    "horizons": {0.1: 26, 0.01: 16, 0.001: 11, }, 
}
----- getting a reward baseline -----------------------------------------------------------------------------------------------------------------------------------
  episode_index=0, episode_discounted_reward_sum=109.23510021736111
  episode_index=1, episode_discounted_reward_sum=112.84004361900806
  episode_index=2, episode_discounted_reward_sum=110.66825194292497
  episode_index=3, episode_discounted_reward_sum=111.63611782813722
  episode_index=4, episode_discounted_reward_sum=112.5041944350266
  episode_index=5, episode_discounted_reward_sum=109.29300714180869
  episode_index=6, episode_discounted_reward_sum=112.5385494052649
  episode_index=7, episode_discounted_reward_sum=112.09981080626112
  episode_index=8, episode_discounted_reward_sum=111.67626182553526
  episode_index=9, episode_discounted_reward_sum=105.52664697161813
  episode_index=10, episode_discounted_reward_sum=111.07170754500387
  episode_index=11, episode_discounted_reward_sum=111.31245912161003
  episode_index=12, episode_discounted_reward_sum=105.80669701253645
  episode_index=13, episode_discounted_reward_sum=109.30092667821992
  episode_index=14, episode_discounted_reward_sum=106.91355200210454
  episode_index=15, episode_discounted_reward_sum=111.24799250237358
  episode_index=16, episode_discounted_reward_sum=110.09390334740658
  episode_index=17, episode_discounted_reward_sum=111.5643106012672
  episode_index=18, episode_discounted_reward_sum=109.5270344236545
  episode_index=19, episode_discounted_reward_sum=111.87954459353666
  episode_index=20, episode_discounted_reward_sum=109.70729638750069
  episode_index=21, episode_discounted_reward_sum=110.52198753708798
  episode_index=22, episode_discounted_reward_sum=110.3701287324192
  episode_index=23, episode_discounted_reward_sum=112.18817750618122
  episode_index=24, episode_discounted_reward_sum=111.79846618035475
  episode_index=25, episode_discounted_reward_sum=112.03796369657542
  episode_index=26, episode_discounted_reward_sum=108.39031566253948
  episode_index=27, episode_discounted_reward_sum=109.56209769842664
  episode_index=28, episode_discounted_reward_sum=108.3233940275072
  episode_index=29, episode_discounted_reward_sum=110.01446183031725
baseline = {
    "max": 112.84004361900806, 
    "min": 105.52664697161813, 
    "range": 7.313396647389922, 
    "count": 30, 
    "sum": 3309.6504012795685, 
    "average": 110.32168004265229, 
    "stdev": 1.9084173084613842, 
    "median": 110.59511974000648, 
}
baseline_min = 109.72965705906714, baseline_max = 110.91370302623747,
baseline_confidence_size = 0.5920229835851671
----- finding optimal epsilon -------------------------------------------------------------------------------------------------------------------------------------
            reward_single_sum=23.474998462509607, 
            reward_single_sum=53.02849760302077, confidence_size=93.29672498303525, confidence_max=131.54847301580037, new_horizon=16
            reward_single_sum=22.057102633269775, confidence_size=29.479541003396733, confidence_max=62.33307390299677, new_horizon=16
        episode=0, horizon=32, effective_score=32.85, baseline_lowerbound=88.26 baseline_stdev=0.35, new_epsilon=6.7333, bad=True, gap_average=4.017401351928711
            reward_single_sum=20.95044963232268, 
            reward_single_sum=46.854720227959554, confidence_size=81.7765638565078, confidence_max=115.67914878664888, new_horizon=10
            reward_single_sum=23.42502305046627, confidence_size=24.09951858737203, confidence_max=54.50958289095486, new_horizon=10
        episode=1, horizon=32, effective_score=30.41, baseline_lowerbound=88.26 baseline_stdev=0.35, new_epsilon=4.4889, bad=True, gap_average=4.988275718688965
            reward_single_sum=21.580108688563957, 
            reward_single_sum=21.944151948931253, confidence_size=1.149239343298543, confidence_max=22.911369662046148, new_horizon=9
        episode=2, horizon=32, effective_score=21.76, baseline_lowerbound=88.26 baseline_stdev=0.35, new_epsilon=2.9926, bad=True, gap_average=5.641900825500488
            reward_single_sum=96.64502882428576, 
            reward_single_sum=22.547699360535066, confidence_size=233.91606307222995, confidence_max=293.5124271646402, new_horizon=8
            reward_single_sum=42.602263675704755, confidence_size=64.6117889829702, confidence_max=118.5434529364787, new_horizon=8
            reward_single_sum=97.4993427343426, confidence_size=44.86510640242804, confidence_max=109.68869005114507, new_horizon=8
        episode=3, horizon=32, effective_score=64.82, baseline_lowerbound=88.26 baseline_stdev=0.35, new_epsilon=1.9951, bad=True, gap_average=1.6137213895195408
            reward_single_sum=98.42127378433396, 
            reward_single_sum=101.5780972984769, confidence_size=9.96569962218961, confidence_max=109.96538516359503, new_horizon=8
            reward_single_sum=101.95660333248091, confidence_size=3.2724223184963463, confidence_max=103.92441379026026, new_horizon=8
        episode=4, horizon=8, effective_score=100.65, baseline_lowerbound=88.26 baseline_stdev=0.35, new_epsilon=2.9926, bad=False, gap_average=1.212712515626798
            reward_single_sum=100.40980949901939, 
            reward_single_sum=22.338896621688132, confidence_size=246.46017222057105, confidence_max=307.83452528092465, new_horizon=8
            reward_single_sum=94.68085045898157, confidence_size=73.35968355257317, confidence_max=145.8362024124695, new_horizon=8
            reward_single_sum=23.32067079660874, confidence_size=50.83530710096604, confidence_max=111.02286394504048, new_horizon=8
            reward_single_sum=21.6092467490801, confidence_size=39.28020716178462, confidence_max=91.7521019868602, new_horizon=8
        episode=5, horizon=8, effective_score=52.47, baseline_lowerbound=88.26 baseline_stdev=0.35, new_epsilon=1.9951, bad=True, gap_average=1.5578800246388176
            reward_single_sum=107.19380148824993, 
            reward_single_sum=96.05070261058782, confidence_size=35.177378709207886, confidence_max=136.79963075862673, new_horizon=8
            reward_single_sum=21.588339336720022, confidence_size=78.46352358163767, confidence_max=153.40780472682357, new_horizon=8
            reward_single_sum=102.38488554264454, confidence_size=47.54099557477754, confidence_max=129.3454278193281, new_horizon=8
            reward_single_sum=105.77001787309888, confidence_size=34.888728155606174, confidence_max=121.4862775258664, new_horizon=8
            reward_single_sum=104.27980197490358, confidence_size=27.572902566149402, confidence_max=117.11749403718352, new_horizon=8
            reward_single_sum=98.3543580015464, confidence_size=22.604904770925515, confidence_max=113.40803431774711, new_horizon=8
            reward_single_sum=99.59046189284503, confidence_size=19.199900659590625, confidence_max=111.10144674966514, new_horizon=8
            reward_single_sum=100.05301480010506, confidence_size=16.70475797592991, confidence_max=109.51202281156338, new_horizon=8
        episode=6, horizon=8, effective_score=92.81, baseline_lowerbound=88.26 baseline_stdev=0.35, new_epsilon=2.9926, bad=False, gap_average=1.1942007862578867
            reward_single_sum=105.17611691278255, 
            reward_single_sum=41.75034913767165, confidence_size=200.2272686837594, confidence_max=273.6905017089864, new_horizon=8
            reward_single_sum=88.24764622237977, confidence_size=55.36605361530657, confidence_max=133.75742437291788, new_horizon=8
            reward_single_sum=100.81131576711682, confidence_size=34.198911581416596, confidence_max=118.19526859140427, new_horizon=8
            reward_single_sum=38.08236747140933, confidence_size=30.969061848520607, confidence_max=105.78262095079262, new_horizon=8
        episode=7, horizon=8, effective_score=74.81, baseline_lowerbound=88.26 baseline_stdev=0.35, new_epsilon=1.9951, bad=True, gap_average=1.4918024161892474
            reward_single_sum=106.23765044388442, 
            reward_single_sum=92.83977935558843, confidence_size=42.295414439418224, confidence_max=141.83412933915463, new_horizon=8
            reward_single_sum=102.2068142365777, confidence_size=11.588168635604042, confidence_max=112.01624998095421, new_horizon=8
            reward_single_sum=102.0628820779122, confidence_size=6.673688820004244, confidence_max=107.51047034849493, new_horizon=8
        episode=8, horizon=8, effective_score=100.84, baseline_lowerbound=88.26 baseline_stdev=0.35, new_epsilon=2.9926, bad=False, gap_average=1.23260382039334
            reward_single_sum=85.43772384222227, 
            reward_single_sum=102.77343318351238, confidence_size=54.72668055685959, confidence_max=148.8322590697269, new_horizon=8
            reward_single_sum=21.59394850848606, confidence_size=72.07451144260693, confidence_max=142.00954662068048, new_horizon=8
            reward_single_sum=22.203513407399445, confidence_size=49.75698055677988, confidence_max=107.75913529218491, new_horizon=8
        episode=9, horizon=8, effective_score=58.00, baseline_lowerbound=88.26 baseline_stdev=0.35, new_epsilon=1.9951, bad=True, gap_average=1.5018821310650563
            reward_single_sum=89.28516980342576, 
            reward_single_sum=101.972783916201, confidence_size=40.05322141187219, confidence_max=135.68219827168554, new_horizon=8
            reward_single_sum=91.07518178888395, confidence_size=11.576829950097157, confidence_max=105.68787511960072, new_horizon=8
        episode=10, horizon=8, effective_score=94.11, baseline_lowerbound=88.26 baseline_stdev=0.35, new_epsilon=2.9926, bad=False, gap_average=1.4701995222518842
            reward_single_sum=22.557777191886824, 
            reward_single_sum=98.70579799191533, confidence_size=240.38984083763654, confidence_max=301.0216284295375, new_horizon=8
            reward_single_sum=95.24851369543208, confidence_size=72.49310440499178, confidence_max=144.66380069806985, new_horizon=8
            reward_single_sum=101.22201959172115, confidence_size=44.709381821548504, confidence_max=124.14290893928734, new_horizon=8
            reward_single_sum=100.56120191625249, confidence_size=32.63963411627865, confidence_max=116.29869619372022, new_horizon=8
            reward_single_sum=22.388243296509422, confidence_size=32.526347515331324, confidence_max=105.97360646261754, new_horizon=8
        episode=11, horizon=8, effective_score=73.45, baseline_lowerbound=88.26 baseline_stdev=0.35, new_epsilon=1.9951, bad=True, gap_average=1.4959207545616062
            reward_single_sum=77.46665472544743, 
            reward_single_sum=97.40686655791292, confidence_size=62.94877133134028, confidence_max=150.38553197302042, new_horizon=8
            reward_single_sum=22.72661668079347, confidence_size=65.18839572749445, confidence_max=131.05510838221238, new_horizon=8
            reward_single_sum=103.72634415475609, confidence_size=43.316282200788095, confidence_max=118.64790273051555, new_horizon=8
            reward_single_sum=95.7460539809319, confidence_size=31.616160663826815, confidence_max=111.03066788379516, new_horizon=8
            reward_single_sum=23.101299592319382, confidence_size=30.87141066450995, confidence_max=100.90038327987014, new_horizon=8
        episode=12, horizon=32, effective_score=70.03, baseline_lowerbound=88.26 baseline_stdev=0.35, new_epsilon=1.3300, bad=True, gap_average=1.2579041157500528
            reward_single_sum=104.99852453379985, 
            reward_single_sum=101.33003429346017, confidence_size=11.580967905988551, confidence_max=114.74524731961856, new_horizon=6
            reward_single_sum=107.6944294633516, confidence_size=5.385561834549421, confidence_max=110.05989126475329, new_horizon=6
            reward_single_sum=104.34910058698006, confidence_size=3.0751526302806838, confidence_max=107.6681748496786, new_horizon=6
        episode=13, horizon=8, effective_score=104.59, baseline_lowerbound=88.26 baseline_stdev=0.35, new_epsilon=1.9951, bad=False, gap_average=0.9907796752921069
            reward_single_sum=88.23206561148073, 
            reward_single_sum=95.16891369122837, confidence_size=21.89876753572529, confidence_max=113.59925718707984, new_horizon=8
            reward_single_sum=98.88876408712325, confidence_size=9.118233138673396, confidence_max=103.2148142686175, new_horizon=8
        episode=14, horizon=8, effective_score=94.10, baseline_lowerbound=88.26 baseline_stdev=0.35, new_epsilon=2.9926, bad=False, gap_average=1.2865326640945793
optimal_epsilon = 2.9925925925925925
optimal_horizon = 8
    scaled_epsilon: 2.9926, forecast_average: 4.1828, episode_reward:1469.38, max_timestep_reward: 4.76, min_timestep_reward: -0.98
    scaled_epsilon: 2.9926, forecast_average: 3.9531, episode_reward:1111.10, max_timestep_reward: 4.94, min_timestep_reward: -0.03
    scaled_epsilon: 2.9926, forecast_average: 2.6354, episode_reward:23.06, max_timestep_reward: 3.40, min_timestep_reward: -1.73
    scaled_epsilon: 2.9926, forecast_average: 1.9766, episode_reward:23.73, max_timestep_reward: 3.45, min_timestep_reward: -1.38
    scaled_epsilon: 2.9926, forecast_average: 2.0114, episode_reward:185.44, max_timestep_reward: 3.51, min_timestep_reward: -0.07
    scaled_epsilon: 2.9926, forecast_average: 2.5511, episode_reward:27.08, max_timestep_reward: 3.16, min_timestep_reward: -1.93
    scaled_epsilon: 2.9926, forecast_average: 2.7158, episode_reward:863.47, max_timestep_reward: 4.86, min_timestep_reward: -0.56
    scaled_epsilon: 2.9926, forecast_average: 2.8416, episode_reward:272.00, max_timestep_reward: 4.48, min_timestep_reward: -1.22
    scaled_epsilon: 2.9926, forecast_average: 2.9161, episode_reward:413.53, max_timestep_reward: 4.30, min_timestep_reward: -0.64
    scaled_epsilon: 2.9926, forecast_average: 2.6745, episode_reward:23.39, max_timestep_reward: 3.72, min_timestep_reward: -1.23
self.recorder = {
    number_of_records: 0,
    records: [ ... ],
    local_data: {},
    parent_data:    {
    }
}
/home/jeffhykin/repos/AFRL/.venv/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(
argv[0]=
argv[0]=


-------------------------------------------------------

 Environment: HalfCheetahBulletEnv-v0

-------------------------------------------------------




-----------------------------------------------------------------------------------------------------
 Agent Model Exists, loading: subrepos/rl-trained-agents/sac/HalfCheetahBulletEnv-v0_1/HalfCheetahBulletEnv-v0.zip
-----------------------------------------------------------------------------------------------------


pybullet build time: Oct 11 2021 20:52:37
Wrapping the env with a `Monitor` wrapper
Wrapping the env in a DummyVecEnv.


-----------------------------------------------------------------------------------------------------
 Training Coach Model from scrach
-----------------------------------------------------------------------------------------------------


coach settings = {
    "force_retrain": False, 
    "learning_rate": 0.0001, 
    "hidden_sizes": [64, 64, 64, 64, ], 
    "loss_function": "state_prediction_loss", 
    "consistent_coach_loss": {"scale_future_state_loss": 1000000, }, 
    "delayed_coach_loss": {"epochs_of_delay": 50, }, 
    "value_plus_state_loss": {"value_proportion": 0.5, }, 
    "number_of_episodes": 100, 
    "number_of_epochs": 100, 
    "train_test_split": 0.7, 
    "batch_size": 128, 
    "with_card": True, 
}
Starting Experience Recording
    Episode: 0, Reward: 2795.553, Average Reward: 2795.553
    Episode: 1, Reward: 2781.528, Average Reward: 2788.540
    Episode: 2, Reward: 2769.659, Average Reward: 2782.247
    Episode: 3, Reward: 2807.045, Average Reward: 2788.446
    Episode: 4, Reward: 2774.428, Average Reward: 2785.643
    Episode: 5, Reward: 2785.799, Average Reward: 2785.669
    Episode: 6, Reward: 2803.275, Average Reward: 2788.184
    Episode: 7, Reward: 2783.809, Average Reward: 2787.637
    Episode: 8, Reward: 2780.809, Average Reward: 2786.878
    Episode: 9, Reward: 2790.612, Average Reward: 2787.252
    Episode: 10, Reward: 2776.396, Average Reward: 2786.265
    Episode: 11, Reward: 2803.845, Average Reward: 2787.730
    Episode: 12, Reward: 2801.271, Average Reward: 2788.772
    Episode: 13, Reward: 2804.312, Average Reward: 2789.882
    Episode: 14, Reward: 2784.503, Average Reward: 2789.523
    Episode: 15, Reward: 2793.632, Average Reward: 2789.780
    Episode: 16, Reward: 2805.545, Average Reward: 2790.707
    Episode: 17, Reward: 2796.475, Average Reward: 2791.028
    Episode: 18, Reward: 2788.378, Average Reward: 2790.888
    Episode: 19, Reward: 2805.728, Average Reward: 2791.630
    Episode: 20, Reward: 2768.792, Average Reward: 2790.543
    Episode: 21, Reward: 2794.579, Average Reward: 2790.726
    Episode: 22, Reward: 2774.911, Average Reward: 2790.038
    Episode: 23, Reward: 2784.194, Average Reward: 2789.795
    Episode: 24, Reward: 2799.072, Average Reward: 2790.166
    Episode: 25, Reward: 2805.761, Average Reward: 2790.766
    Episode: 26, Reward: 2793.822, Average Reward: 2790.879
    Episode: 27, Reward: 2803.827, Average Reward: 2791.341
    Episode: 28, Reward: 2788.921, Average Reward: 2791.258
    Episode: 29, Reward: 2793.673, Average Reward: 2791.339
    Episode: 30, Reward: 2807.538, Average Reward: 2791.861
    Episode: 31, Reward: 2796.412, Average Reward: 2792.003
    Episode: 32, Reward: 2820.098, Average Reward: 2792.855
    Episode: 33, Reward: 2800.555, Average Reward: 2793.081
    Episode: 34, Reward: 2799.613, Average Reward: 2793.268
    Episode: 35, Reward: 2789.808, Average Reward: 2793.172
    Episode: 36, Reward: 2790.061, Average Reward: 2793.088
    Episode: 37, Reward: 2791.758, Average Reward: 2793.053
    Episode: 38, Reward: 2810.972, Average Reward: 2793.512
    Episode: 39, Reward: 2795.627, Average Reward: 2793.565
    Episode: 40, Reward: 2781.539, Average Reward: 2793.272
    Episode: 41, Reward: 2797.314, Average Reward: 2793.368
    Episode: 42, Reward: 2785.210, Average Reward: 2793.178
    Episode: 43, Reward: 2804.480, Average Reward: 2793.435
    Episode: 44, Reward: 2780.798, Average Reward: 2793.154
    Episode: 45, Reward: 2788.901, Average Reward: 2793.062
    Episode: 46, Reward: 2815.065, Average Reward: 2793.530
    Episode: 47, Reward: 2773.183, Average Reward: 2793.106
    Episode: 48, Reward: 2809.264, Average Reward: 2793.436
    Episode: 49, Reward: 2800.670, Average Reward: 2793.580
    Episode: 50, Reward: 2798.543, Average Reward: 2793.678
    Episode: 51, Reward: 2812.234, Average Reward: 2794.035
    Episode: 52, Reward: 2811.411, Average Reward: 2794.362
    Episode: 53, Reward: 2778.908, Average Reward: 2794.076
    Episode: 54, Reward: 2768.399, Average Reward: 2793.609
    Episode: 55, Reward: 2780.581, Average Reward: 2793.377
    Episode: 56, Reward: 2783.115, Average Reward: 2793.197
    Episode: 57, Reward: 2783.925, Average Reward: 2793.037
    Episode: 58, Reward: 2786.973, Average Reward: 2792.934
    Episode: 59, Reward: 2777.667, Average Reward: 2792.680
    Episode: 60, Reward: 2791.907, Average Reward: 2792.667
    Episode: 61, Reward: 2802.643, Average Reward: 2792.828
    Episode: 62, Reward: 2822.178, Average Reward: 2793.294
    Episode: 63, Reward: 2784.756, Average Reward: 2793.160
    Episode: 64, Reward: 2793.433, Average Reward: 2793.164
    Episode: 65, Reward: 2771.181, Average Reward: 2792.831
    Episode: 66, Reward: 2781.093, Average Reward: 2792.656
    Episode: 67, Reward: 2791.164, Average Reward: 2792.634
    Episode: 68, Reward: 2777.755, Average Reward: 2792.419
    Episode: 69, Reward: 2792.233, Average Reward: 2792.416
    Episode: 70, Reward: 2780.040, Average Reward: 2792.242
    Episode: 71, Reward: 2781.581, Average Reward: 2792.094
    Episode: 72, Reward: 2786.565, Average Reward: 2792.018
    Episode: 73, Reward: 2778.816, Average Reward: 2791.839
    Episode: 74, Reward: 2773.862, Average Reward: 2791.600
    Episode: 75, Reward: 2795.952, Average Reward: 2791.657
    Episode: 76, Reward: 2811.109, Average Reward: 2791.910
    Episode: 77, Reward: 2777.735, Average Reward: 2791.728
    Episode: 78, Reward: 2774.435, Average Reward: 2791.509
    Episode: 79, Reward: 2810.258, Average Reward: 2791.743
    Episode: 80, Reward: 2775.608, Average Reward: 2791.544
    Episode: 81, Reward: 2796.244, Average Reward: 2791.601
    Episode: 82, Reward: 2797.986, Average Reward: 2791.678
    Episode: 83, Reward: 2782.791, Average Reward: 2791.573
    Episode: 84, Reward: 2801.019, Average Reward: 2791.684
    Episode: 85, Reward: 2784.257, Average Reward: 2791.597
    Episode: 86, Reward: 2790.845, Average Reward: 2791.589
    Episode: 87, Reward: 2798.929, Average Reward: 2791.672
    Episode: 88, Reward: 2801.526, Average Reward: 2791.783
    Episode: 89, Reward: 2782.390, Average Reward: 2791.679
    Episode: 90, Reward: 2791.977, Average Reward: 2791.682
    Episode: 91, Reward: 2799.088, Average Reward: 2791.762
    Episode: 92, Reward: 2796.701, Average Reward: 2791.815
    Episode: 93, Reward: 2792.079, Average Reward: 2791.818
    Episode: 94, Reward: 2817.912, Average Reward: 2792.093
    Episode: 95, Reward: 2784.050, Average Reward: 2792.009
    Episode: 96, Reward: 2782.131, Average Reward: 2791.907
    Episode: 97, Reward: 2790.243, Average Reward: 2791.890
    Episode: 98, Reward: 2780.731, Average Reward: 2791.778
    Episode: 99, Reward: 2802.588, Average Reward: 2791.886

    Max Episode Reward: 2822.178068752695
    Min Episode Reward: 2768.3986125101915
    Max Timestep Reward: 3.3820754488109883
    Min Timestep Reward: 0.326352096136295
Starting Train/Test
    Epoch: 0
    Epoch: 1
    Epoch: 2
    Epoch: 3
    Epoch: 4
    Epoch: 5
    Epoch: 6
    Epoch: 7
    Epoch: 8
    Epoch: 9
    Epoch: 10
    Epoch: 11
    Epoch: 12
    Epoch: 13
    Epoch: 14
    Epoch: 15
    Epoch: 16
    Epoch: 17
    Epoch: 18
    Epoch: 19
    Epoch: 20
    Epoch: 21
    Epoch: 22
    Epoch: 23
    Epoch: 24
    Epoch: 25
    Epoch: 26
    Epoch: 27
    Epoch: 28
    Epoch: 29
    Epoch: 30
    Epoch: 31
    Epoch: 32
    Epoch: 33
    Epoch: 34
    Epoch: 35
    Epoch: 36
    Epoch: 37
    Epoch: 38
    Epoch: 39
    Epoch: 40
    Epoch: 41
    Epoch: 42
    Epoch: 43
    Epoch: 44
    Epoch: 45
    Epoch: 46
    Epoch: 47
    Epoch: 48
    Epoch: 49
    Epoch: 50
    Epoch: 51
    Epoch: 52
    Epoch: 53
    Epoch: 54
    Epoch: 55
    Epoch: 56
    Epoch: 57
    Epoch: 58
    Epoch: 59
    Epoch: 60
    Epoch: 61
    Epoch: 62
    Epoch: 63
    Epoch: 64
    Epoch: 65
    Epoch: 66
    Epoch: 67
    Epoch: 68
    Epoch: 69
    Epoch: 70
    Epoch: 71
    Epoch: 72
    Epoch: 73
    Epoch: 74
    Epoch: 75
    Epoch: 76
    Epoch: 77
    Epoch: 78
    Epoch: 79
    Epoch: 80
    Epoch: 81
    Epoch: 82
    Epoch: 83
    Epoch: 84
    Epoch: 85
    Epoch: 86
    Epoch: 87
    Epoch: 88
    Epoch: 89
    Epoch: 90
    Epoch: 91
    Epoch: 92
    Epoch: 93
    Epoch: 94
    Epoch: 95
    Epoch: 96
    Epoch: 97
    Epoch: 98
    Epoch: 99
Saving Coach to: models.ignore/coach/HalfCheetahBulletEnv-v0/final_1_90%/


-----------------------------------------------------------------------------------------------------
 Testing Agent+Coach
-----------------------------------------------------------------------------------------------------


test settings = {
    "force_recompute": True, 
    "graph_smoothing": 0, 
    "number_of_episodes_for_baseline": 30, 
    "number_of_epochs_for_optimal_parameters": 15, 
    "number_of_episodes_for_testing": 10, 
    "initial_epsilon": 10.1, 
    "initial_horizon": 16, 
    "reward_discount": 0.98, 
    "acceptable_performance_loss": 1, 
    "acceptable_performance_level": 0.8, 
    "confidence_interval_for_convergence": 90, 
    "min_reward_single_timestep": -60, 
    "max_reward_single_timestep": 100, 
    "horizons": {0: 1, 0.001: 11, 0.01: 13, 0.1: 26, }, 
}
----- getting a reward baseline -----------------------------------------------------------------------------------------------------------------------------------
  episode_index=0, episode_discounted_reward_sum=106.24417688469192
  episode_index=1, episode_discounted_reward_sum=103.59828866787925
  episode_index=2, episode_discounted_reward_sum=102.3543078895597
  episode_index=3, episode_discounted_reward_sum=102.14425109668016
  episode_index=4, episode_discounted_reward_sum=105.82358865469334
  episode_index=5, episode_discounted_reward_sum=105.63687838438202
  episode_index=6, episode_discounted_reward_sum=106.61033058026344
  episode_index=7, episode_discounted_reward_sum=106.12807392323056
  episode_index=8, episode_discounted_reward_sum=103.7163453966428
  episode_index=9, episode_discounted_reward_sum=103.11800619802811
  episode_index=10, episode_discounted_reward_sum=105.40622076013985
  episode_index=11, episode_discounted_reward_sum=102.87385488824283
  episode_index=12, episode_discounted_reward_sum=105.36258037781074
  episode_index=13, episode_discounted_reward_sum=104.02623003901496
  episode_index=14, episode_discounted_reward_sum=103.1716871749039
  episode_index=15, episode_discounted_reward_sum=102.4419979923847
  episode_index=16, episode_discounted_reward_sum=104.0483333517327
  episode_index=17, episode_discounted_reward_sum=103.58973008986776
  episode_index=18, episode_discounted_reward_sum=105.92508793986998
  episode_index=19, episode_discounted_reward_sum=104.95333307247792
  episode_index=20, episode_discounted_reward_sum=104.96258345638411
  episode_index=21, episode_discounted_reward_sum=104.66495915102797
  episode_index=22, episode_discounted_reward_sum=104.66250869955249
  episode_index=23, episode_discounted_reward_sum=105.56865973658489
  episode_index=24, episode_discounted_reward_sum=102.24019213253398
  episode_index=25, episode_discounted_reward_sum=105.34275745290742
  episode_index=26, episode_discounted_reward_sum=103.43519308877845
  episode_index=27, episode_discounted_reward_sum=102.31573294498708
  episode_index=28, episode_discounted_reward_sum=103.91244270491995
  episode_index=29, episode_discounted_reward_sum=101.84710683089358
baseline = {
    "max": 106.61033058026344, 
    "min": 101.84710683089358, 
    "range": 4.763223749369857, 
    "count": 30, 
    "sum": 3126.1254395610667, 
    "average": 104.20418131870223, 
    "stdev": 1.4175429191485511, 
    "median": 104.03728169537382, 
}
baseline_min = 103.76443578472833, baseline_max = 104.64392685267609,
baseline_confidence_size = 0.43974553397387695
----- finding optimal epsilon -------------------------------------------------------------------------------------------------------------------------------------
            reward_single_sum=20.306422160118952, 
            reward_single_sum=5.027868949295102, confidence_size=48.23249423940288, confidence_max=60.89963979410989, new_horizon=8
        episode=0, horizon=32, effective_score=12.67, baseline_lowerbound=83.36 baseline_stdev=0.26, new_epsilon=6.7333, bad=True, gap_average=1.3059800407886506
            reward_single_sum=19.408179918208408, 
            reward_single_sum=31.646940505208953, confidence_size=38.636246597730334, confidence_max=64.163806809439, new_horizon=8
        episode=1, horizon=32, effective_score=25.53, baseline_lowerbound=83.36 baseline_stdev=0.26, new_epsilon=4.4889, bad=True, gap_average=0.8863564515113831
            reward_single_sum=33.050015442038486, 
            reward_single_sum=25.30749606704112, confidence_size=24.4421717161326, confidence_max=53.62092747067239, new_horizon=6
        episode=2, horizon=32, effective_score=29.18, baseline_lowerbound=83.36 baseline_stdev=0.26, new_epsilon=2.9926, bad=True, gap_average=0.5512119216918945
            reward_single_sum=39.92395683293742, 
            reward_single_sum=47.22854733295642, confidence_size=23.05968466724773, confidence_max=66.63593675019465, new_horizon=2
        episode=3, horizon=32, effective_score=43.58, baseline_lowerbound=83.36 baseline_stdev=0.26, new_epsilon=1.9951, bad=True, gap_average=1.4261220993995667
            reward_single_sum=30.647301297400748, 
            reward_single_sum=78.15643439488021, confidence_size=149.9804305305451, confidence_max=204.3822983766855, new_horizon=4
            reward_single_sum=67.07390838799023, confidence_size=41.903108998169685, confidence_max=100.52899035826007, new_horizon=4
        episode=4, horizon=32, effective_score=58.63, baseline_lowerbound=83.36 baseline_stdev=0.26, new_epsilon=1.3300, bad=True, gap_average=0.40654143873850507
            reward_single_sum=84.60248378213083, 
            reward_single_sum=80.62849506309755, confidence_size=12.545388647299127, confidence_max=95.16087806991331, new_horizon=6
        episode=5, horizon=32, effective_score=82.62, baseline_lowerbound=83.36 baseline_stdev=0.26, new_epsilon=0.8867, bad=True, gap_average=0.8821063992977143
            reward_single_sum=86.94915907671673, 
            reward_single_sum=87.99028162072396, confidence_size=3.286694519659548, confidence_max=90.75641486837989, new_horizon=6
        episode=6, horizon=6, effective_score=87.47, baseline_lowerbound=83.36 baseline_stdev=0.26, new_epsilon=1.3300, bad=False, gap_average=0.6394969916343689
            reward_single_sum=92.47375088041937, 
            reward_single_sum=77.17370258828186, confidence_size=48.30035154050536, confidence_max=133.12407827485595, new_horizon=6
            reward_single_sum=82.44732165355254, confidence_size=13.1026039385955, confidence_max=97.13419564601341, new_horizon=6
        episode=7, horizon=4, effective_score=84.03, baseline_lowerbound=83.36 baseline_stdev=0.26, new_epsilon=1.9951, bad=False, gap_average=0.6145989770889282
            reward_single_sum=55.092991629211824, 
            reward_single_sum=21.591754269901912, confidence_size=105.75924406252433, confidence_max=144.10161701208114, new_horizon=4
            reward_single_sum=40.14154132259791, confidence_size=28.293350841724823, confidence_max=67.23544658229537, new_horizon=4
        episode=8, horizon=6, effective_score=38.94, baseline_lowerbound=83.36 baseline_stdev=0.26, new_epsilon=1.3300, bad=True, gap_average=0.7732265582879384
            reward_single_sum=86.91330784285611, 
            reward_single_sum=52.33243779676428, confidence_size=109.16751031832331, confidence_max=178.79038313813345, new_horizon=6
            reward_single_sum=83.16803510120657, confidence_size=31.992005020667378, confidence_max=106.12993193427636, new_horizon=6
            reward_single_sum=49.96386626655047, confidence_size=23.123332403441403, confidence_max=91.21774415528576, new_horizon=6
        episode=9, horizon=6, effective_score=68.09, baseline_lowerbound=83.36 baseline_stdev=0.26, new_epsilon=0.8867, bad=True, gap_average=0.7110721976161003
            reward_single_sum=72.11052552274484, 
            reward_single_sum=78.93476825405857, confidence_size=21.543286441100676, confidence_max=97.06593332950237, new_horizon=6
        episode=10, horizon=32, effective_score=75.52, baseline_lowerbound=83.36 baseline_stdev=0.26, new_epsilon=0.5911, bad=True, gap_average=0.7704961624145508
            reward_single_sum=95.93844656514362, 
            reward_single_sum=88.4178662754711, confidence_size=23.741537598050975, confidence_max=115.91969401835833, new_horizon=4
            reward_single_sum=80.26943338254915, confidence_size=13.211371787384955, confidence_max=101.41995386177291, new_horizon=4
        episode=11, horizon=6, effective_score=88.21, baseline_lowerbound=83.36 baseline_stdev=0.26, new_epsilon=0.8867, bad=False, gap_average=0.5536830606460571
            reward_single_sum=81.47023593869157, 
            reward_single_sum=90.65636863632271, confidence_size=28.999479617415503, confidence_max=115.06278190492263, new_horizon=6
            reward_single_sum=91.81314218318032, confidence_size=9.553973593381158, confidence_max=97.53388917944602, new_horizon=6
        episode=12, horizon=6, effective_score=87.98, baseline_lowerbound=83.36 baseline_stdev=0.26, new_epsilon=1.3300, bad=False, gap_average=0.6709577423731486
            reward_single_sum=85.43719221801803, 
            reward_single_sum=35.95451902302068, confidence_size=156.21065142065706, confidence_max=216.90650704117633, new_horizon=6
            reward_single_sum=85.32992337243037, confidence_size=48.11077845655389, confidence_max=117.0179899943769, new_horizon=6
            reward_single_sum=83.79977430999145, confidence_size=28.783977401786334, confidence_max=101.41432963265146, new_horizon=6
        episode=13, horizon=6, effective_score=72.63, baseline_lowerbound=83.36 baseline_stdev=0.26, new_epsilon=0.8867, bad=True, gap_average=0.8543961429595948
            reward_single_sum=93.71663617775073, 
            reward_single_sum=88.63973084303466, confidence_size=16.027159373782283, confidence_max=107.20534288417497, new_horizon=6
            reward_single_sum=86.2098071832125, confidence_size=6.457504018835351, confidence_max=95.97956208683465, new_horizon=6
        episode=14, horizon=6, effective_score=89.52, baseline_lowerbound=83.36 baseline_stdev=0.26, new_epsilon=1.3300, bad=False, gap_average=0.6766346662839253
optimal_epsilon = 1.3300411522633744
optimal_horizon = 6
    scaled_epsilon: 1.3300, forecast_average: 2.3300, episode_reward:1845.12, max_timestep_reward: 2.92, min_timestep_reward: -1.00
    scaled_epsilon: 1.3300, forecast_average: 2.1278, episode_reward:1293.50, max_timestep_reward: 2.79, min_timestep_reward: -3.04
    scaled_epsilon: 1.3300, forecast_average: 2.2103, episode_reward:1725.51, max_timestep_reward: 2.96, min_timestep_reward: -2.23
    scaled_epsilon: 1.3300, forecast_average: 2.2326, episode_reward:2000.41, max_timestep_reward: 2.87, min_timestep_reward: -0.25
    scaled_epsilon: 1.3300, forecast_average: 2.2286, episode_reward:1865.96, max_timestep_reward: 2.97, min_timestep_reward: -2.23
    scaled_epsilon: 1.3300, forecast_average: 2.1670, episode_reward:1435.04, max_timestep_reward: 2.76, min_timestep_reward: -4.25
    scaled_epsilon: 1.3300, forecast_average: 2.1812, episode_reward:1922.88, max_timestep_reward: 2.77, min_timestep_reward: -1.49
    scaled_epsilon: 1.3300, forecast_average: 2.1779, episode_reward:1216.88, max_timestep_reward: 2.88, min_timestep_reward: -2.02
    scaled_epsilon: 1.3300, forecast_average: 2.1733, episode_reward:1251.43, max_timestep_reward: 2.95, min_timestep_reward: -2.11
    scaled_epsilon: 1.3300, forecast_average: 2.2086, episode_reward:2021.74, max_timestep_reward: 2.87, min_timestep_reward: -0.59
self.recorder = {
    number_of_records: 0,
    records: [ ... ],
    local_data: {},
    parent_data:    {
    }
}
/home/jeffhykin/repos/AFRL/.venv/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(
argv[0]=
argv[0]=
argv[0]=
argv[0]=


-------------------------------------------------------

 Environment: BipedalWalker-v3

-------------------------------------------------------




-----------------------------------------------------------------------------------------------------
 Agent Model Exists, loading: ./subrepos/rl-trained-agents/sac/Walker2d-v3_1/Walker2d-v3.zip
-----------------------------------------------------------------------------------------------------


Wrapping the env with a `Monitor` wrapper
Wrapping the env in a DummyVecEnv.
Traceback (most recent call last):
  File "./main/run/full.py", line 50, in <module>
    full_run(
  File "./main/run/full.py", line 17, in full_run
    agent = Agent.smart_load(
  File "/home/jeffhykin/repos/AFRL/main/agent.py", line 28, in smart_load
    agent = Agent.load(
  File "/home/jeffhykin/repos/AFRL/.venv/lib/python3.8/site-packages/stable_baselines3/common/base_class.py", line 709, in load
    check_for_correct_spaces(env, data["observation_space"], data["action_space"])
  File "/home/jeffhykin/repos/AFRL/.venv/lib/python3.8/site-packages/stable_baselines3/common/utils.py", line 223, in check_for_correct_spaces
    raise ValueError(f"Observation spaces do not match: {observation_space} != {env.observation_space}")
ValueError: Observation spaces do not match: Box([-inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf
 -inf -inf -inf], [inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf], (17,), float64) != Box([-inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf
 -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf], [inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf
 inf inf inf inf inf inf], (24,), float32)


-------------------------------------------------------

 Environment: AntBulletEnv-v0

-------------------------------------------------------




-----------------------------------------------------------------------------------------------------
 Agent Model Exists, loading: subrepos/rl-trained-agents/sac/AntBulletEnv-v0_1/AntBulletEnv-v0.zip
-----------------------------------------------------------------------------------------------------


pybullet build time: Oct 11 2021 20:52:37
Wrapping the env with a `Monitor` wrapper
Wrapping the env in a DummyVecEnv.


-----------------------------------------------------------------------------------------------------
 Training Coach Model from scrach
-----------------------------------------------------------------------------------------------------


coach settings = {
    "force_retrain": False, 
    "learning_rate": 0.0001, 
    "hidden_sizes": [64, 64, 64, 64, ], 
    "loss_function": "state_prediction_loss", 
    "consistent_coach_loss": {"scale_future_state_loss": 1000000, }, 
    "delayed_coach_loss": {"epochs_of_delay": 50, }, 
    "value_plus_state_loss": {"value_proportion": 0.5, }, 
    "number_of_episodes": 100, 
    "number_of_epochs": 100, 
    "train_test_split": 0.7, 
    "batch_size": 128, 
    "with_card": True, 
}
Starting Experience Recording
    Episode: 0, Reward: 3070.567, Average Reward: 3070.567
    Episode: 1, Reward: 3102.490, Average Reward: 3086.528
    Episode: 2, Reward: 3139.285, Average Reward: 3104.114
    Episode: 3, Reward: 3102.930, Average Reward: 3103.818
    Episode: 4, Reward: 3020.622, Average Reward: 3087.179
    Episode: 5, Reward: 3035.669, Average Reward: 3078.594
    Episode: 6, Reward: 3122.190, Average Reward: 3084.822
    Episode: 7, Reward: 3119.096, Average Reward: 3089.106
    Episode: 8, Reward: 3096.605, Average Reward: 3089.939
    Episode: 9, Reward: 3126.693, Average Reward: 3093.614
    Episode: 10, Reward: 3104.048, Average Reward: 3094.563
    Episode: 11, Reward: 3030.114, Average Reward: 3089.192
    Episode: 12, Reward: 3128.817, Average Reward: 3092.240
    Episode: 13, Reward: 3093.000, Average Reward: 3092.294
    Episode: 14, Reward: 3088.794, Average Reward: 3092.061
    Episode: 15, Reward: 3057.783, Average Reward: 3089.919
    Episode: 16, Reward: 3065.516, Average Reward: 3088.483
    Episode: 17, Reward: 3063.039, Average Reward: 3087.070
    Episode: 18, Reward: 3046.384, Average Reward: 3084.928
    Episode: 19, Reward: 3118.435, Average Reward: 3086.604
    Episode: 20, Reward: 3071.178, Average Reward: 3085.869
    Episode: 21, Reward: 3040.357, Average Reward: 3083.800
    Episode: 22, Reward: 3069.269, Average Reward: 3083.169
    Episode: 23, Reward: 3097.886, Average Reward: 3083.782
    Episode: 24, Reward: 3083.138, Average Reward: 3083.756
    Episode: 25, Reward: 3073.607, Average Reward: 3083.366
    Episode: 26, Reward: 3064.832, Average Reward: 3082.679
    Episode: 27, Reward: 3118.878, Average Reward: 3083.972
    Episode: 28, Reward: 3032.246, Average Reward: 3082.188
    Episode: 29, Reward: 3123.899, Average Reward: 3083.579
    Episode: 30, Reward: 3015.030, Average Reward: 3081.368
    Episode: 31, Reward: 3026.357, Average Reward: 3079.648
    Episode: 32, Reward: 3102.757, Average Reward: 3080.349
    Episode: 33, Reward: 3149.781, Average Reward: 3082.391
    Episode: 34, Reward: 3086.754, Average Reward: 3082.516
    Episode: 35, Reward: 3082.364, Average Reward: 3082.511
    Episode: 36, Reward: 3036.241, Average Reward: 3081.261
    Episode: 37, Reward: 3102.385, Average Reward: 3081.817
    Episode: 38, Reward: 3138.864, Average Reward: 3083.279
    Episode: 39, Reward: 3091.751, Average Reward: 3083.491
    Episode: 40, Reward: 3093.529, Average Reward: 3083.736
    Episode: 41, Reward: 3095.598, Average Reward: 3084.018
    Episode: 42, Reward: 3061.894, Average Reward: 3083.504
    Episode: 43, Reward: 3069.968, Average Reward: 3083.196
    Episode: 44, Reward: 3096.599, Average Reward: 3083.494
    Episode: 45, Reward: 3149.715, Average Reward: 3084.934
    Episode: 46, Reward: 3121.680, Average Reward: 3085.716
    Episode: 47, Reward: 3061.811, Average Reward: 3085.218
    Episode: 48, Reward: 3126.914, Average Reward: 3086.068
    Episode: 49, Reward: 3027.029, Average Reward: 3084.888
    Episode: 50, Reward: 3051.899, Average Reward: 3084.241
    Episode: 51, Reward: 3044.769, Average Reward: 3083.482
    Episode: 52, Reward: 3057.264, Average Reward: 3082.987
    Episode: 53, Reward: 3112.620, Average Reward: 3083.536
    Episode: 54, Reward: 3131.586, Average Reward: 3084.410
    Episode: 55, Reward: 3068.918, Average Reward: 3084.133
    Episode: 56, Reward: 3105.367, Average Reward: 3084.505
    Episode: 57, Reward: 3126.918, Average Reward: 3085.237
    Episode: 58, Reward: 3097.692, Average Reward: 3085.448
    Episode: 59, Reward: 3094.351, Average Reward: 3085.596
    Episode: 60, Reward: 3056.871, Average Reward: 3085.125
    Episode: 61, Reward: 2802.112, Average Reward: 3080.561
    Episode: 62, Reward: 3083.736, Average Reward: 3080.611
    Episode: 63, Reward: 3131.344, Average Reward: 3081.404
    Episode: 64, Reward: 3101.344, Average Reward: 3081.710
    Episode: 65, Reward: 3094.716, Average Reward: 3081.907
    Episode: 66, Reward: 3141.093, Average Reward: 3082.791
    Episode: 67, Reward: 3102.080, Average Reward: 3083.074
    Episode: 68, Reward: 3101.991, Average Reward: 3083.349
    Episode: 69, Reward: 3094.954, Average Reward: 3083.514
    Episode: 70, Reward: 3101.260, Average Reward: 3083.764
    Episode: 71, Reward: 3132.188, Average Reward: 3084.437
    Episode: 72, Reward: 3132.978, Average Reward: 3085.102
    Episode: 73, Reward: 3105.092, Average Reward: 3085.372
    Episode: 74, Reward: 3103.374, Average Reward: 3085.612
    Episode: 75, Reward: 3087.065, Average Reward: 3085.631
    Episode: 76, Reward: 3051.540, Average Reward: 3085.188
    Episode: 77, Reward: 3075.560, Average Reward: 3085.065
    Episode: 78, Reward: 3140.351, Average Reward: 3085.765
    Episode: 79, Reward: 3109.950, Average Reward: 3086.067
    Episode: 80, Reward: 3090.538, Average Reward: 3086.122
    Episode: 81, Reward: 3150.058, Average Reward: 3086.902
    Episode: 82, Reward: 3104.639, Average Reward: 3087.116
    Episode: 83, Reward: 3073.211, Average Reward: 3086.950
    Episode: 84, Reward: 3097.008, Average Reward: 3087.068
    Episode: 85, Reward: 3135.914, Average Reward: 3087.636
    Episode: 86, Reward: 3089.460, Average Reward: 3087.657
    Episode: 87, Reward: 3081.877, Average Reward: 3087.592
    Episode: 88, Reward: 3104.420, Average Reward: 3087.781
    Episode: 89, Reward: 3075.928, Average Reward: 3087.649
    Episode: 90, Reward: 2976.594, Average Reward: 3086.429
    Episode: 91, Reward: 3035.791, Average Reward: 3085.878
    Episode: 92, Reward: 3021.966, Average Reward: 3085.191
    Episode: 93, Reward: 3127.405, Average Reward: 3085.640
    Episode: 94, Reward: 3044.012, Average Reward: 3085.202
    Episode: 95, Reward: 3076.667, Average Reward: 3085.113
    Episode: 96, Reward: 3034.811, Average Reward: 3084.594
    Episode: 97, Reward: 3068.964, Average Reward: 3084.435
    Episode: 98, Reward: 3135.805, Average Reward: 3084.954
    Episode: 99, Reward: 3141.063, Average Reward: 3085.515

    Max Episode Reward: 3150.0584939647865
    Min Episode Reward: 2802.112445628809
    Max Timestep Reward: 3.8864958467306963
    Min Timestep Reward: -0.5379339080377605
Starting Train/Test
    Epoch: 0
    Epoch: 1
    Epoch: 2
    Epoch: 3
    Epoch: 4
    Epoch: 5
    Epoch: 6
    Epoch: 7
    Epoch: 8
    Epoch: 9
    Epoch: 10
    Epoch: 11
    Epoch: 12
    Epoch: 13
    Epoch: 14
    Epoch: 15
    Epoch: 16
    Epoch: 17
    Epoch: 18
    Epoch: 19
    Epoch: 20
    Epoch: 21
    Epoch: 22
    Epoch: 23
    Epoch: 24
    Epoch: 25
    Epoch: 26
    Epoch: 27
    Epoch: 28
    Epoch: 29
    Epoch: 30
    Epoch: 31
    Epoch: 32
    Epoch: 33
    Epoch: 34
    Epoch: 35
    Epoch: 36
    Epoch: 37
    Epoch: 38
    Epoch: 39
    Epoch: 40
    Epoch: 41
    Epoch: 42
    Epoch: 43
    Epoch: 44
    Epoch: 45
    Epoch: 46
    Epoch: 47
    Epoch: 48
    Epoch: 49
    Epoch: 50
    Epoch: 51
    Epoch: 52
    Epoch: 53
    Epoch: 54
    Epoch: 55
    Epoch: 56
    Epoch: 57
    Epoch: 58
    Epoch: 59
    Epoch: 60
    Epoch: 61
    Epoch: 62
    Epoch: 63
    Epoch: 64
    Epoch: 65
    Epoch: 66
    Epoch: 67
    Epoch: 68
    Epoch: 69
    Epoch: 70
    Epoch: 71
    Epoch: 72
    Epoch: 73
    Epoch: 74
    Epoch: 75
    Epoch: 76
    Epoch: 77
    Epoch: 78
    Epoch: 79
    Epoch: 80
    Epoch: 81
    Epoch: 82
    Epoch: 83
    Epoch: 84
    Epoch: 85
    Epoch: 86
    Epoch: 87
    Epoch: 88
    Epoch: 89
    Epoch: 90
    Epoch: 91
    Epoch: 92
    Epoch: 93
    Epoch: 94
    Epoch: 95
    Epoch: 96
    Epoch: 97
    Epoch: 98
    Epoch: 99
Saving Coach to: models.ignore/coach/AntBulletEnv-v0/final_1_90%/


-----------------------------------------------------------------------------------------------------
 Testing Agent+Coach
-----------------------------------------------------------------------------------------------------


test settings = {
    "force_recompute": True, 
    "graph_smoothing": 0, 
    "number_of_episodes_for_baseline": 30, 
    "number_of_epochs_for_optimal_parameters": 15, 
    "number_of_episodes_for_testing": 10, 
    "initial_epsilon": 10.1, 
    "initial_horizon": 16, 
    "reward_discount": 0.98, 
    "acceptable_performance_loss": 1, 
    "acceptable_performance_level": 0.8, 
    "confidence_interval_for_convergence": 90, 
    "min_reward_single_timestep": 19, 
    "max_reward_single_timestep": 105, 
    "horizons": {0: 1, 0.0007: 4, 0.0015: 16, 0.002: 26, }, 
}
----- getting a reward baseline -----------------------------------------------------------------------------------------------------------------------------------
  episode_index=0, episode_discounted_reward_sum=102.11694409886327
  episode_index=1, episode_discounted_reward_sum=103.29155844042606
  episode_index=2, episode_discounted_reward_sum=102.83014193985072
  episode_index=3, episode_discounted_reward_sum=104.12337445988204
  episode_index=4, episode_discounted_reward_sum=103.76553098883718
  episode_index=5, episode_discounted_reward_sum=102.21262398866644
  episode_index=6, episode_discounted_reward_sum=103.49177616505096
  episode_index=7, episode_discounted_reward_sum=102.21469585528209
  episode_index=8, episode_discounted_reward_sum=103.98457218999577
  episode_index=9, episode_discounted_reward_sum=103.83508181509714
  episode_index=10, episode_discounted_reward_sum=102.91118892476324
  episode_index=11, episode_discounted_reward_sum=102.74956395643873
  episode_index=12, episode_discounted_reward_sum=100.61144684114394
  episode_index=13, episode_discounted_reward_sum=101.38284195974175
  episode_index=14, episode_discounted_reward_sum=99.248355376958
  episode_index=15, episode_discounted_reward_sum=101.38072437631996
  episode_index=16, episode_discounted_reward_sum=100.80994099281676
  episode_index=17, episode_discounted_reward_sum=100.68621164424961
  episode_index=18, episode_discounted_reward_sum=103.61561851850466
  episode_index=19, episode_discounted_reward_sum=103.11895201442825
  episode_index=20, episode_discounted_reward_sum=102.89559266949696
  episode_index=21, episode_discounted_reward_sum=103.59097724365446
  episode_index=22, episode_discounted_reward_sum=102.58695178997482
  episode_index=23, episode_discounted_reward_sum=102.14920818862286
  episode_index=24, episode_discounted_reward_sum=102.28974485048128
  episode_index=25, episode_discounted_reward_sum=101.38202135474272
  episode_index=26, episode_discounted_reward_sum=103.11116955763666
  episode_index=27, episode_discounted_reward_sum=102.41985357227661
  episode_index=28, episode_discounted_reward_sum=105.61110917996021
  episode_index=29, episode_discounted_reward_sum=100.60625529947286
baseline = {
    "max": 105.61110917996021, 
    "min": 99.248355376958, 
    "range": 6.362753803002207, 
    "count": 30, 
    "sum": 3075.0240282536356, 
    "average": 102.50080094178786, 
    "stdev": 1.3302277340004593, 
    "median": 102.66825787320678, 
}
baseline_min = 102.08814203933555, baseline_max = 102.9134598442402,
baseline_confidence_size = 0.4126589024523213
----- finding optimal epsilon -------------------------------------------------------------------------------------------------------------------------------------
            reward_single_sum=28.64557158007374, 
            reward_single_sum=32.512652414886624, confidence_size=12.20789373932876, confidence_max=42.787005736808936, new_horizon=2
        episode=0, horizon=32, effective_score=30.58, baseline_lowerbound=82.00 baseline_stdev=0.24, new_epsilon=6.7333, bad=True, gap_average=0.6004737930297852
            reward_single_sum=18.97442060234952, 
            reward_single_sum=36.57747794120809, confidence_size=55.570664969173, confidence_max=83.34661424095178, new_horizon=32
        episode=1, horizon=32, effective_score=27.78, baseline_lowerbound=82.00 baseline_stdev=0.24, new_epsilon=4.4889, bad=True, gap_average=0.885893217086792
            reward_single_sum=56.36875434345756, 
            reward_single_sum=37.07755717991419, confidence_size=60.899912656822735, confidence_max=107.62306841850858, new_horizon=6
            reward_single_sum=32.97823282742139, confidence_size=21.057116817294435, confidence_max=63.19863160089214, new_horizon=6
        episode=2, horizon=32, effective_score=42.14, baseline_lowerbound=82.00 baseline_stdev=0.24, new_epsilon=2.9926, bad=True, gap_average=0.22232040023803712
            reward_single_sum=59.076362035398134, 
            reward_single_sum=42.15613488270566, confidence_size=53.41505490804401, confidence_max=104.03130336709589, new_horizon=4
            reward_single_sum=50.215401890730334, confidence_size=14.267855621385593, confidence_max=64.75048855766363, new_horizon=2
        episode=3, horizon=32, effective_score=50.48, baseline_lowerbound=82.00 baseline_stdev=0.24, new_epsilon=1.9951, bad=True, gap_average=0.5029155559539795
            reward_single_sum=83.44371684509552, 
            reward_single_sum=74.3997837733259, confidence_size=28.550573065821887, confidence_max=107.47232337503257, new_horizon=6
            reward_single_sum=74.33450542806007, confidence_size=8.834658048322922, confidence_max=86.22732673048341, new_horizon=6
        episode=4, horizon=32, effective_score=77.39, baseline_lowerbound=82.00 baseline_stdev=0.24, new_epsilon=1.3300, bad=True, gap_average=0.7065413589477539
            reward_single_sum=71.95128985180799, 
            reward_single_sum=81.97152748184875, confidence_size=31.63264525766762, confidence_max=108.59405392449597, new_horizon=6
            reward_single_sum=67.13452666540357, confidence_size=12.760314026114479, confidence_max=86.44609535913457, new_horizon=6
        episode=5, horizon=32, effective_score=73.69, baseline_lowerbound=82.00 baseline_stdev=0.24, new_epsilon=0.8867, bad=True, gap_average=0.6912091785271962
            reward_single_sum=91.6000188705208, 
            reward_single_sum=73.39141778608048, confidence_size=57.482291339645506, confidence_max=139.9780096679461, new_horizon=6
            reward_single_sum=80.41645297852887, confidence_size=15.481377217936952, confidence_max=97.28400709631367, new_horizon=6
        episode=6, horizon=32, effective_score=81.80, baseline_lowerbound=82.00 baseline_stdev=0.24, new_epsilon=0.5911, bad=True, gap_average=0.4847520774205526
            reward_single_sum=98.6208619060295, 
            reward_single_sum=95.43268143711953, confidence_size=10.06468963251956, confidence_max=107.09146130409408, new_horizon=4
            reward_single_sum=88.35448186523092, confidence_size=8.858465077977542, confidence_max=102.99447348077086, new_horizon=6
            reward_single_sum=93.48795764746657, confidence_size=5.062754204711105, confidence_max=99.03674991867273, new_horizon=6
        episode=7, horizon=6, effective_score=93.97, baseline_lowerbound=82.00 baseline_stdev=0.24, new_epsilon=0.8867, bad=False, gap_average=0.32661662220954896
            reward_single_sum=94.37011195890324, 
            reward_single_sum=85.44054176116857, confidence_size=28.18954368123426, confidence_max=118.09487054127014, new_horizon=6
            reward_single_sum=96.0959873650445, confidence_size=9.6417117874717, confidence_max=101.61059214917712, new_horizon=6
        episode=8, horizon=6, effective_score=91.97, baseline_lowerbound=82.00 baseline_stdev=0.24, new_epsilon=1.3300, bad=False, gap_average=0.5773328399658203
            reward_single_sum=80.0882995501769, 
            reward_single_sum=85.57132226966294, confidence_size=17.30922150042148, confidence_max=100.1390324103414, new_horizon=6
        episode=9, horizon=6, effective_score=82.83, baseline_lowerbound=82.00 baseline_stdev=0.24, new_epsilon=1.9951, bad=False, gap_average=0.4774845309257507
            reward_single_sum=60.587726336796464, 
            reward_single_sum=44.499921515884125, confidence_size=50.78720102892855, confidence_max=103.33102495526882, new_horizon=4
            reward_single_sum=75.30266301576435, confidence_size=25.97306437141124, confidence_max=86.10316799422621, new_horizon=4
        episode=10, horizon=6, effective_score=60.13, baseline_lowerbound=82.00 baseline_stdev=0.24, new_epsilon=1.3300, bad=True, gap_average=0.8251722008387248
            reward_single_sum=66.37438990134594, 
            reward_single_sum=53.40873448578544, confidence_size=40.93096326019103, confidence_max=100.8225254537567, new_horizon=2
        episode=11, horizon=6, effective_score=59.89, baseline_lowerbound=82.00 baseline_stdev=0.24, new_epsilon=0.8867, bad=True, gap_average=1.2885550084114075
            reward_single_sum=93.06523467794244, 
            reward_single_sum=91.9249944003454, confidence_size=3.599596889957688, confidence_max=96.0947114291016, new_horizon=6
        episode=12, horizon=2, effective_score=92.50, baseline_lowerbound=82.00 baseline_stdev=0.24, new_epsilon=1.3300, bad=False, gap_average=0.42065318727493284
            reward_single_sum=86.34699586769919, 
            reward_single_sum=63.90807394435727, confidence_size=70.836888642, confidence_max=145.9644235480282, new_horizon=4
            reward_single_sum=78.14842020951303, confidence_size=19.14155564116968, confidence_max=95.27605231502618, new_horizon=4
        episode=13, horizon=6, effective_score=76.13, baseline_lowerbound=82.00 baseline_stdev=0.24, new_epsilon=0.8867, bad=True, gap_average=0.6994080681800843
            reward_single_sum=83.27931312756724, 
            reward_single_sum=90.15030291170432, confidence_size=21.690861078888624, confidence_max=108.40566909852438, new_horizon=6
            reward_single_sum=95.17216044093414, confidence_size=10.065115205480424, confidence_max=99.59904069888232, new_horizon=6
        episode=14, horizon=4, effective_score=89.53, baseline_lowerbound=82.00 baseline_stdev=0.24, new_epsilon=1.3300, bad=False, gap_average=0.5114320599238078
optimal_epsilon = 1.3300411522633744
optimal_horizon = 4
    scaled_epsilon: 1.3300, forecast_average: 1.6606, episode_reward:1305.63, max_timestep_reward: 3.26, min_timestep_reward: -0.47
    scaled_epsilon: 1.3300, forecast_average: 1.7861, episode_reward:1607.26, max_timestep_reward: 3.23, min_timestep_reward: -0.45
    scaled_epsilon: 1.3300, forecast_average: 1.8450, episode_reward:1358.97, max_timestep_reward: 3.14, min_timestep_reward: -0.43
    scaled_epsilon: 1.3300, forecast_average: 1.9365, episode_reward:2271.74, max_timestep_reward: 3.29, min_timestep_reward: -0.19
    scaled_epsilon: 1.3300, forecast_average: 1.8408, episode_reward:1462.23, max_timestep_reward: 3.22, min_timestep_reward: -0.85
    scaled_epsilon: 1.3300, forecast_average: 1.8661, episode_reward:1683.33, max_timestep_reward: 3.11, min_timestep_reward: -0.61
    scaled_epsilon: 1.3300, forecast_average: 1.8943, episode_reward:1673.47, max_timestep_reward: 3.22, min_timestep_reward: -0.48
    scaled_epsilon: 1.3300, forecast_average: 1.9307, episode_reward:2259.68, max_timestep_reward: 3.26, min_timestep_reward: -0.17
    scaled_epsilon: 1.3300, forecast_average: 1.9306, episode_reward:1421.53, max_timestep_reward: 2.92, min_timestep_reward: -0.24
    scaled_epsilon: 1.3300, forecast_average: 1.9299, episode_reward:1774.59, max_timestep_reward: 3.38, min_timestep_reward: -0.19
self.recorder = {
    number_of_records: 0,
    records: [ ... ],
    local_data: {},
    parent_data:    {
    }
}
/home/jeffhykin/repos/AFRL/.venv/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(
argv[0]=
argv[0]=
argv[0]=
argv[0]=


-------------------------------------------------------

 Environment: ReacherBulletEnv-v0

-------------------------------------------------------




-----------------------------------------------------------------------------------------------------
 Agent Model Exists, loading: ./subrepos/rl-trained-agents/sac/ReacherBulletEnv-v0_1/ReacherBulletEnv-v0.zip
-----------------------------------------------------------------------------------------------------


pybullet build time: Oct 11 2021 20:52:37
Wrapping the env with a `Monitor` wrapper
Wrapping the env in a DummyVecEnv.


-----------------------------------------------------------------------------------------------------
 Coach Model Exists, loading: models.ignore/coach/ReacherBulletEnv-v0/final_1_90%/
-----------------------------------------------------------------------------------------------------


coach settings = {
    "force_retrain": False, 
    "learning_rate": 0.0001, 
    "hidden_sizes": [64, 64, 64, 64, ], 
    "loss_function": "state_prediction_loss", 
    "consistent_coach_loss": {"scale_future_state_loss": 1000000, }, 
    "delayed_coach_loss": {"epochs_of_delay": 50, }, 
    "value_plus_state_loss": {"value_proportion": 0.5, }, 
    "number_of_episodes": 100, 
    "number_of_epochs": 100, 
    "train_test_split": 0.7, 
    "batch_size": 128, 
    "with_card": True, 
}


-----------------------------------------------------------------------------------------------------
 Testing Agent+Coach
-----------------------------------------------------------------------------------------------------


test settings = {
    "force_recompute": True, 
    "graph_smoothing": 0, 
    "number_of_episodes_for_baseline": 30, 
    "number_of_epochs_for_optimal_parameters": 15, 
    "number_of_episodes_for_testing": 10, 
    "initial_epsilon": 10.1, 
    "initial_horizon": 16, 
    "reward_discount": 0.98, 
    "acceptable_performance_loss": 1, 
    "acceptable_performance_level": 0.8, 
    "confidence_interval_for_convergence": 90, 
}
----- getting a reward baseline -----------------------------------------------------------------------------------------------------------------------------------
argv[0]=
argv[0]=
  episode_index=0, episode_discounted_reward_sum=11.832928456071302
  episode_index=1, episode_discounted_reward_sum=13.405630696650928
  episode_index=2, episode_discounted_reward_sum=8.35454309152548
  episode_index=3, episode_discounted_reward_sum=25.543868075712172
  episode_index=4, episode_discounted_reward_sum=15.305215319655819
  episode_index=5, episode_discounted_reward_sum=5.192739250718787
  episode_index=6, episode_discounted_reward_sum=13.896911080561566
  episode_index=7, episode_discounted_reward_sum=3.2577218841329354
  episode_index=8, episode_discounted_reward_sum=15.992424496999952
  episode_index=9, episode_discounted_reward_sum=22.821143774257827
  episode_index=10, episode_discounted_reward_sum=24.59437369934267
  episode_index=11, episode_discounted_reward_sum=14.592716981390888
  episode_index=12, episode_discounted_reward_sum=19.465755172742917
  episode_index=13, episode_discounted_reward_sum=22.66571265926505
  episode_index=14, episode_discounted_reward_sum=6.1651432856535
  episode_index=15, episode_discounted_reward_sum=17.112060758763633
  episode_index=16, episode_discounted_reward_sum=23.677794251039245
  episode_index=17, episode_discounted_reward_sum=7.8263504439364775
  episode_index=18, episode_discounted_reward_sum=17.560263216757964
  episode_index=19, episode_discounted_reward_sum=24.22997190900571
  episode_index=20, episode_discounted_reward_sum=13.74173031452983
  episode_index=21, episode_discounted_reward_sum=18.305988479652644
  episode_index=22, episode_discounted_reward_sum=1.0327790872624394
  episode_index=23, episode_discounted_reward_sum=16.144623524292065
  episode_index=24, episode_discounted_reward_sum=13.73667586743892
  episode_index=25, episode_discounted_reward_sum=25.662608484549764
  episode_index=26, episode_discounted_reward_sum=17.5212412402419
  episode_index=27, episode_discounted_reward_sum=8.801511569845726
  episode_index=28, episode_discounted_reward_sum=27.199850082214457
  episode_index=29, episode_discounted_reward_sum=17.014420851945772
baseline = {
    "max": 27.199850082214457, 
    "min": 1.0327790872624394, 
    "range": 26.167070994952017, 
    "count": 30, 
    "sum": 472.6546980061583, 
    "average": 15.755156600205277, 
    "stdev": 7.051891777691163, 
    "median": 16.06852401064601, 
}
baseline_min = 13.56754176793659, baseline_max = 17.942771432473968,
baseline_confidence_size = 2.1876148322686895
----- finding optimal epsilon -------------------------------------------------------------------------------------------------------------------------------------
            reward_single_sum=-11.417615765915697, 
            reward_single_sum=4.38364111077292, confidence_size=49.88260477047572, confidence_max=46.36561744290431, new_horizon=2
            reward_single_sum=0.2098776667660218, confidence_size=13.804437908217093, confidence_max=11.529738912091503, new_horizon=2
        episode=0, horizon=32, effective_score=-2.27, baseline_lowerbound=12.60 baseline_stdev=1.29, new_epsilon=6.7333, bad=True, gap_average=0.6139620039198134
            reward_single_sum=-7.213515686656711, 
            reward_single_sum=-4.589176524073966, confidence_size=8.284712681554112, confidence_max=2.38336657618877, new_horizon=2
        episode=1, horizon=32, effective_score=-5.90, baseline_lowerbound=12.60 baseline_stdev=1.29, new_epsilon=4.4889, bad=True, gap_average=0.6436426186561585
            reward_single_sum=-15.320683521503542, 
            reward_single_sum=-6.394241979447365, confidence_size=28.179666903969586, confidence_max=17.32220415349412, new_horizon=2
            reward_single_sum=-8.52707539873589, confidence_size=7.858790033146839, confidence_max=-2.2218769334154302, new_horizon=2
        episode=2, horizon=32, effective_score=-10.08, baseline_lowerbound=12.60 baseline_stdev=1.29, new_epsilon=2.9926, bad=True, gap_average=0.5719210331969791
            reward_single_sum=-10.395575459227338, 
            reward_single_sum=-11.703190159007544, confidence_size=4.127977145756624, confidence_max=-6.921405663360819, new_horizon=2
        episode=3, horizon=32, effective_score=-11.05, baseline_lowerbound=12.60 baseline_stdev=1.29, new_epsilon=1.9951, bad=True, gap_average=0.550031581223011
            reward_single_sum=-3.7483905591008, 
            reward_single_sum=-5.000799173498671, confidence_size=3.953698393152151, confidence_max=-0.4208964731475877, new_horizon=66
        episode=4, horizon=32, effective_score=-4.37, baseline_lowerbound=12.60 baseline_stdev=1.29, new_epsilon=1.3300, bad=True, gap_average=0.4248102577527364
            reward_single_sum=3.9680510840440695, 
            reward_single_sum=12.17390600331974, confidence_size=25.904864463406735, confidence_max=33.97584300708863, new_horizon=8
            reward_single_sum=15.940083176870687, confidence_size=10.320259281260714, confidence_max=21.01427270267221, new_horizon=8
            reward_single_sum=-8.591207237489774, confidence_size=12.780043745008284, confidence_max=18.65275200169446, new_horizon=6
            reward_single_sum=-7.984551755262227, confidence_size=10.738971853624127, confidence_max=13.840228107920623, new_horizon=6
            reward_single_sum=-3.2426129775333714, confidence_size=8.55740105423295, confidence_max=10.601345769891134, new_horizon=4
        episode=5, horizon=32, effective_score=2.04, baseline_lowerbound=12.60 baseline_stdev=1.29, new_epsilon=0.8867, bad=True, gap_average=0.5942314425441954
            reward_single_sum=-3.4699145196410557, 
            reward_single_sum=5.380015752322895, confidence_size=27.938130330247525, confidence_max=28.89318094658843, new_horizon=6
            reward_single_sum=10.154054632654127, confidence_size=11.654064654859415, confidence_max=15.675449943304734, new_horizon=6
            reward_single_sum=-6.490272152984839, confidence_size=9.07511504437911, confidence_max=10.468585972466888, new_horizon=6
        episode=6, horizon=32, effective_score=1.39, baseline_lowerbound=12.60 baseline_stdev=1.29, new_epsilon=0.5911, bad=True, gap_average=0.36389642347892126
            reward_single_sum=-3.546631893994383, 
            reward_single_sum=8.766449284034943, confidence_size=38.870867469824766, confidence_max=41.480776164845025, new_horizon=14
            reward_single_sum=8.068299189901603, confidence_size=11.659769004690656, confidence_max=16.089141198004707, new_horizon=8
            reward_single_sum=11.108573343062227, confidence_size=7.719829172363904, confidence_max=13.819001653115, new_horizon=8
            reward_single_sum=14.843757260545985, confidence_size=6.576007437341992, confidence_max=14.424096874052065, new_horizon=6
            reward_single_sum=17.76468390221201, confidence_size=6.070288915019008, confidence_max=15.571144095979404, new_horizon=6
            reward_single_sum=22.624691791882476, confidence_size=6.144003453736497, confidence_max=17.51969243625719, new_horizon=8
            reward_single_sum=15.657884780175348, confidence_size=5.285972763473934, confidence_max=17.19693622070146, new_horizon=7
            reward_single_sum=3.1427827744854757, confidence_size=4.921192468935588, confidence_max=15.857913628080663, new_horizon=8
            reward_single_sum=12.187675922577927, confidence_size=4.345130310699079, confidence_max=15.406946946187439, new_horizon=8
            reward_single_sum=13.991353132399725, confidence_size=3.9159033081002415, confidence_max=15.244041443307816, new_horizon=8
            reward_single_sum=18.312330538736788, confidence_size=3.693023892506745, confidence_max=15.603178061341755, new_horizon=8
            reward_single_sum=16.756991965517265, confidence_size=3.4362296170817714, confidence_max=15.719217462584648, new_horizon=8
            reward_single_sum=5.13225335034558, confidence_size=3.2879331963468745, confidence_max=15.060154292195657, new_horizon=8
            reward_single_sum=4.397050096930268, confidence_size=3.165044865365072, confidence_max=14.445587894619289, new_horizon=8
            reward_single_sum=14.387685574822056, confidence_size=2.966344344970782, confidence_max=14.441083783322988, new_horizon=8
            reward_single_sum=2.3817016788750887, confidence_size=2.927920281656217, confidence_max=13.867775145921534, new_horizon=8
            reward_single_sum=3.3310615243870645, confidence_size=2.847145638656417, confidence_max=13.364289761817385, new_horizon=8
        episode=7, horizon=32, effective_score=10.52, baseline_lowerbound=12.60 baseline_stdev=1.29, new_epsilon=0.3941, bad=True, gap_average=0.25592835553266385
            reward_single_sum=20.864222045979222, 
            reward_single_sum=7.187277448697743, confidence_size=43.17641483446721, confidence_max=57.20216458180566, new_horizon=22
            reward_single_sum=-0.46409811171647375, confidence_size=18.21580635212286, confidence_max=27.411606813109685, new_horizon=18
            reward_single_sum=14.612067577471299, confidence_size=10.859138829937777, confidence_max=21.40900607004572, new_horizon=18
            reward_single_sum=20.262169769543636, confidence_size=8.672253212613217, confidence_max=21.164580958608298, new_horizon=12
            reward_single_sum=17.13453137086853, confidence_size=6.872105409039237, confidence_max=20.138133759179894, new_horizon=12
            reward_single_sum=-3.970590908175153, confidence_size=7.36641764807142, confidence_max=18.170071818452676, new_horizon=10
            reward_single_sum=13.757078473997726, confidence_size=6.259146940614368, confidence_max=17.431979148947683, new_horizon=8
            reward_single_sum=22.94322632883516, confidence_size=5.938767654591662, confidence_max=18.419421431869626, new_horizon=8
            reward_single_sum=14.633165423156248, confidence_size=5.251129161417616, confidence_max=17.947034103283407, new_horizon=8
            reward_single_sum=17.85522530977251, confidence_size=4.772633088572611, confidence_max=17.93756715479356, new_horizon=8
            reward_single_sum=10.039854106028365, confidence_size=4.34221227757922, confidence_max=17.24672301378412, new_horizon=8
            reward_single_sum=23.648107905452193, confidence_size=4.228822295202669, confidence_max=17.95976358288813, new_horizon=8
            reward_single_sum=14.857937086097946, confidence_size=3.8927983791943426, confidence_max=17.70423936676641, new_horizon=8
            reward_single_sum=8.311255603811633, confidence_size=3.6617099520342062, confidence_max=17.106471914022244, new_horizon=8
            reward_single_sum=0.23219421748888763, confidence_size=3.7037815356530643, confidence_max=16.322758013609906, new_horizon=8
            reward_single_sum=13.238292090931594, confidence_size=3.4654577031407268, confidence_max=16.120864511272554, new_horizon=8
            reward_single_sum=20.4583000324339, confidence_size=3.3417179441127622, confidence_max=16.43061882026137, new_horizon=8
            reward_single_sum=6.411208490718556, confidence_size=3.209275976197767, confidence_max=15.946719358376372, new_horizon=8
            reward_single_sum=8.579387827189363, confidence_size=3.0571392887229676, confidence_max=15.58667989315211, new_horizon=8
            reward_single_sum=18.321145676335828, confidence_size=2.9392392487153742, confidence_max=15.74457057085436, new_horizon=8
            reward_single_sum=10.049876942673194, confidence_size=2.8042892064451284, confidence_max=15.484372602244761, new_horizon=8
            reward_single_sum=15.31395933236905, confidence_size=2.6812079851579886, confidence_max=15.475807726025856, new_horizon=8
            reward_single_sum=10.896604608018853, confidence_size=2.5657509075325295, confidence_max=15.28126751786502, new_horizon=8
            reward_single_sum=21.48392776017121, confidence_size=2.528913663666459, confidence_max=15.595166719992498, new_horizon=8
            reward_single_sum=7.187768297169844, confidence_size=2.4563591126965703, confidence_max=15.296516601362756, new_horizon=8
            reward_single_sum=7.975718660346916, confidence_size=2.3800623790955955, confidence_max=15.04005546671292, new_horizon=8
            reward_single_sum=10.410687710312773, confidence_size=2.2944366812899393, confidence_max=14.87409743400353, new_horizon=8
            reward_single_sum=8.787535266194899, confidence_size=2.222261817267311, confidence_max=14.671159622169913, new_horizon=8
            reward_single_sum=5.984275914024876, confidence_size=2.1754134763407453, confidence_max=14.408823884880757, new_horizon=8
        episode=8, horizon=32, effective_score=12.23, baseline_lowerbound=12.60 baseline_stdev=1.29, new_epsilon=0.2627, bad=True, gap_average=0.17030054197377628
            reward_single_sum=3.1145997209498217, 
            reward_single_sum=17.39579325258123, confidence_size=45.08395364675157, confidence_max=55.33915013351707, new_horizon=10
            reward_single_sum=8.847743882991573, confidence_size=12.115703616346497, confidence_max=21.901749235187367, new_horizon=6
            reward_single_sum=8.68710472336448, confidence_size=6.934859757827985, confidence_max=16.44617015279976, new_horizon=8
            reward_single_sum=3.8161057674853986, confidence_size=5.4383176872185395, confidence_max=13.810587156693039, new_horizon=8
            reward_single_sum=24.839531037578375, confidence_size=6.942677383123197, confidence_max=18.059490447281675, new_horizon=6
            reward_single_sum=7.034566197377176, confidence_size=5.7707215743006355, confidence_max=16.304356514633213, new_horizon=6
            reward_single_sum=7.078526754724111, confidence_size=4.940820445526354, confidence_max=15.042566862657875, new_horizon=7
            reward_single_sum=17.741607995735535, confidence_size=4.558834477075949, confidence_max=15.509454402941248, new_horizon=6
            reward_single_sum=12.007871692932179, confidence_size=4.024249155331198, confidence_max=15.080594257903186, new_horizon=8
            reward_single_sum=17.327595762068835, confidence_size=3.7444585782080813, confidence_max=15.370917377097964, new_horizon=8
            reward_single_sum=21.136092417249298, confidence_size=3.673806074783786, confidence_max=16.092734341870287, new_horizon=8
            reward_single_sum=24.99329784537403, confidence_size=3.7709521231056504, confidence_max=17.15713958852196, new_horizon=8
            reward_single_sum=4.919125660213567, confidence_size=3.6305606993962147, confidence_max=16.41195803586947, new_horizon=8
            reward_single_sum=4.886678114117006, confidence_size=3.4869800363109107, confidence_max=15.742062757960419, new_horizon=8
            reward_single_sum=16.966926431759827, confidence_size=3.287266264982189, confidence_max=15.83683921851359, new_horizon=8
            reward_single_sum=16.551241782831262, confidence_size=3.102564439389546, confidence_max=15.887529676997412, new_horizon=8
            reward_single_sum=14.292770937160395, confidence_size=2.9182496885808673, confidence_max=15.786981909497205, new_horizon=8
            reward_single_sum=8.697784287741738, confidence_size=2.7777989630642894, confidence_max=15.427007608550385, new_horizon=8
            reward_single_sum=10.351138818070313, confidence_size=2.6352586350612475, confidence_max=15.169563789176554, new_horizon=8
            reward_single_sum=19.670323779852442, confidence_size=2.5680037974915457, confidence_max=15.442119362356241, new_horizon=8
            reward_single_sum=12.31059736085759, confidence_size=2.443249554172646, confidence_max=15.291750655218836, new_horizon=8
            reward_single_sum=13.609290310939043, confidence_size=2.3304157975835134, confidence_max=15.211994690364174, new_horizon=8
            reward_single_sum=10.59559507085544, confidence_size=2.2329258929714353, confidence_max=15.019255459838547, new_horizon=8
            reward_single_sum=-1.478301238979051, confidence_size=2.3503334914117486, confidence_max=14.566077826045012, new_horizon=8
            reward_single_sum=10.969605638264145, confidence_size=2.2559951798414106, confidence_max=14.423811103075861, new_horizon=8
            reward_single_sum=21.16190809141001, confidence_size=2.2408515985365076, confidence_max=14.741782046518201, new_horizon=8
            reward_single_sum=8.768097742974145, confidence_size=2.168312362173843, confidence_max=14.535927356405269, new_horizon=8
        episode=9, horizon=32, effective_score=12.37, baseline_lowerbound=12.60 baseline_stdev=1.29, new_epsilon=0.1751, bad=True, gap_average=0.11573214416347799
            reward_single_sum=1.5140653258624144, 
            reward_single_sum=13.59798345518895, confidence_size=38.147428196882956, confidence_max=45.70345258740862, new_horizon=6
            reward_single_sum=4.420684184144721, confidence_size=10.633192796956529, confidence_max=17.14410378535522, new_horizon=6
            reward_single_sum=14.342924297596454, confidence_size=7.612728627869983, confidence_max=16.081642943568117, new_horizon=6
            reward_single_sum=8.180517641596138, confidence_size=5.343157728117394, confidence_max=13.754392708995127, new_horizon=6
            reward_single_sum=8.72752018502028, confidence_size=4.125018395537552, confidence_max=12.58896757710571, new_horizon=6
        episode=10, horizon=32, effective_score=8.46, baseline_lowerbound=12.60 baseline_stdev=1.29, new_epsilon=0.1168, bad=True, gap_average=0.09752810281183985
            reward_single_sum=11.520410484984449, 
            reward_single_sum=16.996337790448212, confidence_size=17.28682215990582, confidence_max=31.54519629762214, new_horizon=4
            reward_single_sum=16.49661746504795, confidence_size=5.104091369735592, confidence_max=20.10854661656246, new_horizon=4
            reward_single_sum=3.3281595810789657, confidence_size=7.4600947292088335, confidence_max=19.545476059598727, new_horizon=4
            reward_single_sum=25.203457137944163, confidence_size=7.660597231206548, confidence_max=22.369593723107293, new_horizon=4
            reward_single_sum=9.642735150442679, confidence_size=6.152125865041404, confidence_max=20.01674546669914, new_horizon=4
            reward_single_sum=3.25655473461334, confidence_size=5.814841273088907, confidence_max=18.164023036597445, new_horizon=4
            reward_single_sum=22.826440283471047, confidence_size=5.501199935003978, confidence_max=19.160039013507827, new_horizon=4
            reward_single_sum=27.17706954997575, confidence_size=5.520598231514436, confidence_max=20.681462917959607, new_horizon=4
            reward_single_sum=6.162343668406349, confidence_size=5.13948153007929, confidence_max=19.40049411472058, new_horizon=4
            reward_single_sum=4.1178680908459375, confidence_size=4.890872592644087, confidence_max=18.22978113239489, new_horizon=4
            reward_single_sum=9.78140351734701, confidence_size=4.455823779918895, confidence_max=17.498273567802716, new_horizon=4
            reward_single_sum=-0.8789272319246136, confidence_size=4.493239188546237, confidence_max=16.46481382106018, new_horizon=4
            reward_single_sum=16.95755343801388, confidence_size=4.181271343481075, confidence_max=16.508987319245012, new_horizon=4
            reward_single_sum=2.941603302793198, confidence_size=4.025221955078969, confidence_max=15.72719708597819, new_horizon=4
            reward_single_sum=16.444394927855456, confidence_size=3.7834433801255285, confidence_max=15.781819748334513, new_horizon=4
            reward_single_sum=4.45834811110721, confidence_size=3.623114292724135, confidence_max=15.177959586985956, new_horizon=4
            reward_single_sum=16.735826661439503, confidence_size=3.4402572893235677, confidence_max=15.282934881761927, new_horizon=4
            reward_single_sum=10.090313102972415, confidence_size=3.2477287629271796, confidence_max=14.998176645393649, new_horizon=4
            reward_single_sum=9.962626322962238, confidence_size=3.0761908107025233, confidence_max=14.737247615193779, new_horizon=4
            reward_single_sum=8.63560747630971, confidence_size=2.929129032541982, confidence_max=14.446116821405546, new_horizon=4
            reward_single_sum=10.842738940268816, confidence_size=2.7868767527014624, confidence_max=14.273216866628902, new_horizon=4
            reward_single_sum=15.30317669091505, confidence_size=2.67261883577636, confidence_max=14.324908366094565, new_horizon=4
            reward_single_sum=3.8163882650712577, confidence_size=2.6145431130221333, confidence_max=13.940336757288382, new_horizon=4
            reward_single_sum=9.170813501742618, confidence_size=2.5077476088385326, confidence_max=13.747342047403835, new_horizon=4
            reward_single_sum=5.3253094203852696, confidence_size=2.4366845600912894, confidence_max=13.44880649795736, new_horizon=4
        episode=11, horizon=32, effective_score=11.01, baseline_lowerbound=12.60 baseline_stdev=1.29, new_epsilon=0.0778, bad=True, gap_average=0.05923329336520953
            reward_single_sum=9.41076538392948, 
            reward_single_sum=20.08167496498094, confidence_size=33.68673576583373, confidence_max=48.43295594028893, new_horizon=4
            reward_single_sum=26.2732658456575, confidence_size=14.38005247901759, confidence_max=32.96862121054023, new_horizon=4
            reward_single_sum=1.7629372778720485, confidence_size=12.851217882097979, confidence_max=27.23337875020797, new_horizon=2
            reward_single_sum=4.577685761532958, confidence_size=9.939352407424316, confidence_max=22.3606182542189, new_horizon=2
            reward_single_sum=27.39189649265505, confidence_size=9.17168833990932, confidence_max=24.088059294347314, new_horizon=3
            reward_single_sum=9.197568300038858, confidence_size=7.64174661525021, confidence_max=21.7411457619169, new_horizon=4
            reward_single_sum=16.33844510510452, confidence_size=6.474173925969174, confidence_max=20.85345381744059, new_horizon=4
            reward_single_sum=18.712442997602594, confidence_size=5.675178593049999, confidence_max=20.535921051869327, new_horizon=4
            reward_single_sum=10.675551857861388, confidence_size=5.062345049544492, confidence_max=19.504568448268024, new_horizon=4
            reward_single_sum=11.825264007291784, confidence_size=4.5479627612019335, confidence_max=18.752280760704398, new_horizon=4
            reward_single_sum=4.722420901081199, confidence_size=4.351603174345628, confidence_max=17.765763082312986, new_horizon=4
            reward_single_sum=4.119794131239537, confidence_size=4.171947354269924, confidence_max=16.871156048642835, new_horizon=2
            reward_single_sum=20.956859014019237, confidence_size=3.9774756056469105, confidence_max=17.266516465708843, new_horizon=4
            reward_single_sum=12.609330987723993, confidence_size=3.6835718343437636, confidence_max=16.927298702916502, new_horizon=4
            reward_single_sum=26.419221207983554, confidence_size=3.7209471647130243, confidence_max=17.78814242949894, new_horizon=2
            reward_single_sum=11.735661970564323, confidence_size=3.489158157764253, confidence_max=17.419204405243015, new_horizon=4
            reward_single_sum=11.377823808306742, confidence_size=3.287050440726989, confidence_max=17.075306552696194, new_horizon=2
            reward_single_sum=-0.40300650163564117, confidence_size=3.359071672477981, confidence_max=16.400419225836405, new_horizon=2
            reward_single_sum=6.346163910186832, confidence_size=3.2299251098687307, confidence_max=15.936513481068573, new_horizon=2
            reward_single_sum=4.3494283852872275, confidence_size=3.140353598537753, confidence_max=15.448981970408425, new_horizon=4
            reward_single_sum=22.761844490853484, confidence_size=3.0971741335848035, confidence_max=15.880948692681967, new_horizon=4
            reward_single_sum=13.453958075993713, confidence_size=2.9536871806325697, confidence_max=15.766600153507843, new_horizon=4
            reward_single_sum=21.06726853557743, confidence_size=2.883443414810797, confidence_max=16.040287869465327, new_horizon=4
            reward_single_sum=25.38395333303318, confidence_size=2.8848955163328096, confidence_max=16.530824326122485, new_horizon=4
            reward_single_sum=11.663607027440444, confidence_size=2.7703397310841877, confidence_max=16.340025395398893, new_horizon=4
            reward_single_sum=5.913054447180906, confidence_size=2.705410840657933, confidence_max=15.99151757100472, new_horizon=4
            reward_single_sum=6.213352050182673, confidence_size=2.638750772643543, confidence_max=15.672259121555898, new_horizon=4
            reward_single_sum=3.5570013432394916, confidence_size=2.602959485213752, confidence_max=15.309691730482216, new_horizon=4
            reward_single_sum=4.88158692085703, confidence_size=2.550536990553063, confidence_max=14.996431058341145, new_horizon=4
        hit cap of: 30 iterations
        episode=12, horizon=32, effective_score=12.45, baseline_lowerbound=12.60 baseline_stdev=1.29, new_epsilon=0.0519, bad=True, gap_average=0.060844172421428894
            reward_single_sum=5.75230232960937, 
            reward_single_sum=14.241506841922067, confidence_size=26.799363924534617, confidence_max=36.79626851030032, new_horizon=4
            reward_single_sum=15.898681546784633, confidence_size=9.176217808923406, confidence_max=21.140381381695427, new_horizon=4
            reward_single_sum=14.18514318659629, confidence_size=5.390242980072579, confidence_max=17.90965145630067, new_horizon=4
            reward_single_sum=18.250882117681307, confidence_size=4.503027119638144, confidence_max=18.168730324156876, new_horizon=4
            reward_single_sum=16.806699756244697, confidence_size=3.6318396303328786, confidence_max=17.821042260139272, new_horizon=4
            reward_single_sum=10.172197385380333, confidence_size=3.163069782237243, confidence_max=16.77841451998277, new_horizon=4
            reward_single_sum=7.141506604920419, confidence_size=3.079551907963519, confidence_max=15.885666879105909, new_horizon=4
            reward_single_sum=19.24657704640975, confidence_size=2.979378112401691, confidence_max=16.50109998079601, new_horizon=4
            reward_single_sum=13.108154174578694, confidence_size=2.6280475073055616, confidence_max=16.108412606318318, new_horizon=4
            reward_single_sum=19.287315027521984, confidence_size=2.5376667441291687, confidence_max=16.54593638209731, new_horizon=4
            reward_single_sum=5.039110091368994, confidence_size=2.659043626212185, confidence_max=15.91988330196373, new_horizon=4
            reward_single_sum=15.740958776398005, confidence_size=2.451143583195104, confidence_max=15.902761651304068, new_horizon=4
            reward_single_sum=4.457751790828421, confidence_size=2.525612339161591, confidence_max=15.334811387464802, new_horizon=4
            reward_single_sum=10.301085718875926, confidence_size=2.3569126601000265, confidence_max=14.998904153108086, new_horizon=4
            reward_single_sum=8.233027286134758, confidence_size=2.2468941631338364, confidence_max=14.613325393212314, new_horizon=4
            reward_single_sum=11.572303621846931, confidence_size=2.1035428093376627, confidence_max=14.423260650696639, new_horizon=4
        episode=13, horizon=32, effective_score=12.32, baseline_lowerbound=12.60 baseline_stdev=1.29, new_epsilon=0.0346, bad=True, gap_average=0.043863554626118906
            reward_single_sum=4.457280008585725, 
            reward_single_sum=26.17299165612033, confidence_size=68.55380365480097, confidence_max=83.86893948715397, new_horizon=2
            reward_single_sum=16.19915197759349, confidence_size=18.324976496952544, confidence_max=33.934784377719055, new_horizon=2
            reward_single_sum=5.687868016688035, confidence_size=11.964037798131017, confidence_max=25.09336071287791, new_horizon=2
            reward_single_sum=21.15851132035685, confidence_size=9.066178386887259, confidence_max=23.801338982756143, new_horizon=2
            reward_single_sum=23.722490416354347, confidence_size=7.620196989975133, confidence_max=23.85324588925826, new_horizon=2
            reward_single_sum=13.509116147605576, confidence_size=6.256409231649723, confidence_max=22.1003248806932, new_horizon=2
            reward_single_sum=-0.21641466944466217, confidence_size=6.509456962754026, confidence_max=20.345831321986488, new_horizon=2
            reward_single_sum=10.25863195909357, confidence_size=5.682937550507452, confidence_max=19.121784976391147, new_horizon=2
            reward_single_sum=16.136123019338267, confidence_size=5.035050765451784, confidence_max=18.743625750680938, new_horizon=2
            reward_single_sum=15.394784589804544, confidence_size=4.511628258376009, confidence_max=18.373495025839286, new_horizon=2
            reward_single_sum=23.930985301963187, confidence_size=4.350203372881419, confidence_max=19.051163351553022, new_horizon=2
            reward_single_sum=12.100784930781717, confidence_size=3.9872751121217975, confidence_max=18.488221625571104, new_horizon=2
            reward_single_sum=14.742622530338291, confidence_size=3.6681099389632834, confidence_max=18.186319025047517, new_horizon=2
            reward_single_sum=16.667310050642755, confidence_size=3.405630426844457, confidence_max=18.067112910565925, new_horizon=2
            reward_single_sum=13.91551026572771, confidence_size=3.1717893761452514, confidence_max=17.78664859624211, new_horizon=2
            reward_single_sum=15.969775711009856, confidence_size=2.970458219779097, confidence_max=17.66501840992966, new_horizon=2
            reward_single_sum=19.7584867377258, confidence_size=2.8330969476240995, confidence_max=17.808986390417733, new_horizon=2
            reward_single_sum=24.361131678518337, confidence_size=2.8052714950817146, confidence_max=18.275121055545068, new_horizon=2
            reward_single_sum=25.73566518867216, confidence_size=2.798233970701352, confidence_max=18.781374312575146, new_horizon=2
            reward_single_sum=13.22403111413931, confidence_size=2.6645093772864916, confidence_max=18.516263565458644, new_horizon=2
            reward_single_sum=9.305951856529992, confidence_size=2.5858460842305337, confidence_max=18.14006380278259, new_horizon=2
            reward_single_sum=7.6389853495952025, confidence_size=2.535519096863883, confidence_max=17.745596277635205, new_horizon=2
            reward_single_sum=20.291752483382663, confidence_size=2.449972357685806, confidence_max=17.871786009399266, new_horizon=2
            reward_single_sum=8.139745287655389, confidence_size=2.3981828613530647, confidence_max=17.5287137785042, new_horizon=2
            reward_single_sum=17.487050432999215, confidence_size=2.305611499332091, confidence_max=17.526777782477385, new_horizon=2
            reward_single_sum=15.192508170421409, confidence_size=2.215299710095846, confidence_max=17.435404581658773, new_horizon=2
            reward_single_sum=6.087033283051338, confidence_size=2.203007930295481, confidence_max=17.09693167369728, new_horizon=2
            reward_single_sum=23.75887235615532, confidence_size=2.1857523651973656, confidence_max=17.38536399179756, new_horizon=2
        episode=14, horizon=4, effective_score=15.20, baseline_lowerbound=12.60 baseline_stdev=1.29, new_epsilon=0.0519, bad=False, gap_average=0.04992984868671702
optimal_epsilon = 0.39408626733729607
optimal_horizon = 8
/home/jeffhykin/repos/AFRL/.venv/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(
Traceback (most recent call last):
  File "./main/run/full.py", line 50, in <module>
    full_run(
  File "./main/run/full.py", line 26, in full_run
    results = Tester.smart_load(
  File "/home/jeffhykin/repos/AFRL/main/test_prediction.py", line 675, in smart_load
    return Tester(
  File "/home/jeffhykin/repos/AFRL/main/test_prediction.py", line 451, in run_all_episodes
    normalized_rewards = normalize(rewards, min=settings.min_reward_single_timestep, max=settings.max_reward_single_timestep)
AttributeError: 'LazyDict' object has no attribute 'min_reward_single_timestep'

[silver_spectacle] There were unviewed results
[silver_spectacle] So the display server is still running at: http://localhost:9900
[silver_spectacle] (use the stop server button to kill it now)



-------------------------------------------------------

 Environment: LunarLanderContinuous-v2

-------------------------------------------------------




-----------------------------------------------------------------------------------------------------
 Agent Model Exists, loading: ./subrepos/rl-trained-agents/sac/LunarLanderContinuous-v2_1/LunarLanderContinuous-v2.zip
-----------------------------------------------------------------------------------------------------


Wrapping the env with a `Monitor` wrapper
Wrapping the env in a DummyVecEnv.


-----------------------------------------------------------------------------------------------------
 Coach Model Exists, loading: models.ignore/coach/LunarLanderContinuous-v2/final_1_95%/
-----------------------------------------------------------------------------------------------------


coach settings = {
    "force_retrain": False, 
    "learning_rate": 0.0001, 
    "hidden_sizes": [64, 64, 64, 64, ], 
    "loss_function": "state_prediction_loss", 
    "consistent_coach_loss": {"scale_future_state_loss": 1000000, }, 
    "delayed_coach_loss": {"epochs_of_delay": 50, }, 
    "value_plus_state_loss": {"value_proportion": 0.5, }, 
    "number_of_episodes": 100, 
    "number_of_epochs": 100, 
    "train_test_split": 0.7, 
    "batch_size": 128, 
    "with_card": True, 
}


-----------------------------------------------------------------------------------------------------
 Testing Agent+Coach
-----------------------------------------------------------------------------------------------------


test settings = {
    "force_recompute": True, 
    "graph_smoothing": 0, 
    "number_of_episodes_for_baseline": 30, 
    "number_of_epochs_for_optimal_parameters": 15, 
    "number_of_episodes_for_testing": 10, 
    "initial_epsilon": 10.1, 
    "initial_horizon": 16, 
    "reward_discount": 0.98, 
    "acceptable_performance_loss": 1, 
    "acceptable_performance_level": 0.8, 
    "confidence_interval_for_convergence": 90, 
    "min_reward_single_timestep": -100, 
    "max_reward_single_timestep": 70, 
    "horizons": {0.0: 1, 0.005: 10, 0.01: 20, }, 
}
----- getting a reward baseline -----------------------------------------------------------------------------------------------------------------------------------
  episode_index=0, episode_discounted_reward_sum=24.32979022537819
  episode_index=1, episode_discounted_reward_sum=21.345796166635186
  episode_index=2, episode_discounted_reward_sum=17.56484934550784
  episode_index=3, episode_discounted_reward_sum=52.61281986025887
  episode_index=4, episode_discounted_reward_sum=4.177129225033124
  episode_index=5, episode_discounted_reward_sum=49.0438012663799
  episode_index=6, episode_discounted_reward_sum=17.62224951739184
  episode_index=7, episode_discounted_reward_sum=11.203608807460219
  episode_index=8, episode_discounted_reward_sum=19.581000900020282
  episode_index=9, episode_discounted_reward_sum=8.372994312267267
  episode_index=10, episode_discounted_reward_sum=20.417570292982944
  episode_index=11, episode_discounted_reward_sum=20.16916093671262
  episode_index=12, episode_discounted_reward_sum=22.387483036529634
  episode_index=13, episode_discounted_reward_sum=30.153366596600307
  episode_index=14, episode_discounted_reward_sum=19.45889338533932
  episode_index=15, episode_discounted_reward_sum=20.410612866183982
  episode_index=16, episode_discounted_reward_sum=21.204723606674474
  episode_index=17, episode_discounted_reward_sum=19.512562605862037
  episode_index=18, episode_discounted_reward_sum=22.97829725795669
  episode_index=19, episode_discounted_reward_sum=19.65784483369008
  episode_index=20, episode_discounted_reward_sum=41.38566990582241
  episode_index=21, episode_discounted_reward_sum=22.513149226718742
  episode_index=22, episode_discounted_reward_sum=33.46033230877779
  episode_index=23, episode_discounted_reward_sum=19.039150756366208
  episode_index=24, episode_discounted_reward_sum=19.40053428400315
  episode_index=25, episode_discounted_reward_sum=54.803643297186525
  episode_index=26, episode_discounted_reward_sum=21.679727998410407
  episode_index=27, episode_discounted_reward_sum=7.658308295312798
  episode_index=28, episode_discounted_reward_sum=21.994713244121684
  episode_index=29, episode_discounted_reward_sum=14.077647388577674
baseline = {
    "max": 54.803643297186525, 
    "min": 4.177129225033124, 
    "range": 50.6265140721534, 
    "count": 30, 
    "sum": 698.2174317501623, 
    "average": 23.273914391672076, 
    "stdev": 12.109515355057265, 
    "median": 20.414091579583463, 
}
baseline_min = 19.517340093961845, baseline_max = 27.0304886893823,
baseline_confidence_size = 3.7565742977102268
----- finding optimal epsilon -------------------------------------------------------------------------------------------------------------------------------------
            reward_single_sum=-16.050418717331524, 
            reward_single_sum=-37.0116616521878, confidence_size=66.1720396660296, confidence_max=39.6409994812699, new_horizon=32
            reward_single_sum=-23.11768632548818, confidence_size=17.97844139487806, confidence_max=-7.414814170124448, new_horizon=31
        episode=0, horizon=32, effective_score=-25.39, baseline_lowerbound=18.62 baseline_stdev=2.21, new_epsilon=6.7333, bad=True, gap_average=1.2168317602462126
            reward_single_sum=14.140151892856963, 
            reward_single_sum=14.394511884693022, confidence_size=0.8029828918798367, confidence_max=15.07031478065483, new_horizon=14
        episode=1, horizon=32, effective_score=14.27, baseline_lowerbound=18.62 baseline_stdev=2.21, new_epsilon=4.4889, bad=True, gap_average=1.219717326594013
            reward_single_sum=-8.295848174715669, 
            reward_single_sum=-4.153147451875139, confidence_size=13.077991482100666, confidence_max=6.853493668805253, new_horizon=23
        episode=2, horizon=32, effective_score=-6.22, baseline_lowerbound=18.62 baseline_stdev=2.21, new_epsilon=2.9926, bad=True, gap_average=1.1386485713900942
            reward_single_sum=20.600232923758043, 
            reward_single_sum=10.999535850885248, confidence_size=30.308207843497758, confidence_max=46.10809223081939, new_horizon=6
            reward_single_sum=21.3554247310562, confidence_size=9.732996512261376, confidence_max=27.38472768082787, new_horizon=4
            reward_single_sum=19.12130880962668, confidence_size=5.61374836857941, confidence_max=23.63287394741095, new_horizon=5
            reward_single_sum=14.180612607293371, confidence_size=4.265553205385226, confidence_max=21.516976189909133, new_horizon=6
            reward_single_sum=9.747310231591083, confidence_size=4.145913653171513, confidence_max=20.14665117887328, new_horizon=6
            reward_single_sum=20.28208945305166, confidence_size=3.581888370949362, confidence_max=20.19424760055826, new_horizon=6
        episode=3, horizon=32, effective_score=16.61, baseline_lowerbound=18.62 baseline_stdev=2.21, new_epsilon=1.9951, bad=True, gap_average=1.1110567827502218
            reward_single_sum=53.06428855801252, 
            reward_single_sum=12.96163429160131, confidence_size=126.599097061046, confidence_max=159.61205848585286, new_horizon=4
            reward_single_sum=8.477274679029279, confidence_size=41.38840894371866, confidence_max=66.22280811993302, new_horizon=4
            reward_single_sum=17.394702573439588, confidence_size=23.989656880857602, confidence_max=46.96413190637827, new_horizon=7
            reward_single_sum=11.0459138184177, confidence_size=17.584757532714853, confidence_max=38.17352031681493, new_horizon=8
            reward_single_sum=25.170351952022692, confidence_size=13.658210633331288, confidence_max=35.01057161208513, new_horizon=8
            reward_single_sum=-5.869275671529523, confidence_size=13.454195129722864, confidence_max=30.917750872721943, new_horizon=6
            reward_single_sum=12.851812254277533, confidence_size=11.412629026803796, confidence_max=28.299716833712683, new_horizon=6
            reward_single_sum=14.026935752596517, confidence_size=9.89655167290668, confidence_max=26.46584480711419, new_horizon=14
            reward_single_sum=17.758472465765003, confidence_size=8.728632082683184, confidence_max=25.416843150046446, new_horizon=12
            reward_single_sum=50.68209329744226, confidence_size=9.607946647729161, confidence_max=29.3865106450996, new_horizon=10
            reward_single_sum=-13.434104369244325, confidence_size=10.011618055701955, confidence_max=27.02245968918783, new_horizon=8
            reward_single_sum=36.540101206171684, confidence_size=9.523727756985002, confidence_max=28.036820126831326, new_horizon=9
            reward_single_sum=13.158332656109794, confidence_size=8.78723300730414, confidence_max=26.917842540455, new_horizon=8
            reward_single_sum=23.67786398556123, confidence_size=8.162048136814612, confidence_max=26.662474633459496, new_horizon=8
            reward_single_sum=38.89621507085083, confidence_size=7.9208580779659155, confidence_max=27.69602136049867, new_horizon=8
            reward_single_sum=20.612594187195455, confidence_size=7.410432641175536, confidence_max=27.23485656515904, new_horizon=10
            reward_single_sum=53.158690155564585, confidence_size=7.670797998252892, confidence_max=29.347125601768674, new_horizon=8
            reward_single_sum=8.145283328229139, confidence_size=7.337398298037586, confidence_max=28.30156567653828, new_horizon=12
            reward_single_sum=-5.9352981882665325, confidence_size=7.3203209221729955, confidence_max=26.93951502233533, new_horizon=12
            reward_single_sum=20.808573280525543, confidence_size=6.945924052749672, confidence_max=26.621755256738826, new_horizon=12
            reward_single_sum=25.10816353304494, confidence_size=6.62106136382609, confidence_max=26.54381676459051, new_horizon=11
            reward_single_sum=12.934550452894074, confidence_size=6.33493316833029, confidence_max=25.953853571361215, new_horizon=8
            reward_single_sum=19.61017495790307, confidence_size=6.053676540057524, confidence_max=25.672232549541455, new_horizon=9
            reward_single_sum=40.318614691488534, confidence_size=5.966952476806151, confidence_max=26.413510833570264, new_horizon=10
            reward_single_sum=53.43197796491048, confidence_size=6.120184267989998, confidence_max=27.83541260968282, new_horizon=10
            reward_single_sum=4.830947952216535, confidence_size=5.976399136464433, confidence_max=27.066283760028504, new_horizon=10
            reward_single_sum=15.96491769566143, confidence_size=5.759580703917381, confidence_max=26.66643079434207, new_horizon=10
            reward_single_sum=13.987540832941512, confidence_size=5.565208172495646, confidence_max=26.23346139197264, new_horizon=10
            reward_single_sum=33.731800505598784, confidence_size=5.420898698579404, confidence_max=26.524603494260457, new_horizon=9
        hit cap of: 30 iterations
        episode=4, horizon=6, effective_score=21.10, baseline_lowerbound=18.62 baseline_stdev=2.21, new_epsilon=2.9926, bad=False, gap_average=0.6637387346204966
            reward_single_sum=27.48365888482864, 
            reward_single_sum=-8.280553111395584, confidence_size=112.90317383341122, confidence_max=122.50472672012768, new_horizon=6
            reward_single_sum=25.169272186620155, confidence_size=33.74044466818789, confidence_max=48.53123732153895, new_horizon=7
            reward_single_sum=7.885638859700876, confidence_size=19.65292789518442, confidence_max=32.717432100122934, new_horizon=8
            reward_single_sum=19.746400324736474, confidence_size=14.08139051413356, confidence_max=28.482273943031668, new_horizon=6
            reward_single_sum=16.7643000665305, confidence_size=10.896441340260097, confidence_max=25.69122754209694, new_horizon=6
            reward_single_sum=10.9242570859581, confidence_size=8.945482968310927, confidence_max=23.18733643930795, new_horizon=6
            reward_single_sum=12.844743545610456, confidence_size=7.560494995899462, confidence_max=21.627709726223163, new_horizon=6
            reward_single_sum=23.728524328615276, confidence_size=6.842112707237918, confidence_max=21.982806281816238, new_horizon=6
            reward_single_sum=6.635205128199162, confidence_size=6.230996239601808, confidence_max=20.521140969542213, new_horizon=6
            reward_single_sum=9.479706902677226, confidence_size=5.628736786416473, confidence_max=19.48156898660568, new_horizon=6
        episode=5, horizon=9, effective_score=13.85, baseline_lowerbound=18.62 baseline_stdev=2.21, new_epsilon=1.9951, bad=True, gap_average=0.9501413043166386
            reward_single_sum=33.05163228657248, 
            reward_single_sum=12.064477136509758, confidence_size=66.25384131003537, confidence_max=88.81189602157646, new_horizon=8
            reward_single_sum=24.09899586224736, confidence_size=17.754110259167895, confidence_max=40.82581202094442, new_horizon=8
            reward_single_sum=27.177889152319864, confidence_size=10.402357132698004, confidence_max=34.500605742110366, new_horizon=8
            reward_single_sum=-9.495215125317273, confidence_size=16.07584627586549, confidence_max=33.45540213833192, new_horizon=8
            reward_single_sum=26.50392193959068, confidence_size=12.779569306283095, confidence_max=31.679852848270237, new_horizon=8
            reward_single_sum=17.28308784757673, confidence_size=10.425163070460503, confidence_max=29.0944186561033, new_horizon=8
            reward_single_sum=10.275816215496691, confidence_size=9.024281990230204, confidence_max=26.64435765460474, new_horizon=8
            reward_single_sum=23.903475629739297, confidence_size=7.918662037621723, confidence_max=26.236893253703453, new_horizon=8
            reward_single_sum=4.368748884700414, confidence_size=7.435509242929582, confidence_max=24.35879222587318, new_horizon=8
            reward_single_sum=-3.640657609859465, confidence_size=7.463361979958968, confidence_max=22.517195818102287, new_horizon=8
            reward_single_sum=15.26894557980998, confidence_size=6.7508522086062035, confidence_max=21.822612025221744, new_horizon=8
            reward_single_sum=10.873391928323482, confidence_size=6.189684588757144, confidence_max=20.93849302935022, new_horizon=8
            reward_single_sum=10.741219451840692, confidence_size=5.716549731009927, confidence_max=20.179101815263547, new_horizon=8
            reward_single_sum=51.0112403764788, confidence_size=6.81413027736345, confidence_max=23.71326158109875, new_horizon=8
            reward_single_sum=45.293293735912236, confidence_size=7.065878239470125, confidence_max=25.73964469521648, new_horizon=8
            reward_single_sum=32.95723268846947, confidence_size=6.770912280623829, confidence_max=26.284882632412724, new_horizon=8
            reward_single_sum=20.791982676239012, confidence_size=6.3619252596513505, confidence_max=25.946896296131918, new_horizon=8
            reward_single_sum=17.517666218328078, confidence_size=6.001570162801358, confidence_max=25.477735682537055, new_horizon=8
            reward_single_sum=20.616986829823205, confidence_size=5.678256384808177, confidence_max=25.21146297004825, new_horizon=8
            reward_single_sum=11.41856171184249, confidence_size=5.428375897331668, confidence_max=24.57517082193376, new_horizon=8
            reward_single_sum=21.458429657106038, confidence_size=5.166988027749772, confidence_max=24.41885725837477, new_horizon=8
            reward_single_sum=18.26137613382595, confidence_size=4.927457888225626, confidence_max=24.136262201598495, new_horizon=8
            reward_single_sum=17.430172854253925, confidence_size=4.710402698600227, confidence_max=23.845097367843138, new_horizon=8
            reward_single_sum=6.387144759090895, confidence_size=4.593774478709507, confidence_max=23.21856715154634, new_horizon=8
            reward_single_sum=14.995140603920566, confidence_size=4.412931480636528, confidence_max=22.898122150822733, new_horizon=8
            reward_single_sum=15.526600658316006, confidence_size=4.244190749359416, confidence_max=22.619804011698577, new_horizon=8
            reward_single_sum=-9.005515173664719, confidence_size=4.410805454513479, confidence_max=21.808521272709644, new_horizon=8
            reward_single_sum=15.78356186722652, confidence_size=4.251655352367477, confidence_max=21.593710689495722, new_horizon=8
            reward_single_sum=38.55905403766527, confidence_size=4.275017656591851, confidence_max=22.324306283737997, new_horizon=8
        hit cap of: 30 iterations
        episode=6, horizon=32, effective_score=18.05, baseline_lowerbound=18.62 baseline_stdev=2.21, new_epsilon=1.3300, bad=True, gap_average=0.6805917614988991
            reward_single_sum=17.29780990357742, 
            reward_single_sum=46.35273924452709, confidence_size=91.72280206942759, confidence_max=123.54807664347979, new_horizon=4
            reward_single_sum=19.069006632030018, confidence_size=27.458632138254657, confidence_max=55.031817398299495, new_horizon=7
            reward_single_sum=23.951292739962163, confidence_size=15.79290136732125, confidence_max=42.46061349734542, new_horizon=8
            reward_single_sum=21.606281745137068, confidence_size=11.289825419106256, confidence_max=36.945251472153004, new_horizon=8
            reward_single_sum=60.87589834554375, confidence_size=14.691180474487046, confidence_max=46.216685242949964, new_horizon=8
            reward_single_sum=20.653954955207755, confidence_size=12.347953215322999, confidence_max=42.32037943903518, new_horizon=14
            reward_single_sum=6.290665440796814, confidence_size=11.83887667712346, confidence_max=38.85108280297122, new_horizon=14
            reward_single_sum=19.20652953165308, confidence_size=10.373987945304314, confidence_max=36.51889667179711, new_horizon=14
            reward_single_sum=24.50759589616208, confidence_size=9.151793868206958, confidence_max=35.13297131166668, new_horizon=14
            reward_single_sum=22.786775315415028, confidence_size=8.201754085563895, confidence_max=33.89253133556501, new_horizon=14
            reward_single_sum=52.50554593729193, confidence_size=8.43450962638363, confidence_max=36.359850933658976, new_horizon=14
            reward_single_sum=48.4289496690052, confidence_size=8.19694889755782, confidence_max=37.699490848043155, new_horizon=14
            reward_single_sum=34.66180436734479, confidence_size=7.568743524362185, confidence_max=37.43980421890891, new_horizon=12
            reward_single_sum=22.146036398409777, confidence_size=7.066289324274665, confidence_max=36.42234839907893, new_horizon=12
            reward_single_sum=20.846803147636663, confidence_size=6.644643858696478, confidence_max=35.468874438052765, new_horizon=10
            reward_single_sum=35.62122948694327, confidence_size=6.25511140560816, confidence_max=35.479165450116625, new_horizon=10
            reward_single_sum=19.045521458692583, confidence_size=5.957942406911759, confidence_max=34.616522418874894, new_horizon=8
            reward_single_sum=24.261480572654143, confidence_size=5.632007528919313, confidence_max=34.059161254603026, new_horizon=9
            reward_single_sum=10.781046340807327, confidence_size=5.541927941340267, confidence_max=33.08677629778016, new_horizon=8
            reward_single_sum=47.93720236413053, confidence_size=5.518261621378617, confidence_max=34.03417445437521, new_horizon=8
            reward_single_sum=47.539692707863814, confidence_size=5.456140201574586, confidence_max=34.8367703016106, new_horizon=8
            reward_single_sum=10.4858694078169, confidence_size=5.3904725671216, confidence_max=33.949591332713304, new_horizon=8
            reward_single_sum=21.769588267705867, confidence_size=5.173915600770663, confidence_max=33.45013726228379, new_horizon=9
            reward_single_sum=18.03264190566664, confidence_size=5.003343137994765, confidence_max=32.869821609274034, new_horizon=10
            reward_single_sum=26.810274020635646, confidence_size=4.799855932407178, confidence_max=32.62571115558477, new_horizon=10
            reward_single_sum=53.47584915269236, confidence_size=4.888207983260893, confidence_max=33.664062981605696, new_horizon=10
            reward_single_sum=29.319589051421907, confidence_size=4.704078068421927, confidence_max=33.49935214009091, new_horizon=8
            reward_single_sum=8.060590680857212, confidence_size=4.693555067843668, confidence_max=32.77384005707087, new_horizon=10
            reward_single_sum=15.050209113675578, confidence_size=4.58879595051925, confidence_max=32.23474507722806, new_horizon=10
        hit cap of: 30 iterations
        episode=7, horizon=8, effective_score=27.65, baseline_lowerbound=18.62 baseline_stdev=2.21, new_epsilon=1.9951, bad=False, gap_average=0.5887840558923668
            reward_single_sum=26.581968354808566, 
            reward_single_sum=7.738239010089679, confidence_size=59.487312347358866, confidence_max=76.64741602980796, new_horizon=8
            reward_single_sum=21.597306817217117, confidence_size=16.46057606061988, confidence_max=35.09974745465833, new_horizon=8
            reward_single_sum=19.393477766309587, confidence_size=9.391258132535334, confidence_max=28.21900611964157, new_horizon=8
            reward_single_sum=9.060513969221553, confidence_size=7.795314821270198, confidence_max=24.669616004799497, new_horizon=8
            reward_single_sum=-1.7040238743092475, confidence_size=8.667388115780923, confidence_max=22.445301789670467, new_horizon=8
            reward_single_sum=7.273103613003853, confidence_size=7.291157310911991, confidence_max=20.139812404675006, new_horizon=8
            reward_single_sum=13.484069467140998, confidence_size=6.158236046627032, confidence_max=19.086317937062294, new_horizon=8
        episode=8, horizon=10, effective_score=12.93, baseline_lowerbound=18.62 baseline_stdev=2.21, new_epsilon=1.3300, bad=True, gap_average=0.6929504946179149
            reward_single_sum=48.636712136248946, 
            reward_single_sum=30.649464687010315, confidence_size=56.78350541486482, confidence_max=96.42659382649443, new_horizon=10
            reward_single_sum=25.12505988577343, confidence_size=20.725932391963084, confidence_max=55.529677961640644, new_horizon=10
            reward_single_sum=18.20450091321248, confidence_size=15.326057847624213, confidence_max=45.9799922531855, new_horizon=10
            reward_single_sum=13.854263253752356, confidence_size=12.921175250802609, confidence_max=40.21517542600211, new_horizon=10
            reward_single_sum=33.799445650785884, confidence_size=10.20861288026056, confidence_max=38.586853968057795, new_horizon=10
            reward_single_sum=17.155265774982546, confidence_size=8.884298791082475, confidence_max=35.65925769133475, new_horizon=10
            reward_single_sum=32.124586996971274, confidence_size=7.607819421978482, confidence_max=35.05148183432063, new_horizon=10
            reward_single_sum=47.36458884988958, confidence_size=7.7658863309666835, confidence_max=37.422985014147436, new_horizon=10
            reward_single_sum=-49.15864143426167, confidence_size=15.988263070515771, confidence_max=37.763787741952285, new_horizon=10
            reward_single_sum=36.26676287719266, confidence_size=14.496984774163565, confidence_max=37.58989473703245, new_horizon=10
            reward_single_sum=32.16532561730311, confidence_size=13.182948155654914, confidence_max=37.03189275639332, new_horizon=10
            reward_single_sum=39.297237579843824, confidence_size=12.219677796749494, confidence_max=37.256952626649856, new_horizon=10
            reward_single_sum=6.443124154235961, confidence_size=11.484586840859215, confidence_max=35.193708051069265, new_horizon=10
            reward_single_sum=28.769387578513943, confidence_size=10.650058397199398, confidence_max=34.69653069862971, new_horizon=10
            reward_single_sum=18.840352458026672, confidence_size=9.931892518246439, confidence_max=33.65298232946402, new_horizon=10
            reward_single_sum=19.773873649623358, confidence_size=9.30008788850952, confidence_max=32.78898851375097, new_horizon=10
            reward_single_sum=47.76025984954147, confidence_size=9.04610076058372, confidence_max=33.883410231619614, new_horizon=10
            reward_single_sum=20.923208955983224, confidence_size=8.536966533905165, confidence_max=33.16827071467513, new_horizon=10
            reward_single_sum=-0.21063700789135176, confidence_size=8.356564009478756, confidence_max=31.745771130815655, new_horizon=10
            reward_single_sum=20.67218589384637, confidence_size=7.931524137570365, confidence_max=31.191349295693428, new_horizon=10
            reward_single_sum=51.119869533503696, confidence_size=7.853353861805946, confidence_max=32.379544673355404, new_horizon=10
            reward_single_sum=5.68206489669819, confidence_size=7.619456705777649, confidence_max=31.326337694942268, new_horizon=10
            reward_single_sum=16.53899875658264, confidence_size=7.299140152694077, confidence_max=30.70735938216778, new_horizon=10
            reward_single_sum=19.45859323030862, confidence_size=6.994102357695111, confidence_max=30.24433654720221, new_horizon=10
            reward_single_sum=20.003240579145054, confidence_size=6.71233983887862, confidence_max=29.83768965875641, new_horizon=10
            reward_single_sum=18.901970216386157, confidence_size=6.454928411710069, confidence_max=29.42385676479187, new_horizon=10
            reward_single_sum=27.4158698644234, confidence_size=6.217517073555376, confidence_max=29.34526476632795, new_horizon=10
            reward_single_sum=4.869704424866469, confidence_size=6.086659706324426, confidence_max=28.58481970020369, new_horizon=10
            reward_single_sum=34.30084576841017, confidence_size=5.911263950284338, confidence_max=28.80284680331463, new_horizon=10
        hit cap of: 30 iterations
        episode=9, horizon=8, effective_score=22.89, baseline_lowerbound=18.62 baseline_stdev=2.21, new_epsilon=1.9951, bad=False, gap_average=0.47640016253019407
            reward_single_sum=13.016788393110897, 
            reward_single_sum=29.236264720775402, confidence_size=51.20287161653485, confidence_max=72.32939817347797, new_horizon=8
            reward_single_sum=46.53431909807349, confidence_size=28.25771497754782, confidence_max=57.853505714867744, new_horizon=10
            reward_single_sum=22.03060538574022, confidence_size=16.70764210145254, confidence_max=44.41213650087754, new_horizon=8
            reward_single_sum=9.189208872159247, confidence_size=14.133701877863604, confidence_max=38.13513917183545, new_horizon=8
            reward_single_sum=0.3701574847750617, confidence_size=13.489528429649152, confidence_max=33.55241908875487, new_horizon=8
            reward_single_sum=49.17183093259045, confidence_size=13.644268514710795, confidence_max=37.86557921288576, new_horizon=8
            reward_single_sum=17.608388503824038, confidence_size=11.626698145662889, confidence_max=35.02139356954399, new_horizon=8
            reward_single_sum=34.94813559756385, confidence_size=10.34342131340634, confidence_max=35.02183231214108, new_horizon=8
            reward_single_sum=8.931578021472289, confidence_size=9.56583714268799, confidence_max=32.669564843696485, new_horizon=10
            reward_single_sum=9.793355958004206, confidence_size=8.83178153489324, confidence_max=30.725475441083162, new_horizon=8
            reward_single_sum=10.321308623014556, confidence_size=8.174119485144542, confidence_max=29.10344795106985, new_horizon=8
            reward_single_sum=12.685298708257294, confidence_size=7.547277188902088, confidence_max=27.842449519622164, new_horizon=8
            reward_single_sum=19.172894823075783, confidence_size=6.944358391956613, confidence_max=27.15936804355924, new_horizon=8
            reward_single_sum=3.7653403519114237, confidence_size=6.713572730314711, confidence_max=25.831937761937922, new_horizon=8
            reward_single_sum=11.430482273818336, confidence_size=6.30702309304439, confidence_max=24.9448954523048, new_horizon=8
            reward_single_sum=7.192679037560766, confidence_size=6.016137688284495, confidence_max=23.98076338156257, new_horizon=8
            reward_single_sum=48.346325911555894, confidence_size=6.368902140391844, confidence_max=26.021400068018686, new_horizon=8
            reward_single_sum=27.643496256305085, confidence_size=6.049306231520291, confidence_max=26.122383018551254, new_horizon=8
            reward_single_sum=22.15497891756607, confidence_size=5.7253873641960995, confidence_max=25.902559257753815, new_horizon=8
            reward_single_sum=41.31267587843149, confidence_size=5.70263717612222, confidence_max=26.886261640388213, new_horizon=8
            reward_single_sum=27.371683688312636, confidence_size=5.446267743496197, confidence_max=26.911167627037038, new_horizon=8
            reward_single_sum=20.742803588184017, confidence_size=5.193485712300779, confidence_max=26.626990104739146, new_horizon=10
            reward_single_sum=20.02017321885747, confidence_size=4.963932879394614, confidence_max=26.338548472933777, new_horizon=10
            reward_single_sum=-5.84158304445948, confidence_size=5.104845874470468, confidence_max=25.390813522489687, new_horizon=8
            reward_single_sum=-9.586872796876456, confidence_size=5.275373460478038, confidence_max=24.4123857067705, new_horizon=8
            reward_single_sum=15.016758361341434, confidence_size=5.075411471172772, confidence_max=24.05982172172631, new_horizon=8
            reward_single_sum=55.08172680702424, confidence_size=5.355028811519283, confidence_max=25.62862893908963, new_horizon=8
            reward_single_sum=18.059416295977854, confidence_size=5.162163395124386, confidence_max=25.359412356088093, new_horizon=8
            reward_single_sum=29.22661904466402, confidence_size=5.007431671982092, confidence_max=25.50565963573581, new_horizon=8
        hit cap of: 30 iterations
        episode=10, horizon=6, effective_score=20.50, baseline_lowerbound=18.62 baseline_stdev=2.21, new_epsilon=2.9926, bad=False, gap_average=0.8141817472392221
            reward_single_sum=-11.044065705655058, 
            reward_single_sum=-8.728191516522513, confidence_size=7.310927084861998, confidence_max=-2.5752015262267918, new_horizon=6
        episode=11, horizon=8, effective_score=-9.89, baseline_lowerbound=18.62 baseline_stdev=2.21, new_epsilon=1.9951, bad=True, gap_average=1.7734526508259323
            reward_single_sum=12.241305323703127, 
            reward_single_sum=1.4776336747871444, confidence_size=33.97957408908158, confidence_max=40.839043588326696, new_horizon=8
            reward_single_sum=41.271830752328604, confidence_size=34.70162698725848, confidence_max=53.031883570864764, new_horizon=8
            reward_single_sum=29.622501482450247, confidence_size=20.86233951833738, confidence_max=42.01565732665465, new_horizon=8
            reward_single_sum=15.313225492399665, confidence_size=14.84907028738288, confidence_max=34.83436963251663, new_horizon=8
            reward_single_sum=22.373668744760696, confidence_size=11.487997252005762, confidence_max=31.871358163744006, new_horizon=9
            reward_single_sum=3.0841288136376406, confidence_size=10.522556789697347, confidence_max=28.43459883027836, new_horizon=10
            reward_single_sum=10.251657241203073, confidence_size=9.068196558691607, confidence_max=26.022690499350382, new_horizon=10
            reward_single_sum=38.261866638363436, confidence_size=8.999814759829583, confidence_max=28.321794555788877, new_horizon=10
            reward_single_sum=17.2739916815177, confidence_size=7.944121442655555, confidence_max=27.061302427170688, new_horizon=8
            reward_single_sum=29.773590582551197, confidence_size=7.318525481291397, confidence_max=27.404470974718897, new_horizon=8
            reward_single_sum=41.4304520423174, confidence_size=7.350186095232888, confidence_max=29.214840467734547, new_horizon=8
            reward_single_sum=14.947483565696201, confidence_size=6.776680433984334, confidence_max=28.109244744424036, new_horizon=8
            reward_single_sum=29.339696581580526, confidence_size=6.315764121921653, confidence_max=28.220266451728556, new_horizon=8
            reward_single_sum=29.286623533879343, confidence_size=5.911601756520318, confidence_max=28.30824549993205, new_horizon=8
            reward_single_sum=-8.708461133506765, confidence_size=6.473587754417336, confidence_max=26.92616244302166, new_horizon=8
            reward_single_sum=52.54162605940696, confidence_size=6.894618592689326, confidence_max=29.23478453839969, new_horizon=8
            reward_single_sum=26.72997130037029, confidence_size=6.490818069876351, confidence_max=29.0748620908456, new_horizon=8
            reward_single_sum=10.31041228432641, confidence_size=6.221803877391446, confidence_max=28.15986728064265, new_horizon=8
            reward_single_sum=19.43515434140471, confidence_size=5.889714228695384, confidence_max=27.702632178854262, new_horizon=8
            reward_single_sum=5.819520155206918, confidence_size=5.7402397050353935, confidence_max=26.791567284006085, new_horizon=8
            reward_single_sum=19.495362673812448, confidence_size=5.461844772173529, confidence_max=26.442446673637026, new_horizon=8
            reward_single_sum=7.390887879503514, confidence_size=5.305965204175314, confidence_max=25.695709974249247, new_horizon=8
            reward_single_sum=19.871321862758304, confidence_size=5.0705275396601, confidence_max=25.43867135526255, new_horizon=8
            reward_single_sum=17.55315979671644, confidence_size=4.858816479910844, confidence_max=25.11436093475785, new_horizon=8
            reward_single_sum=9.585906544592351, confidence_size=4.713138233026241, confidence_max=24.5583119990173, new_horizon=8
            reward_single_sum=22.17796303239403, confidence_size=4.530918176409449, confidence_max=24.462491544859876, new_horizon=8
            reward_single_sum=3.0515462598595837, confidence_size=4.479421219791771, confidence_max=23.808136477221097, new_horizon=8
            reward_single_sum=32.6412698505332, confidence_size=4.386790502891558, confidence_max=24.17455936697964, new_horizon=8
            reward_single_sum=27.319213714901107, confidence_size=4.254487896277235, confidence_max=24.293304922059082, new_horizon=8
        hit cap of: 30 iterations
        episode=12, horizon=6, effective_score=20.04, baseline_lowerbound=18.62 baseline_stdev=2.21, new_epsilon=2.9926, bad=False, gap_average=0.8229371626579205
            reward_single_sum=19.672618006990202, 
            reward_single_sum=20.052739713542746, confidence_size=1.1999970002774187, confidence_max=21.06267586054389, new_horizon=6
        episode=13, horizon=23, effective_score=19.86, baseline_lowerbound=18.62 baseline_stdev=2.21, new_epsilon=4.4889, bad=False, gap_average=1.0309341637984566
            reward_single_sum=-8.552055298571865, 
            reward_single_sum=-13.65781656409346, confidence_size=16.118253962199454, confidence_max=5.013318030866785, new_horizon=15
        episode=14, horizon=6, effective_score=-11.10, baseline_lowerbound=18.62 baseline_stdev=2.21, new_epsilon=2.9926, bad=True, gap_average=0.0403478498812075
optimal_epsilon = 2.9925925925925925
optimal_horizon = 6
    scaled_epsilon: 2.9926, forecast_average: 4.5182, episode_reward:83.69, max_timestep_reward: 19.02, min_timestep_reward: -100.00
    scaled_epsilon: 2.9926, forecast_average: 4.6003, episode_reward:254.15, max_timestep_reward: 100.00, min_timestep_reward: -2.63
    scaled_epsilon: 2.9926, forecast_average: 4.5993, episode_reward:296.68, max_timestep_reward: 100.00, min_timestep_reward: -2.43
    scaled_epsilon: 2.9926, forecast_average: 4.5972, episode_reward:264.26, max_timestep_reward: 100.00, min_timestep_reward: -9.92
    scaled_epsilon: 2.9926, forecast_average: 4.6155, episode_reward:46.87, max_timestep_reward: 20.92, min_timestep_reward: -100.00
    scaled_epsilon: 2.9926, forecast_average: 4.6077, episode_reward:77.73, max_timestep_reward: 16.00, min_timestep_reward: -100.00
    scaled_epsilon: 2.9926, forecast_average: 4.6057, episode_reward:266.36, max_timestep_reward: 100.00, min_timestep_reward: -9.82
    scaled_epsilon: 2.9926, forecast_average: 4.6214, episode_reward:290.08, max_timestep_reward: 100.00, min_timestep_reward: -12.72
    scaled_epsilon: 2.9926, forecast_average: 4.6221, episode_reward:238.09, max_timestep_reward: 100.00, min_timestep_reward: -10.61
    scaled_epsilon: 2.9926, forecast_average: 4.6339, episode_reward:304.43, max_timestep_reward: 100.00, min_timestep_reward: -1.92
self.recorder = {
    number_of_records: 0,
    records: [ ... ],
    local_data: {},
    parent_data:    {
    }
}


-------------------------------------------------------

 Environment: HopperBulletEnv-v0

-------------------------------------------------------




-----------------------------------------------------------------------------------------------------
 Agent Model Exists, loading: ./subrepos/rl-trained-agents/sac/HopperBulletEnv-v0_1/HopperBulletEnv-v0.zip
-----------------------------------------------------------------------------------------------------


pybullet build time: Oct 11 2021 20:52:37
Wrapping the env with a `Monitor` wrapper
Wrapping the env in a DummyVecEnv.


-----------------------------------------------------------------------------------------------------
 Coach Model Exists, loading: models.ignore/coach/HopperBulletEnv-v0/final_1_95%/
-----------------------------------------------------------------------------------------------------


coach settings = {
    "force_retrain": False, 
    "learning_rate": 0.0001, 
    "hidden_sizes": [64, 64, 64, 64, ], 
    "loss_function": "state_prediction_loss", 
    "consistent_coach_loss": {"scale_future_state_loss": 1000000, }, 
    "delayed_coach_loss": {"epochs_of_delay": 50, }, 
    "value_plus_state_loss": {"value_proportion": 0.5, }, 
    "number_of_episodes": 100, 
    "number_of_epochs": 100, 
    "train_test_split": 0.7, 
    "batch_size": 128, 
    "with_card": True, 
}


-----------------------------------------------------------------------------------------------------
 Testing Agent+Coach
-----------------------------------------------------------------------------------------------------


test settings = {
    "force_recompute": True, 
    "graph_smoothing": 0, 
    "number_of_episodes_for_baseline": 30, 
    "number_of_epochs_for_optimal_parameters": 15, 
    "number_of_episodes_for_testing": 10, 
    "initial_epsilon": 10.1, 
    "initial_horizon": 16, 
    "reward_discount": 0.98, 
    "acceptable_performance_loss": 1, 
    "acceptable_performance_level": 0.8, 
    "confidence_interval_for_convergence": 90, 
    "min_reward_single_timestep": 18, 
    "max_reward_single_timestep": 110, 
    "horizons": {0.1: 26, 0.01: 16, 0.001: 11, }, 
}
----- getting a reward baseline -----------------------------------------------------------------------------------------------------------------------------------
  episode_index=0, episode_discounted_reward_sum=112.02948699544818
  episode_index=1, episode_discounted_reward_sum=107.36386076594235
  episode_index=2, episode_discounted_reward_sum=111.43243375992068
  episode_index=3, episode_discounted_reward_sum=109.85109490707806
  episode_index=4, episode_discounted_reward_sum=110.34763754537701
  episode_index=5, episode_discounted_reward_sum=110.17463546021075
  episode_index=6, episode_discounted_reward_sum=106.67362848428458
  episode_index=7, episode_discounted_reward_sum=107.65116721400817
  episode_index=8, episode_discounted_reward_sum=110.19187900798993
  episode_index=9, episode_discounted_reward_sum=111.6764916749992
  episode_index=10, episode_discounted_reward_sum=110.86760817539172
  episode_index=11, episode_discounted_reward_sum=111.89608866471453
  episode_index=12, episode_discounted_reward_sum=108.72435577483556
  episode_index=13, episode_discounted_reward_sum=109.703240441655
  episode_index=14, episode_discounted_reward_sum=111.69003526006689
  episode_index=15, episode_discounted_reward_sum=111.4663847618949
  episode_index=16, episode_discounted_reward_sum=107.65345207645467
  episode_index=17, episode_discounted_reward_sum=111.40227615518663
  episode_index=18, episode_discounted_reward_sum=111.11769193416124
  episode_index=19, episode_discounted_reward_sum=110.37588235924395
  episode_index=20, episode_discounted_reward_sum=104.80266482849875
  episode_index=21, episode_discounted_reward_sum=111.54742659508345
  episode_index=22, episode_discounted_reward_sum=110.44461836055157
  episode_index=23, episode_discounted_reward_sum=110.56836841244343
  episode_index=24, episode_discounted_reward_sum=107.33359315431242
  episode_index=25, episode_discounted_reward_sum=111.53993739122242
  episode_index=26, episode_discounted_reward_sum=110.72492651452802
  episode_index=27, episode_discounted_reward_sum=112.44806109494188
  episode_index=28, episode_discounted_reward_sum=110.70864176276613
  episode_index=29, episode_discounted_reward_sum=110.67531051171864
baseline = {
    "max": 112.44806109494188, 
    "min": 104.80266482849875, 
    "range": 7.645396266443129, 
    "count": 30, 
    "sum": 3303.08288004493, 
    "average": 110.10276266816433, 
    "stdev": 1.8497360494642465, 
    "median": 110.62183946208103, 
}
baseline_min = 109.52894359306293, baseline_max = 110.67658174326579,
baseline_confidence_size = 0.5738190751014258
----- finding optimal epsilon -------------------------------------------------------------------------------------------------------------------------------------
            reward_single_sum=24.404428904027927, 
            reward_single_sum=27.677714877960145, confidence_size=10.333357138145598, confidence_max=36.37442902913963, new_horizon=20
        episode=0, horizon=32, effective_score=26.04, baseline_lowerbound=88.08 baseline_stdev=0.34, new_epsilon=6.7333, bad=True, gap_average=2.540688943862915
            reward_single_sum=22.21022585923779, 
            reward_single_sum=21.82146915276831, confidence_size=1.2272566221803594, confidence_max=23.243104128183408, new_horizon=18
        episode=1, horizon=32, effective_score=22.02, baseline_lowerbound=88.08 baseline_stdev=0.34, new_epsilon=4.4889, bad=True, gap_average=4.300335311889649
            reward_single_sum=23.387112970487316, 
            reward_single_sum=59.76117732033015, confidence_size=114.82840194414344, confidence_max=156.40254708955212, new_horizon=10
            reward_single_sum=21.951644966664002, confidence_size=36.12277657532443, confidence_max=71.15608832781824, new_horizon=10
        episode=2, horizon=32, effective_score=35.03, baseline_lowerbound=88.08 baseline_stdev=0.34, new_epsilon=2.9926, bad=True, gap_average=3.8355539723446497
            reward_single_sum=22.220967786374338, 
            reward_single_sum=22.506084423776954, confidence_size=0.9000778006478605, confidence_max=23.263603905723507, new_horizon=10
        episode=3, horizon=32, effective_score=22.36, baseline_lowerbound=88.08 baseline_stdev=0.34, new_epsilon=1.9951, bad=True, gap_average=4.1807826814197355
            reward_single_sum=101.43481455588092, 
            reward_single_sum=100.49635713989987, confidence_size=2.9625934658632573, confidence_max=103.92817931375365, new_horizon=8
        episode=4, horizon=10, effective_score=100.97, baseline_lowerbound=88.08 baseline_stdev=0.34, new_epsilon=2.9926, bad=False, gap_average=1.3203195681174595
            reward_single_sum=99.57370791261826, 
            reward_single_sum=94.92775326881595, confidence_size=14.666701585001647, confidence_max=111.91743217571873, new_horizon=8
            reward_single_sum=23.423514873392346, confidence_size=71.96477066734937, confidence_max=144.60642935229154, new_horizon=8
            reward_single_sum=22.900649358331766, confidence_size=50.382762058074746, confidence_max=110.58916841136431, new_horizon=8
            reward_single_sum=92.4626744552976, confidence_size=37.933787259341855, confidence_max=104.59144723303302, new_horizon=8
        episode=5, horizon=8, effective_score=66.66, baseline_lowerbound=88.08 baseline_stdev=0.34, new_epsilon=1.9951, bad=True, gap_average=1.2485014929433609
            reward_single_sum=106.47133725394201, 
            reward_single_sum=99.1606226750508, confidence_size=23.07901762337582, confidence_max=125.89499758787221, new_horizon=8
            reward_single_sum=103.67227809339292, confidence_size=6.218507309763247, confidence_max=109.31991998389182, new_horizon=8
        episode=6, horizon=8, effective_score=103.10, baseline_lowerbound=88.08 baseline_stdev=0.34, new_epsilon=2.9926, bad=False, gap_average=1.2326442250468999
            reward_single_sum=101.22397855158093, 
            reward_single_sum=97.28077482676596, confidence_size=12.448204245359584, confidence_max=111.70058093453302, new_horizon=8
            reward_single_sum=97.3042884606726, confidence_size=3.8266407573923686, confidence_max=102.4296547037322, new_horizon=8
        episode=7, horizon=10, effective_score=98.60, baseline_lowerbound=88.08 baseline_stdev=0.34, new_epsilon=4.4889, bad=False, gap_average=1.498276083804548
            reward_single_sum=22.0564679097337, 
            reward_single_sum=22.083318015269285, confidence_size=0.08476244724892723, confidence_max=22.15465540975042, new_horizon=10
        episode=8, horizon=8, effective_score=22.07, baseline_lowerbound=88.08 baseline_stdev=0.34, new_epsilon=2.9926, bad=True, gap_average=5.337179183959961
            reward_single_sum=21.328032386243155, 
            reward_single_sum=99.99447994801561, confidence_size=248.34020122857464, confidence_max=309.0014573957039, new_horizon=8
            reward_single_sum=22.04621100507774, confidence_size=76.22118970939616, confidence_max=124.01076415584164, new_horizon=8
            reward_single_sum=22.380912848481582, confidence_size=45.93826403937471, confidence_max=87.37567308632921, new_horizon=8
        episode=9, horizon=8, effective_score=41.44, baseline_lowerbound=88.08 baseline_stdev=0.34, new_epsilon=1.9951, bad=True, gap_average=2.133907455710036
            reward_single_sum=100.31373648044338, 
            reward_single_sum=99.15575315561502, confidence_size=3.6556094856246517, confidence_max=103.39035430365385, new_horizon=8
        episode=10, horizon=8, effective_score=99.73, baseline_lowerbound=88.08 baseline_stdev=0.34, new_epsilon=2.9926, bad=False, gap_average=1.250991846679093
            reward_single_sum=92.68787367166534, 
            reward_single_sum=23.42946873867352, confidence_size=218.64017952918687, confidence_max=276.6988507343562, new_horizon=8
            reward_single_sum=102.42880426467481, confidence_size=72.61744265097722, confidence_max=145.4661582093151, new_horizon=8
            reward_single_sum=54.833586170594515, confidence_size=42.71989100442346, confidence_max=111.0648242158255, new_horizon=8
            reward_single_sum=22.004426495475286, confidence_size=35.90183951139571, confidence_max=94.9786713796124, new_horizon=8
        episode=11, horizon=8, effective_score=59.08, baseline_lowerbound=88.08 baseline_stdev=0.34, new_epsilon=1.9951, bad=True, gap_average=1.4817452879874937
            reward_single_sum=22.315824458139165, 
            reward_single_sum=100.96194948788958, confidence_size=248.27604551990524, confidence_max=309.9149324929195, new_horizon=8
            reward_single_sum=105.64160613075265, confidence_size=78.92457509971007, confidence_max=155.23103512530383, new_horizon=8
            reward_single_sum=98.1051053150573, confidence_size=46.771277156947974, confidence_max=128.52739850490764, new_horizon=8
            reward_single_sum=89.48010861814561, confidence_size=32.98355686853702, confidence_max=116.28447567053387, new_horizon=8
            reward_single_sum=104.81670411777066, confidence_size=26.46120049857267, confidence_max=113.34808351986516, new_horizon=8
            reward_single_sum=105.46882774067474, confidence_size=22.174488888567645, confidence_max=111.71593544120046, new_horizon=8
            reward_single_sum=104.37165823552337, confidence_size=19.049914106543604, confidence_max=110.44513711953773, new_horizon=8
            reward_single_sum=98.72794359427888, confidence_size=16.559261110433127, confidence_max=108.76923085468113, new_horizon=8
        episode=12, horizon=8, effective_score=92.21, baseline_lowerbound=88.08 baseline_stdev=0.34, new_epsilon=2.9926, bad=False, gap_average=1.2712694675112381
            reward_single_sum=40.02995039771198, 
            reward_single_sum=23.239785910701787, confidence_size=53.004463231808735, confidence_max=84.63933138601558, new_horizon=8
        episode=13, horizon=8, effective_score=31.63, baseline_lowerbound=88.08 baseline_stdev=0.34, new_epsilon=1.9951, bad=True, gap_average=1.9474386917917352
            reward_single_sum=22.24045776908977, 
            reward_single_sum=102.9645944824542, confidence_size=254.83607022750127, confidence_max=317.4385963532731, new_horizon=8
            reward_single_sum=102.71392771263247, confidence_size=78.44939905189268, confidence_max=154.42239237328482, new_horizon=8
            reward_single_sum=100.55646409094821, confidence_size=46.98910035303371, confidence_max=129.10796136681486, new_horizon=8
            reward_single_sum=102.1293171270142, confidence_size=34.057559911599625, confidence_max=120.17851214802738, new_horizon=8
            reward_single_sum=94.19307943789102, confidence_size=26.42379219639151, confidence_max=113.89009896639648, new_horizon=8
            reward_single_sum=98.92377954674693, confidence_size=21.769286229592055, confidence_max=110.87237482484588, new_horizon=8
            reward_single_sum=101.40951584026664, confidence_size=18.61083622283722, confidence_max=109.25222822371765, new_horizon=8
        episode=14, horizon=8, effective_score=90.64, baseline_lowerbound=88.08 baseline_stdev=0.34, new_epsilon=2.9926, bad=False, gap_average=1.216019634427971
optimal_epsilon = 2.9925925925925925
optimal_horizon = 8
    scaled_epsilon: 2.9926, forecast_average: 4.6667, episode_reward:25.51, max_timestep_reward: 3.17, min_timestep_reward: -1.49
    scaled_epsilon: 2.9926, forecast_average: 3.3902, episode_reward:315.37, max_timestep_reward: 5.24, min_timestep_reward: -0.65
    scaled_epsilon: 2.9926, forecast_average: 3.4268, episode_reward:447.78, max_timestep_reward: 4.95, min_timestep_reward: -1.04
    scaled_epsilon: 2.9926, forecast_average: 3.5240, episode_reward:214.66, max_timestep_reward: 3.74, min_timestep_reward: -0.90
    scaled_epsilon: 2.9926, forecast_average: 3.3362, episode_reward:494.35, max_timestep_reward: 4.86, min_timestep_reward: 0.28
    scaled_epsilon: 2.9926, forecast_average: 3.3260, episode_reward:532.04, max_timestep_reward: 4.83, min_timestep_reward: -0.32
    scaled_epsilon: 2.9926, forecast_average: 3.4105, episode_reward:455.78, max_timestep_reward: 5.03, min_timestep_reward: -0.24
    scaled_epsilon: 2.9926, forecast_average: 3.4709, episode_reward:1063.42, max_timestep_reward: 5.07, min_timestep_reward: 0.38
    scaled_epsilon: 2.9926, forecast_average: 3.5011, episode_reward:505.16, max_timestep_reward: 4.14, min_timestep_reward: -0.08
    scaled_epsilon: 2.9926, forecast_average: 3.4963, episode_reward:725.99, max_timestep_reward: 4.95, min_timestep_reward: -0.60
self.recorder = {
    number_of_records: 0,
    records: [ ... ],
    local_data: {},
    parent_data:    {
    }
}
/home/jeffhykin/repos/AFRL/.venv/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(
argv[0]=
argv[0]=


-------------------------------------------------------

 Environment: HalfCheetahBulletEnv-v0

-------------------------------------------------------




-----------------------------------------------------------------------------------------------------
 Agent Model Exists, loading: subrepos/rl-trained-agents/sac/HalfCheetahBulletEnv-v0_1/HalfCheetahBulletEnv-v0.zip
-----------------------------------------------------------------------------------------------------


pybullet build time: Oct 11 2021 20:52:37
Wrapping the env with a `Monitor` wrapper
Wrapping the env in a DummyVecEnv.


-----------------------------------------------------------------------------------------------------
 Training Coach Model from scrach
-----------------------------------------------------------------------------------------------------


coach settings = {
    "force_retrain": False, 
    "learning_rate": 0.0001, 
    "hidden_sizes": [64, 64, 64, 64, ], 
    "loss_function": "state_prediction_loss", 
    "consistent_coach_loss": {"scale_future_state_loss": 1000000, }, 
    "delayed_coach_loss": {"epochs_of_delay": 50, }, 
    "value_plus_state_loss": {"value_proportion": 0.5, }, 
    "number_of_episodes": 100, 
    "number_of_epochs": 100, 
    "train_test_split": 0.7, 
    "batch_size": 128, 
    "with_card": True, 
}
Starting Experience Recording
    Episode: 0, Reward: 2783.058, Average Reward: 2783.058
    Episode: 1, Reward: 2791.685, Average Reward: 2787.371
    Episode: 2, Reward: 2786.662, Average Reward: 2787.135
    Episode: 3, Reward: 2800.886, Average Reward: 2790.573
    Episode: 4, Reward: 2757.158, Average Reward: 2783.890
    Episode: 5, Reward: 2785.329, Average Reward: 2784.130
    Episode: 6, Reward: 2771.409, Average Reward: 2782.312
    Episode: 7, Reward: 2785.194, Average Reward: 2782.673
    Episode: 8, Reward: 2787.717, Average Reward: 2783.233
    Episode: 9, Reward: 2790.350, Average Reward: 2783.945
    Episode: 10, Reward: 2800.332, Average Reward: 2785.434
    Episode: 11, Reward: 2800.860, Average Reward: 2786.720
    Episode: 12, Reward: 2802.839, Average Reward: 2787.960
    Episode: 13, Reward: 2787.621, Average Reward: 2787.936
    Episode: 14, Reward: 2798.170, Average Reward: 2788.618
    Episode: 15, Reward: 2795.556, Average Reward: 2789.052
    Episode: 16, Reward: 2795.914, Average Reward: 2789.455
    Episode: 17, Reward: 2778.422, Average Reward: 2788.842
    Episode: 18, Reward: 2789.868, Average Reward: 2788.896
    Episode: 19, Reward: 2767.104, Average Reward: 2787.807
    Episode: 20, Reward: 2797.443, Average Reward: 2788.266
    Episode: 21, Reward: 2784.972, Average Reward: 2788.116
    Episode: 22, Reward: 2796.249, Average Reward: 2788.469
    Episode: 23, Reward: 2814.546, Average Reward: 2789.556
    Episode: 24, Reward: 2788.638, Average Reward: 2789.519
    Episode: 25, Reward: 2812.456, Average Reward: 2790.401
    Episode: 26, Reward: 2782.859, Average Reward: 2790.122
    Episode: 27, Reward: 2799.619, Average Reward: 2790.461
    Episode: 28, Reward: 2788.767, Average Reward: 2790.403
    Episode: 29, Reward: 2780.308, Average Reward: 2790.066
    Episode: 30, Reward: 2788.272, Average Reward: 2790.008
    Episode: 31, Reward: 2792.689, Average Reward: 2790.092
    Episode: 32, Reward: 2806.531, Average Reward: 2790.590
    Episode: 33, Reward: 2805.867, Average Reward: 2791.040
    Episode: 34, Reward: 2771.843, Average Reward: 2790.491
    Episode: 35, Reward: 2804.814, Average Reward: 2790.889
    Episode: 36, Reward: 2790.488, Average Reward: 2790.878
    Episode: 37, Reward: 2790.400, Average Reward: 2790.866
    Episode: 38, Reward: 2782.505, Average Reward: 2790.651
    Episode: 39, Reward: 2783.419, Average Reward: 2790.470
    Episode: 40, Reward: 2785.159, Average Reward: 2790.341
    Episode: 41, Reward: 2798.982, Average Reward: 2790.547
    Episode: 42, Reward: 2779.985, Average Reward: 2790.301
    Episode: 43, Reward: 2783.755, Average Reward: 2790.152
    Episode: 44, Reward: 2776.223, Average Reward: 2789.843
    Episode: 45, Reward: 2800.544, Average Reward: 2790.075
    Episode: 46, Reward: 2780.143, Average Reward: 2789.864
    Episode: 47, Reward: 2771.673, Average Reward: 2789.485
    Episode: 48, Reward: 2805.290, Average Reward: 2789.808
    Episode: 49, Reward: 2781.505, Average Reward: 2789.642
    Episode: 50, Reward: 2796.257, Average Reward: 2789.771
    Episode: 51, Reward: 2791.248, Average Reward: 2789.800
    Episode: 52, Reward: 2791.057, Average Reward: 2789.823
    Episode: 53, Reward: 2784.021, Average Reward: 2789.716
    Episode: 54, Reward: 2802.449, Average Reward: 2789.947
    Episode: 55, Reward: 2809.065, Average Reward: 2790.289
    Episode: 56, Reward: 2794.125, Average Reward: 2790.356
    Episode: 57, Reward: 2787.545, Average Reward: 2790.308
    Episode: 58, Reward: 2764.435, Average Reward: 2789.869
    Episode: 59, Reward: 2797.810, Average Reward: 2790.001
    Episode: 60, Reward: 2797.182, Average Reward: 2790.119
    Episode: 61, Reward: 2806.198, Average Reward: 2790.379
    Episode: 62, Reward: 2803.731, Average Reward: 2790.590
    Episode: 63, Reward: 2789.752, Average Reward: 2790.577
    Episode: 64, Reward: 2776.956, Average Reward: 2790.368
    Episode: 65, Reward: 2808.076, Average Reward: 2790.636
    Episode: 66, Reward: 2796.171, Average Reward: 2790.719
    Episode: 67, Reward: 2807.787, Average Reward: 2790.970
    Episode: 68, Reward: 2814.941, Average Reward: 2791.317
    Episode: 69, Reward: 2771.745, Average Reward: 2791.038
    Episode: 70, Reward: 2773.035, Average Reward: 2790.784
    Episode: 71, Reward: 2807.201, Average Reward: 2791.012
    Episode: 72, Reward: 2787.436, Average Reward: 2790.963
    Episode: 73, Reward: 2792.744, Average Reward: 2790.987
    Episode: 74, Reward: 2781.737, Average Reward: 2790.864
    Episode: 75, Reward: 2784.387, Average Reward: 2790.779
    Episode: 76, Reward: 2814.088, Average Reward: 2791.081
    Episode: 77, Reward: 2783.796, Average Reward: 2790.988
    Episode: 78, Reward: 2795.998, Average Reward: 2791.051
    Episode: 79, Reward: 2793.029, Average Reward: 2791.076
    Episode: 80, Reward: 2786.197, Average Reward: 2791.016
    Episode: 81, Reward: 2787.234, Average Reward: 2790.970
    Episode: 82, Reward: 2802.688, Average Reward: 2791.111
    Episode: 83, Reward: 2744.016, Average Reward: 2790.550
    Episode: 84, Reward: 2821.697, Average Reward: 2790.917
    Episode: 85, Reward: 2783.682, Average Reward: 2790.832
    Episode: 86, Reward: 2789.337, Average Reward: 2790.815
    Episode: 87, Reward: 2760.687, Average Reward: 2790.473
    Episode: 88, Reward: 2774.511, Average Reward: 2790.294
    Episode: 89, Reward: 2807.068, Average Reward: 2790.480
    Episode: 90, Reward: 2791.139, Average Reward: 2790.487
    Episode: 91, Reward: 2784.253, Average Reward: 2790.419
    Episode: 92, Reward: 2812.626, Average Reward: 2790.658
    Episode: 93, Reward: 2801.304, Average Reward: 2790.771
    Episode: 94, Reward: 2789.978, Average Reward: 2790.763
    Episode: 95, Reward: 2796.707, Average Reward: 2790.825
    Episode: 96, Reward: 2806.214, Average Reward: 2790.984
    Episode: 97, Reward: 2801.761, Average Reward: 2791.094
    Episode: 98, Reward: 2782.391, Average Reward: 2791.006
    Episode: 99, Reward: 2798.487, Average Reward: 2791.081

    Max Episode Reward: 2821.6968304376815
    Min Episode Reward: 2744.0160948872576
    Max Timestep Reward: 3.3539539716367774
    Min Timestep Reward: 0.37548473789314163
Starting Train/Test
    Epoch: 0
    Epoch: 1
    Epoch: 2
    Epoch: 3
    Epoch: 4
    Epoch: 5
    Epoch: 6
    Epoch: 7
    Epoch: 8
    Epoch: 9
    Epoch: 10
    Epoch: 11
    Epoch: 12
    Epoch: 13
    Epoch: 14
    Epoch: 15
    Epoch: 16
    Epoch: 17
    Epoch: 18
    Epoch: 19
    Epoch: 20
    Epoch: 21
    Epoch: 22
    Epoch: 23
    Epoch: 24
    Epoch: 25
    Epoch: 26
    Epoch: 27
    Epoch: 28
    Epoch: 29
    Epoch: 30
    Epoch: 31
    Epoch: 32
    Epoch: 33
    Epoch: 34
    Epoch: 35
    Epoch: 36
    Epoch: 37
    Epoch: 38
    Epoch: 39
    Epoch: 40
    Epoch: 41
    Epoch: 42
    Epoch: 43
    Epoch: 44
    Epoch: 45
    Epoch: 46
    Epoch: 47
    Epoch: 48
    Epoch: 49
    Epoch: 50
    Epoch: 51
    Epoch: 52
    Epoch: 53
    Epoch: 54
    Epoch: 55
    Epoch: 56
    Epoch: 57
    Epoch: 58
    Epoch: 59
    Epoch: 60
    Epoch: 61
    Epoch: 62
    Epoch: 63
    Epoch: 64
    Epoch: 65
    Epoch: 66
    Epoch: 67
    Epoch: 68
    Epoch: 69
    Epoch: 70
    Epoch: 71
    Epoch: 72
    Epoch: 73
    Epoch: 74
    Epoch: 75
    Epoch: 76
    Epoch: 77
    Epoch: 78
    Epoch: 79
    Epoch: 80
    Epoch: 81
    Epoch: 82
    Epoch: 83
    Epoch: 84
    Epoch: 85
    Epoch: 86
    Epoch: 87
    Epoch: 88
    Epoch: 89
    Epoch: 90
    Epoch: 91
    Epoch: 92
    Epoch: 93
    Epoch: 94
    Epoch: 95
    Epoch: 96
    Epoch: 97
    Epoch: 98
    Epoch: 99
Saving Coach to: models.ignore/coach/HalfCheetahBulletEnv-v0/final_1_95%/


-----------------------------------------------------------------------------------------------------
 Testing Agent+Coach
-----------------------------------------------------------------------------------------------------


test settings = {
    "force_recompute": True, 
    "graph_smoothing": 0, 
    "number_of_episodes_for_baseline": 30, 
    "number_of_epochs_for_optimal_parameters": 15, 
    "number_of_episodes_for_testing": 10, 
    "initial_epsilon": 3.5, 
    "initial_horizon": 10, 
    "reward_discount": 0.98, 
    "acceptable_performance_loss": 1, 
    "acceptable_performance_level": 0.8, 
    "confidence_interval_for_convergence": 90, 
    "min_reward_single_timestep": -60, 
    "max_reward_single_timestep": 100, 
    "horizons": {0: 1, 0.001: 11, 0.01: 13, 0.1: 26, }, 
}
----- getting a reward baseline -----------------------------------------------------------------------------------------------------------------------------------
  episode_index=0, episode_discounted_reward_sum=104.68788092552178
  episode_index=1, episode_discounted_reward_sum=101.80949042261297
  episode_index=2, episode_discounted_reward_sum=105.89240262854601
  episode_index=3, episode_discounted_reward_sum=103.24751409582207
  episode_index=4, episode_discounted_reward_sum=101.5097088026517
  episode_index=5, episode_discounted_reward_sum=104.36904161565846
  episode_index=6, episode_discounted_reward_sum=102.85470683737603
  episode_index=7, episode_discounted_reward_sum=103.01997325051691
  episode_index=8, episode_discounted_reward_sum=104.59420178918278
  episode_index=9, episode_discounted_reward_sum=105.31903929672964
  episode_index=10, episode_discounted_reward_sum=105.12493242427608
  episode_index=11, episode_discounted_reward_sum=104.48827537883167
  episode_index=12, episode_discounted_reward_sum=101.51755494032399
  episode_index=13, episode_discounted_reward_sum=104.79627480585636
  episode_index=14, episode_discounted_reward_sum=105.51005767247408
  episode_index=15, episode_discounted_reward_sum=106.6472799657738
  episode_index=16, episode_discounted_reward_sum=105.02326982785569
  episode_index=17, episode_discounted_reward_sum=104.48956970389472
  episode_index=18, episode_discounted_reward_sum=100.68180807608178
  episode_index=19, episode_discounted_reward_sum=104.7280555837557
  episode_index=20, episode_discounted_reward_sum=101.7280259733553
  episode_index=21, episode_discounted_reward_sum=104.26251126973276
  episode_index=22, episode_discounted_reward_sum=104.25445348924377
  episode_index=23, episode_discounted_reward_sum=106.06257525263514
  episode_index=24, episode_discounted_reward_sum=105.10364020012709
  episode_index=25, episode_discounted_reward_sum=102.82664964406999
  episode_index=26, episode_discounted_reward_sum=103.00424956599596
  episode_index=27, episode_discounted_reward_sum=104.26584841607634
  episode_index=28, episode_discounted_reward_sum=104.78617751391494
  episode_index=29, episode_discounted_reward_sum=106.11264420732685
baseline = {
    "max": 106.6472799657738, 
    "min": 100.68180807608178, 
    "range": 5.965471889692026, 
    "count": 30, 
    "sum": 3122.717813576221, 
    "average": 104.09059378587403, 
    "stdev": 1.5377957678318828, 
    "median": 104.4889225413632, 
}
baseline_min = 103.61354380598755, baseline_max = 104.56764376576047,
baseline_confidence_size = 0.47704997988645914
----- finding optimal epsilon -------------------------------------------------------------------------------------------------------------------------------------
            reward_single_sum=34.59797933216235, 
            reward_single_sum=45.515133657507235, confidence_size=34.464099829480915, confidence_max=74.5206563243157, new_horizon=4
        episode=0, horizon=20, effective_score=40.06, baseline_lowerbound=83.27 baseline_stdev=0.28, new_epsilon=2.3333, bad=True, gap_average=1.129545380592346
            reward_single_sum=35.66134335550418, 
            reward_single_sum=52.251093472317535, confidence_size=52.37177996509966, confidence_max=96.3279983790105, new_horizon=2
        episode=1, horizon=20, effective_score=43.96, baseline_lowerbound=83.27 baseline_stdev=0.28, new_epsilon=1.5556, bad=True, gap_average=1.9744876137971878
            reward_single_sum=70.51028123050678, 
            reward_single_sum=79.32091940676905, confidence_size=27.814090065869436, confidence_max=102.72969038450734, new_horizon=6
        episode=2, horizon=20, effective_score=74.92, baseline_lowerbound=83.27 baseline_stdev=0.28, new_epsilon=1.0370, bad=True, gap_average=0.9127364234924317
            reward_single_sum=74.49113570785697, 
            reward_single_sum=57.45271425735154, confidence_size=53.78817962147271, confidence_max=119.76010460407693, new_horizon=6
            reward_single_sum=91.36227463308207, confidence_size=28.583407748046376, confidence_max=103.01878261414322, new_horizon=4
        episode=3, horizon=20, effective_score=74.44, baseline_lowerbound=83.27 baseline_stdev=0.28, new_epsilon=0.6914, bad=True, gap_average=0.8051872262954712
            reward_single_sum=97.39747606773449, 
            reward_single_sum=92.16063897282095, confidence_size=16.532044070388046, confidence_max=111.31110159066576, new_horizon=6
            reward_single_sum=86.97594379965356, confidence_size=8.784629982051534, confidence_max=100.96264959545452, new_horizon=6
        episode=4, horizon=4, effective_score=92.18, baseline_lowerbound=83.27 baseline_stdev=0.28, new_epsilon=1.0370, bad=False, gap_average=0.6264708611170451
            reward_single_sum=93.09550244377273, 
            reward_single_sum=33.15509320085686, confidence_size=189.22442482762403, confidence_max=252.3497226499387, new_horizon=6
            reward_single_sum=89.30498407137542, confidence_size=56.58727698300369, confidence_max=128.43913688833868, new_horizon=6
            reward_single_sum=70.09127486005163, confidence_size=32.26532210835469, confidence_max=103.67703575236884, new_horizon=6
            reward_single_sum=70.59197292023055, confidence_size=22.64281208384778, confidence_max=93.89057758310521, new_horizon=6
        episode=5, horizon=6, effective_score=71.25, baseline_lowerbound=83.27 baseline_stdev=0.28, new_epsilon=0.6914, bad=True, gap_average=0.7725016396045685
            reward_single_sum=83.75064361782653, 
            reward_single_sum=96.95679636296583, confidence_size=41.690183449657916, confidence_max=132.04390344005407, new_horizon=6
            reward_single_sum=95.29160847589753, confidence_size=12.125056088736685, confidence_max=104.12473890763331, new_horizon=6
            reward_single_sum=84.62083397896517, confidence_size=8.160550122457863, confidence_max=98.31552073137163, new_horizon=6
        episode=6, horizon=6, effective_score=90.15, baseline_lowerbound=83.27 baseline_stdev=0.28, new_epsilon=1.0370, bad=False, gap_average=0.6160890176594257
            reward_single_sum=78.14864406532284, 
            reward_single_sum=82.26886742980285, confidence_size=13.00703325440194, confidence_max=93.21578900196478, new_horizon=4
        episode=7, horizon=6, effective_score=80.21, baseline_lowerbound=83.27 baseline_stdev=0.28, new_epsilon=0.6914, bad=True, gap_average=0.778800931930542
            reward_single_sum=97.45323397365328, 
            reward_single_sum=91.55064371293557, confidence_size=18.63374409992786, confidence_max=113.13568294322228, new_horizon=6
            reward_single_sum=94.73274840545776, confidence_size=4.980523297819019, confidence_max=99.55939866183455, new_horizon=6
        episode=8, horizon=4, effective_score=94.58, baseline_lowerbound=83.27 baseline_stdev=0.28, new_epsilon=1.0370, bad=False, gap_average=0.5762998873392741
            reward_single_sum=83.32789401090146, 
            reward_single_sum=88.33517530743298, confidence_size=15.807364935505127, confidence_max=101.63889959467234, new_horizon=4
        episode=9, horizon=20, effective_score=85.83, baseline_lowerbound=83.27 baseline_stdev=0.28, new_epsilon=1.5556, bad=False, gap_average=0.7140443959832191
            reward_single_sum=83.26144308865132, 
            reward_single_sum=66.84872806800236, confidence_size=51.812902161809234, confidence_max=126.86798774013604, new_horizon=6
            reward_single_sum=70.69211210477106, confidence_size=14.471809382516561, confidence_max=88.07257046965813, new_horizon=6
        episode=10, horizon=4, effective_score=73.60, baseline_lowerbound=83.27 baseline_stdev=0.28, new_epsilon=1.0370, bad=True, gap_average=0.8289715461730957
            reward_single_sum=79.71320026005908, 
            reward_single_sum=85.47589563725364, confidence_size=18.19211333354925, confidence_max=100.7866612822056, new_horizon=6
        episode=11, horizon=6, effective_score=82.59, baseline_lowerbound=83.27 baseline_stdev=0.28, new_epsilon=0.6914, bad=True, gap_average=0.7632601909637451
            reward_single_sum=41.43958124045453, 
            reward_single_sum=88.60271752717249, confidence_size=148.88816158651426, confidence_max=213.90931097032768, new_horizon=6
            reward_single_sum=82.67206405038375, confidence_size=43.30848064733743, confidence_max=114.21326825334101, new_horizon=6
            reward_single_sum=89.81702722241994, confidence_size=27.073385469711816, confidence_max=102.70623297981949, new_horizon=6
        episode=12, horizon=20, effective_score=75.63, baseline_lowerbound=83.27 baseline_stdev=0.28, new_epsilon=0.4609, bad=True, gap_average=0.5997833316326141
            reward_single_sum=71.42799296803545, 
            reward_single_sum=96.42668352242858, confidence_size=78.91776017791972, confidence_max=162.84509842315168, new_horizon=4
            reward_single_sum=95.10048969864772, confidence_size=23.71289210600937, confidence_max=111.36461416904662, new_horizon=4
            reward_single_sum=100.68237012598821, confidence_size=15.53697917450387, confidence_max=106.44636325327886, new_horizon=6
            reward_single_sum=93.66310969461435, confidence_size=10.965116349440166, confidence_max=102.42524555138303, new_horizon=4
        episode=13, horizon=6, effective_score=91.46, baseline_lowerbound=83.27 baseline_stdev=0.28, new_epsilon=0.6914, bad=False, gap_average=0.45826357707977294
            reward_single_sum=85.6967531897766, 
            reward_single_sum=94.65349638304953, confidence_size=28.275325452104966, confidence_max=118.45045023851802, new_horizon=6
            reward_single_sum=96.66166456351553, confidence_size=9.841821144989524, confidence_max=102.17912585710341, new_horizon=6
        episode=14, horizon=6, effective_score=92.34, baseline_lowerbound=83.27 baseline_stdev=0.28, new_epsilon=1.0370, bad=False, gap_average=0.5918309256235759
optimal_epsilon = 1.037037037037037
optimal_horizon = 6
    scaled_epsilon: 1.0370, forecast_average: 2.0322, episode_reward:2145.90, max_timestep_reward: 2.98, min_timestep_reward: -0.79
    scaled_epsilon: 1.0370, forecast_average: 2.0659, episode_reward:2154.25, max_timestep_reward: 3.00, min_timestep_reward: -0.31
    scaled_epsilon: 1.0370, forecast_average: 2.0973, episode_reward:2190.00, max_timestep_reward: 2.96, min_timestep_reward: -0.25
    scaled_epsilon: 1.0370, forecast_average: 2.0928, episode_reward:2069.12, max_timestep_reward: 2.97, min_timestep_reward: -1.01
    scaled_epsilon: 1.0370, forecast_average: 2.0817, episode_reward:2196.10, max_timestep_reward: 2.97, min_timestep_reward: -0.37
    scaled_epsilon: 1.0370, forecast_average: 2.0967, episode_reward:2227.15, max_timestep_reward: 3.08, min_timestep_reward: -0.29
    scaled_epsilon: 1.0370, forecast_average: 2.0914, episode_reward:2082.28, max_timestep_reward: 2.92, min_timestep_reward: -0.78
    scaled_epsilon: 1.0370, forecast_average: 2.0868, episode_reward:2165.29, max_timestep_reward: 3.03, min_timestep_reward: -0.48
    scaled_epsilon: 1.0370, forecast_average: 2.0668, episode_reward:1823.19, max_timestep_reward: 2.99, min_timestep_reward: -2.23
    scaled_epsilon: 1.0370, forecast_average: 2.0756, episode_reward:2187.31, max_timestep_reward: 3.03, min_timestep_reward: -0.43
self.recorder = {
    number_of_records: 0,
    records: [ ... ],
    local_data: {},
    parent_data:    {
    }
}
/home/jeffhykin/repos/AFRL/.venv/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(
argv[0]=
argv[0]=
argv[0]=
argv[0]=


-------------------------------------------------------

 Environment: BipedalWalker-v3

-------------------------------------------------------




-----------------------------------------------------------------------------------------------------
 Agent Model Exists, loading: ./subrepos/rl-trained-agents/sac/BipedalWalker-v3_1/BipedalWalker-v3.zip
-----------------------------------------------------------------------------------------------------


Wrapping the env with a `Monitor` wrapper
Wrapping the env in a DummyVecEnv.


-----------------------------------------------------------------------------------------------------
 Training Coach Model from scrach
-----------------------------------------------------------------------------------------------------


coach settings = {
    "force_retrain": False, 
    "learning_rate": 0.0001, 
    "hidden_sizes": [64, 64, 64, 64, ], 
    "loss_function": "state_prediction_loss", 
    "consistent_coach_loss": {"scale_future_state_loss": 1000000, }, 
    "delayed_coach_loss": {"epochs_of_delay": 50, }, 
    "value_plus_state_loss": {"value_proportion": 0.5, }, 
    "number_of_episodes": 100, 
    "number_of_epochs": 100, 
    "train_test_split": 0.7, 
    "batch_size": 128, 
    "with_card": True, 
}
Starting Experience Recording
    Episode: 0, Reward: 300.612, Average Reward: 300.612
    Episode: 1, Reward: 302.322, Average Reward: 301.467
    Episode: 2, Reward: 299.881, Average Reward: 300.938
    Episode: 3, Reward: 298.631, Average Reward: 300.361
    Episode: 4, Reward: 300.879, Average Reward: 300.465
    Episode: 5, Reward: 301.314, Average Reward: 300.606
    Episode: 6, Reward: -88.407, Average Reward: 245.033
    Episode: 7, Reward: 300.225, Average Reward: 251.932
    Episode: 8, Reward: 302.239, Average Reward: 257.522
    Episode: 9, Reward: 301.204, Average Reward: 261.890
    Episode: 10, Reward: 301.527, Average Reward: 265.493
    Episode: 11, Reward: 300.530, Average Reward: 268.413
    Episode: 12, Reward: 299.449, Average Reward: 270.800
    Episode: 13, Reward: 300.945, Average Reward: 272.953
    Episode: 14, Reward: 298.891, Average Reward: 274.683
    Episode: 15, Reward: -83.876, Average Reward: 252.273
    Episode: 16, Reward: 299.988, Average Reward: 255.080
    Episode: 17, Reward: 301.896, Average Reward: 257.680
    Episode: 18, Reward: -87.756, Average Reward: 239.500
    Episode: 19, Reward: 299.720, Average Reward: 242.511
    Episode: 20, Reward: 300.070, Average Reward: 245.252
    Episode: 21, Reward: 300.344, Average Reward: 247.756
    Episode: 22, Reward: 300.413, Average Reward: 250.045
    Episode: 23, Reward: 301.611, Average Reward: 252.194
    Episode: 24, Reward: 299.644, Average Reward: 254.092
    Episode: 25, Reward: 300.717, Average Reward: 255.885
    Episode: 26, Reward: 300.033, Average Reward: 257.520
    Episode: 27, Reward: 301.796, Average Reward: 259.101
    Episode: 28, Reward: 300.954, Average Reward: 260.545
    Episode: 29, Reward: 300.504, Average Reward: 261.877
    Episode: 30, Reward: 300.356, Average Reward: 263.118
    Episode: 31, Reward: 300.836, Average Reward: 264.297
    Episode: 32, Reward: 301.969, Average Reward: 265.438
    Episode: 33, Reward: 300.800, Average Reward: 266.478
    Episode: 34, Reward: 297.963, Average Reward: 267.378
    Episode: 35, Reward: 299.273, Average Reward: 268.264
    Episode: 36, Reward: 300.929, Average Reward: 269.147
    Episode: 37, Reward: 299.313, Average Reward: 269.940
    Episode: 38, Reward: 301.195, Average Reward: 270.742
    Episode: 39, Reward: 300.161, Average Reward: 271.477
    Episode: 40, Reward: 301.135, Average Reward: 272.201
    Episode: 41, Reward: 303.228, Average Reward: 272.939
    Episode: 42, Reward: 301.150, Average Reward: 273.596
    Episode: 43, Reward: 300.775, Average Reward: 274.213
    Episode: 44, Reward: 300.847, Average Reward: 274.805
    Episode: 45, Reward: 300.524, Average Reward: 275.364
    Episode: 46, Reward: 300.783, Average Reward: 275.905
    Episode: 47, Reward: 300.143, Average Reward: 276.410
    Episode: 48, Reward: 21.016, Average Reward: 271.198
    Episode: 49, Reward: 296.170, Average Reward: 271.697
    Episode: 50, Reward: 301.528, Average Reward: 272.282
    Episode: 51, Reward: 301.270, Average Reward: 272.840
    Episode: 52, Reward: 301.536, Average Reward: 273.381
    Episode: 53, Reward: 302.160, Average Reward: 273.914
    Episode: 54, Reward: 301.309, Average Reward: 274.412
    Episode: 55, Reward: 299.275, Average Reward: 274.856
    Episode: 56, Reward: 296.231, Average Reward: 275.231
    Episode: 57, Reward: 297.375, Average Reward: 275.613
    Episode: 58, Reward: 297.375, Average Reward: 275.982
    Episode: 59, Reward: 300.830, Average Reward: 276.396
    Episode: 60, Reward: 298.298, Average Reward: 276.755
    Episode: 61, Reward: 300.250, Average Reward: 277.134
    Episode: 62, Reward: 299.048, Average Reward: 277.482
    Episode: 63, Reward: 299.174, Average Reward: 277.821
    Episode: 64, Reward: 302.484, Average Reward: 278.200
    Episode: 65, Reward: 301.447, Average Reward: 278.552
    Episode: 66, Reward: 299.619, Average Reward: 278.867
    Episode: 67, Reward: 301.091, Average Reward: 279.194
    Episode: 68, Reward: 188.703, Average Reward: 277.882
    Episode: 69, Reward: 298.788, Average Reward: 278.181
    Episode: 70, Reward: 301.207, Average Reward: 278.505
    Episode: 71, Reward: 299.639, Average Reward: 278.799
    Episode: 72, Reward: 300.039, Average Reward: 279.090
    Episode: 73, Reward: 300.436, Average Reward: 279.378
    Episode: 74, Reward: 298.306, Average Reward: 279.630
    Episode: 75, Reward: 302.529, Average Reward: 279.932
    Episode: 76, Reward: 302.855, Average Reward: 280.229
    Episode: 77, Reward: 300.729, Average Reward: 280.492
    Episode: 78, Reward: 301.041, Average Reward: 280.752
    Episode: 79, Reward: 298.661, Average Reward: 280.976
    Episode: 80, Reward: 300.247, Average Reward: 281.214
    Episode: 81, Reward: 299.074, Average Reward: 281.432
    Episode: 82, Reward: 302.928, Average Reward: 281.691
    Episode: 83, Reward: 299.834, Average Reward: 281.907
    Episode: 84, Reward: 301.119, Average Reward: 282.133
    Episode: 85, Reward: 301.296, Average Reward: 282.356
    Episode: 86, Reward: 303.037, Average Reward: 282.593
    Episode: 87, Reward: 299.550, Average Reward: 282.786
    Episode: 88, Reward: 297.850, Average Reward: 282.955
    Episode: 89, Reward: 298.973, Average Reward: 283.133
    Episode: 90, Reward: 300.757, Average Reward: 283.327
    Episode: 91, Reward: 301.192, Average Reward: 283.521
    Episode: 92, Reward: 302.968, Average Reward: 283.730
    Episode: 93, Reward: 298.803, Average Reward: 283.891
    Episode: 94, Reward: 302.797, Average Reward: 284.090
    Episode: 95, Reward: 300.585, Average Reward: 284.262
    Episode: 96, Reward: 301.096, Average Reward: 284.435
    Episode: 97, Reward: 301.996, Average Reward: 284.614
    Episode: 98, Reward: 299.916, Average Reward: 284.769
    Episode: 99, Reward: 299.040, Average Reward: 284.912

    Max Episode Reward: 303.22780887138885
    Min Episode Reward: -88.40722809476156
    Max Timestep Reward: 0.528635137071193
    Min Timestep Reward: -100
Starting Train/Test
    Epoch: 0
    Epoch: 1
    Epoch: 2
    Epoch: 3
    Epoch: 4
    Epoch: 5
    Epoch: 6
    Epoch: 7
    Epoch: 8
    Epoch: 9
    Epoch: 10
    Epoch: 11
    Epoch: 12
    Epoch: 13
    Epoch: 14
    Epoch: 15
    Epoch: 16
    Epoch: 17
    Epoch: 18
    Epoch: 19
    Epoch: 20
    Epoch: 21
    Epoch: 22
    Epoch: 23
    Epoch: 24
    Epoch: 25
    Epoch: 26
    Epoch: 27
    Epoch: 28
    Epoch: 29
    Epoch: 30
    Epoch: 31
    Epoch: 32
    Epoch: 33
    Epoch: 34
    Epoch: 35
    Epoch: 36
    Epoch: 37
    Epoch: 38
    Epoch: 39
    Epoch: 40
    Epoch: 41
    Epoch: 42
    Epoch: 43
    Epoch: 44
    Epoch: 45
    Epoch: 46
    Epoch: 47
    Epoch: 48
    Epoch: 49
    Epoch: 50
    Epoch: 51
    Epoch: 52
    Epoch: 53
    Epoch: 54
    Epoch: 55
    Epoch: 56
    Epoch: 57
    Epoch: 58
    Epoch: 59
    Epoch: 60
    Epoch: 61
    Epoch: 62
    Epoch: 63
    Epoch: 64
    Epoch: 65
    Epoch: 66
    Epoch: 67
    Epoch: 68
    Epoch: 69
    Epoch: 70
    Epoch: 71
    Epoch: 72
    Epoch: 73
    Epoch: 74
    Epoch: 75
    Epoch: 76
    Epoch: 77
    Epoch: 78
    Epoch: 79
    Epoch: 80
    Epoch: 81
    Epoch: 82
    Epoch: 83
    Epoch: 84
    Epoch: 85
    Epoch: 86
    Epoch: 87
    Epoch: 88
    Epoch: 89
    Epoch: 90
    Epoch: 91
    Epoch: 92
    Epoch: 93
    Epoch: 94
    Epoch: 95
    Epoch: 96
    Epoch: 97
    Epoch: 98
    Epoch: 99
Saving Coach to: models.ignore/coach/BipedalWalker-v3/final_1_95%/


-----------------------------------------------------------------------------------------------------
 Testing Agent+Coach
-----------------------------------------------------------------------------------------------------


test settings = {
    "override_save_path": False, 
    "force_recompute": True, 
    "graph_smoothing": 0, 
    "number_of_episodes_for_baseline": 30, 
    "number_of_epochs_for_optimal_parameters": 15, 
    "number_of_episodes_for_testing": 10, 
    "initial_epsilon": 3.5, 
    "initial_horizon": 10, 
    "reward_discount": 0.98, 
    "acceptable_performance_levels": [0.8, 0.9, 0.95, 0.98, 0.99, ], 
    "acceptable_performance_loss": 1, 
    "acceptable_performance_level": 0.8, 
    "confidence_interval_for_convergence": 90, 
    "min_reward_single_timestep": -23, 
    "max_reward_single_timestep": 5, 
    "horizons": {0.1: 26, 0.01: 16, 0.001: 11, }, 
}
Traceback (most recent call last):
  File "./main/run/full.py", line 50, in <module>
    full_run(
  File "./main/run/full.py", line 26, in full_run
    results = Tester.smart_load(
TypeError: run_epoch() missing 1 required positional argument: 'method'

[silver_spectacle] There were unviewed results
[silver_spectacle] So the display server is still running at: http://localhost:9900
[silver_spectacle] (use the stop server button to kill it now)



-------------------------------------------------------

 Environment: AntBulletEnv-v0

-------------------------------------------------------




-----------------------------------------------------------------------------------------------------
 Agent Model Exists, loading: subrepos/rl-trained-agents/sac/AntBulletEnv-v0_1/AntBulletEnv-v0.zip
-----------------------------------------------------------------------------------------------------


pybullet build time: Oct 11 2021 20:52:37
Wrapping the env with a `Monitor` wrapper
Wrapping the env in a DummyVecEnv.


-----------------------------------------------------------------------------------------------------
 Training Coach Model from scrach
-----------------------------------------------------------------------------------------------------


coach settings = {
    "force_retrain": False, 
    "learning_rate": 0.0001, 
    "hidden_sizes": [64, 64, 64, 64, ], 
    "loss_function": "state_prediction_loss", 
    "consistent_coach_loss": {"scale_future_state_loss": 1000000, }, 
    "delayed_coach_loss": {"epochs_of_delay": 50, }, 
    "value_plus_state_loss": {"value_proportion": 0.5, }, 
    "number_of_episodes": 100, 
    "number_of_epochs": 100, 
    "train_test_split": 0.7, 
    "batch_size": 128, 
    "with_card": True, 
}
Starting Experience Recording
    Episode: 0, Reward: 3079.037, Average Reward: 3079.037
    Episode: 1, Reward: 3087.831, Average Reward: 3083.434
    Episode: 2, Reward: 3100.762, Average Reward: 3089.210
    Episode: 3, Reward: 3063.714, Average Reward: 3082.836
    Episode: 4, Reward: 3102.805, Average Reward: 3086.830
    Episode: 5, Reward: 3078.667, Average Reward: 3085.469
    Episode: 6, Reward: 3055.053, Average Reward: 3081.124
    Episode: 7, Reward: 3126.057, Average Reward: 3086.741
    Episode: 8, Reward: 3092.430, Average Reward: 3087.373
    Episode: 9, Reward: 3107.236, Average Reward: 3089.359
    Episode: 10, Reward: 3083.358, Average Reward: 3088.814
    Episode: 11, Reward: 3102.738, Average Reward: 3089.974
    Episode: 12, Reward: 3088.224, Average Reward: 3089.839
    Episode: 13, Reward: 3094.084, Average Reward: 3090.142
    Episode: 14, Reward: 3103.765, Average Reward: 3091.051
    Episode: 15, Reward: 3085.183, Average Reward: 3090.684
    Episode: 16, Reward: 3101.387, Average Reward: 3091.314
    Episode: 17, Reward: 3104.180, Average Reward: 3092.028
    Episode: 18, Reward: 3081.148, Average Reward: 3091.456
    Episode: 19, Reward: 3054.245, Average Reward: 3089.595
    Episode: 20, Reward: 3114.857, Average Reward: 3090.798
    Episode: 21, Reward: 3120.662, Average Reward: 3092.156
    Episode: 22, Reward: 3056.011, Average Reward: 3090.584
    Episode: 23, Reward: 3088.579, Average Reward: 3090.501
    Episode: 24, Reward: 3104.704, Average Reward: 3091.069
    Episode: 25, Reward: 3068.323, Average Reward: 3090.194
    Episode: 26, Reward: 3132.124, Average Reward: 3091.747
    Episode: 27, Reward: 3150.368, Average Reward: 3093.840
    Episode: 28, Reward: 2967.010, Average Reward: 3089.467
    Episode: 29, Reward: 3039.809, Average Reward: 3087.812
    Episode: 30, Reward: 3059.204, Average Reward: 3086.889
    Episode: 31, Reward: 3136.502, Average Reward: 3088.439
    Episode: 32, Reward: 3073.862, Average Reward: 3087.997
    Episode: 33, Reward: 3059.493, Average Reward: 3087.159
    Episode: 34, Reward: 3028.324, Average Reward: 3085.478
    Episode: 35, Reward: 3117.456, Average Reward: 3086.366
    Episode: 36, Reward: 3123.154, Average Reward: 3087.361
    Episode: 37, Reward: 3075.808, Average Reward: 3087.057
    Episode: 38, Reward: 3119.370, Average Reward: 3087.885
    Episode: 39, Reward: 3097.663, Average Reward: 3088.130
    Episode: 40, Reward: 3136.824, Average Reward: 3089.317
    Episode: 41, Reward: 3132.882, Average Reward: 3090.355
    Episode: 42, Reward: 3076.547, Average Reward: 3090.033
    Episode: 43, Reward: 3096.995, Average Reward: 3090.192
    Episode: 44, Reward: 3137.338, Average Reward: 3091.239
    Episode: 45, Reward: 3094.072, Average Reward: 3091.301
    Episode: 46, Reward: 3132.696, Average Reward: 3092.182
    Episode: 47, Reward: 3121.190, Average Reward: 3092.786
    Episode: 48, Reward: 3111.280, Average Reward: 3093.163
    Episode: 49, Reward: 3104.544, Average Reward: 3093.391
    Episode: 50, Reward: 3004.039, Average Reward: 3091.639
    Episode: 51, Reward: 3109.236, Average Reward: 3091.977
    Episode: 52, Reward: 3070.012, Average Reward: 3091.563
    Episode: 53, Reward: 3036.041, Average Reward: 3090.535
    Episode: 54, Reward: 3128.148, Average Reward: 3091.219
    Episode: 55, Reward: 3063.832, Average Reward: 3090.730
    Episode: 56, Reward: 3120.505, Average Reward: 3091.252
    Episode: 57, Reward: 3097.061, Average Reward: 3091.352
    Episode: 58, Reward: 2616.729, Average Reward: 3083.308
    Episode: 59, Reward: 3107.901, Average Reward: 3083.718
    Episode: 60, Reward: 3055.931, Average Reward: 3083.262
    Episode: 61, Reward: 3129.824, Average Reward: 3084.013
    Episode: 62, Reward: 3103.418, Average Reward: 3084.321
    Episode: 63, Reward: 3106.233, Average Reward: 3084.663
    Episode: 64, Reward: 3095.458, Average Reward: 3084.830
    Episode: 65, Reward: 3083.492, Average Reward: 3084.809
    Episode: 66, Reward: 3106.169, Average Reward: 3085.128
    Episode: 67, Reward: 3049.493, Average Reward: 3084.604
    Episode: 68, Reward: 3052.765, Average Reward: 3084.143
    Episode: 69, Reward: 3069.514, Average Reward: 3083.934
    Episode: 70, Reward: 3078.326, Average Reward: 3083.855
    Episode: 71, Reward: 3029.836, Average Reward: 3083.104
    Episode: 72, Reward: 3102.420, Average Reward: 3083.369
    Episode: 73, Reward: 3109.371, Average Reward: 3083.720
    Episode: 74, Reward: 3072.457, Average Reward: 3083.570
    Episode: 75, Reward: 3054.384, Average Reward: 3083.186
    Episode: 76, Reward: 3078.454, Average Reward: 3083.125
    Episode: 77, Reward: 3106.549, Average Reward: 3083.425
    Episode: 78, Reward: 3078.511, Average Reward: 3083.363
    Episode: 79, Reward: 3105.936, Average Reward: 3083.645
    Episode: 80, Reward: 3041.385, Average Reward: 3083.123
    Episode: 81, Reward: 3156.135, Average Reward: 3084.014
    Episode: 82, Reward: 3096.057, Average Reward: 3084.159
    Episode: 83, Reward: 3083.667, Average Reward: 3084.153
    Episode: 84, Reward: 3092.678, Average Reward: 3084.253
    Episode: 85, Reward: 3111.701, Average Reward: 3084.572
    Episode: 86, Reward: 3089.617, Average Reward: 3084.630
    Episode: 87, Reward: 3099.176, Average Reward: 3084.796
    Episode: 88, Reward: 3098.595, Average Reward: 3084.951
    Episode: 89, Reward: 3096.093, Average Reward: 3085.074
    Episode: 90, Reward: 3101.840, Average Reward: 3085.259
    Episode: 91, Reward: 3158.592, Average Reward: 3086.056
    Episode: 92, Reward: 3069.718, Average Reward: 3085.880
    Episode: 93, Reward: 3093.104, Average Reward: 3085.957
    Episode: 94, Reward: 3088.042, Average Reward: 3085.979
    Episode: 95, Reward: 3128.184, Average Reward: 3086.419
    Episode: 96, Reward: 1643.653, Average Reward: 3071.545
    Episode: 97, Reward: 3058.191, Average Reward: 3071.408
    Episode: 98, Reward: 3125.157, Average Reward: 3071.951
    Episode: 99, Reward: 3115.474, Average Reward: 3072.387

    Max Episode Reward: 3158.5921401879696
    Min Episode Reward: 1643.6525326206677
    Max Timestep Reward: 3.8108226635786138
    Min Timestep Reward: -0.385434190957977
Starting Train/Test
    Epoch: 0
    Epoch: 1
    Epoch: 2
    Epoch: 3
    Epoch: 4
    Epoch: 5
    Epoch: 6
    Epoch: 7
    Epoch: 8
    Epoch: 9
    Epoch: 10
    Epoch: 11
    Epoch: 12
    Epoch: 13
    Epoch: 14
    Epoch: 15
    Epoch: 16
    Epoch: 17
    Epoch: 18
    Epoch: 19
    Epoch: 20
    Epoch: 21
    Epoch: 22
    Epoch: 23
    Epoch: 24
    Epoch: 25
    Epoch: 26
    Epoch: 27
    Epoch: 28
    Epoch: 29
    Epoch: 30
    Epoch: 31
    Epoch: 32
    Epoch: 33
    Epoch: 34
    Epoch: 35
    Epoch: 36
    Epoch: 37
    Epoch: 38
    Epoch: 39
    Epoch: 40
    Epoch: 41
    Epoch: 42
    Epoch: 43
    Epoch: 44
    Epoch: 45
    Epoch: 46
    Epoch: 47
    Epoch: 48
    Epoch: 49
    Epoch: 50
    Epoch: 51
    Epoch: 52
    Epoch: 53
    Epoch: 54
    Epoch: 55
    Epoch: 56
    Epoch: 57
    Epoch: 58
    Epoch: 59
    Epoch: 60
    Epoch: 61
    Epoch: 62
    Epoch: 63
    Epoch: 64
    Epoch: 65
    Epoch: 66
    Epoch: 67
    Epoch: 68
    Epoch: 69
    Epoch: 70
    Epoch: 71
    Epoch: 72
    Epoch: 73
    Epoch: 74
    Epoch: 75
    Epoch: 76
    Epoch: 77
    Epoch: 78
    Epoch: 79
    Epoch: 80
    Epoch: 81
    Epoch: 82
    Epoch: 83
    Epoch: 84
    Epoch: 85
    Epoch: 86
    Epoch: 87
    Epoch: 88
    Epoch: 89
    Epoch: 90
    Epoch: 91
    Epoch: 92
    Epoch: 93
    Epoch: 94
    Epoch: 95
    Epoch: 96
    Epoch: 97
    Epoch: 98
    Epoch: 99
Saving Coach to: models.ignore/coach/AntBulletEnv-v0/final_1_95%/


-----------------------------------------------------------------------------------------------------
 Testing Agent+Coach
-----------------------------------------------------------------------------------------------------


test settings = {
    "override_save_path": False, 
    "force_recompute": True, 
    "graph_smoothing": 0, 
    "number_of_episodes_for_baseline": 30, 
    "number_of_epochs_for_optimal_parameters": 15, 
    "number_of_episodes_for_testing": 10, 
    "initial_epsilon": 3.5, 
    "initial_horizon": 10, 
    "reward_discount": 0.98, 
    "acceptable_performance_levels": [0.8, 0.9, 0.95, 0.98, 0.99, ], 
    "acceptable_performance_loss": 1, 
    "acceptable_performance_level": 0.8, 
    "confidence_interval_for_convergence": 90, 
    "min_reward_single_timestep": 19, 
    "max_reward_single_timestep": 105, 
    "horizons": {0: 1, 0.0007: 4, 0.0015: 16, 0.002: 26, }, 
}
/home/jeffhykin/repos/AFRL/.venv/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(
argv[0]=
argv[0]=
Traceback (most recent call last):
  File "./main/run/full.py", line 50, in <module>
    full_run(
  File "./main/run/full.py", line 26, in full_run
    results = Tester.smart_load(
TypeError: run_epoch() missing 1 required positional argument: 'method'


-------------------------------------------------------

 Environment: ReacherBulletEnv-v0

-------------------------------------------------------




-----------------------------------------------------------------------------------------------------
 Agent Model Exists, loading: ./subrepos/rl-trained-agents/sac/ReacherBulletEnv-v0_1/ReacherBulletEnv-v0.zip
-----------------------------------------------------------------------------------------------------


pybullet build time: Oct 11 2021 20:52:37
Wrapping the env with a `Monitor` wrapper
Wrapping the env in a DummyVecEnv.


-----------------------------------------------------------------------------------------------------
 Coach Model Exists, loading: models.ignore/coach/ReacherBulletEnv-v0/final_1_95%/
-----------------------------------------------------------------------------------------------------


coach settings = {
    "force_retrain": False, 
    "learning_rate": 0.0001, 
    "hidden_sizes": [64, 64, 64, 64, ], 
    "loss_function": "state_prediction_loss", 
    "consistent_coach_loss": {"scale_future_state_loss": 1000000, }, 
    "delayed_coach_loss": {"epochs_of_delay": 50, }, 
    "value_plus_state_loss": {"value_proportion": 0.5, }, 
    "number_of_episodes": 100, 
    "number_of_epochs": 100, 
    "train_test_split": 0.7, 
    "batch_size": 128, 
    "with_card": True, 
}


-----------------------------------------------------------------------------------------------------
 Testing Agent+Coach
-----------------------------------------------------------------------------------------------------


test settings = {
    "override_save_path": False, 
    "force_recompute": True, 
    "graph_smoothing": 0, 
    "number_of_episodes_for_baseline": 30, 
    "number_of_epochs_for_optimal_parameters": 15, 
    "number_of_episodes_for_testing": 10, 
    "initial_epsilon": 3.5, 
    "initial_horizon": 10, 
    "reward_discount": 0.98, 
    "acceptable_performance_levels": [0.8, 0.9, 0.95, 0.98, 0.99, ], 
    "acceptable_performance_loss": 1, 
    "acceptable_performance_level": 0.8, 
    "confidence_interval_for_convergence": 90, 
}
/home/jeffhykin/repos/AFRL/.venv/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(
Traceback (most recent call last):
  File "./main/run/full.py", line 50, in <module>
    full_run(
  File "./main/run/full.py", line 26, in full_run
    results = Tester.smart_load(
TypeError: run_epoch() missing 1 required positional argument: 'method'

[silver_spectacle] There were unviewed results
[silver_spectacle] So the display server is still running at: http://localhost:9900
[silver_spectacle] (use the stop server button to kill it now)



-------------------------------------------------------

 Environment: LunarLanderContinuous-v2

-------------------------------------------------------




-----------------------------------------------------------------------------------------------------
 Agent Model Exists, loading: ./subrepos/rl-trained-agents/sac/LunarLanderContinuous-v2_1/LunarLanderContinuous-v2.zip
-----------------------------------------------------------------------------------------------------


Wrapping the env with a `Monitor` wrapper
Wrapping the env in a DummyVecEnv.


-----------------------------------------------------------------------------------------------------
 Coach Model Exists, loading: models.ignore/coach/LunarLanderContinuous-v2/final_1_98%/
-----------------------------------------------------------------------------------------------------


coach settings = {
    "force_retrain": False, 
    "learning_rate": 0.0001, 
    "hidden_sizes": [64, 64, 64, 64, ], 
    "loss_function": "state_prediction_loss", 
    "consistent_coach_loss": {"scale_future_state_loss": 1000000, }, 
    "delayed_coach_loss": {"epochs_of_delay": 50, }, 
    "value_plus_state_loss": {"value_proportion": 0.5, }, 
    "number_of_episodes": 100, 
    "number_of_epochs": 100, 
    "train_test_split": 0.7, 
    "batch_size": 128, 
    "with_card": True, 
}


-----------------------------------------------------------------------------------------------------
 Testing Agent+Coach
-----------------------------------------------------------------------------------------------------


test settings = {
    "override_save_path": False, 
    "force_recompute": True, 
    "graph_smoothing": 0, 
    "number_of_episodes_for_baseline": 30, 
    "number_of_epochs_for_optimal_parameters": 15, 
    "number_of_episodes_for_testing": 10, 
    "initial_epsilon": 3.5, 
    "initial_horizon": 10, 
    "reward_discount": 0.98, 
    "acceptable_performance_levels": [0.8, 0.9, 0.95, 0.98, 0.99, ], 
    "acceptable_performance_loss": 1, 
    "acceptable_performance_level": 0.8, 
    "confidence_interval_for_convergence": 90, 
    "min_reward_single_timestep": -100, 
    "max_reward_single_timestep": 70, 
    "horizons": {0.0: 1, 0.005: 10, 0.01: 20, }, 
}
Traceback (most recent call last):
  File "./main/run/full.py", line 50, in <module>
    full_run(
  File "./main/run/full.py", line 26, in full_run
    results = Tester.smart_load(
TypeError: run_epoch() missing 1 required positional argument: 'method'

[silver_spectacle] There were unviewed results
[silver_spectacle] So the display server is still running at: http://localhost:9900
[silver_spectacle] (use the stop server button to kill it now)



-------------------------------------------------------

 Environment: HopperBulletEnv-v0

-------------------------------------------------------




-----------------------------------------------------------------------------------------------------
 Agent Model Exists, loading: ./subrepos/rl-trained-agents/sac/HopperBulletEnv-v0_1/HopperBulletEnv-v0.zip
-----------------------------------------------------------------------------------------------------


pybullet build time: Oct 11 2021 20:52:37
Wrapping the env with a `Monitor` wrapper
Wrapping the env in a DummyVecEnv.


-----------------------------------------------------------------------------------------------------
 Coach Model Exists, loading: models.ignore/coach/HopperBulletEnv-v0/final_1_98%/
-----------------------------------------------------------------------------------------------------


coach settings = {
    "force_retrain": False, 
    "learning_rate": 0.0001, 
    "hidden_sizes": [64, 64, 64, 64, ], 
    "loss_function": "state_prediction_loss", 
    "consistent_coach_loss": {"scale_future_state_loss": 1000000, }, 
    "delayed_coach_loss": {"epochs_of_delay": 50, }, 
    "value_plus_state_loss": {"value_proportion": 0.5, }, 
    "number_of_episodes": 100, 
    "number_of_epochs": 100, 
    "train_test_split": 0.7, 
    "batch_size": 128, 
    "with_card": True, 
}


-----------------------------------------------------------------------------------------------------
 Testing Agent+Coach
-----------------------------------------------------------------------------------------------------


test settings = {
    "override_save_path": False, 
    "force_recompute": True, 
    "graph_smoothing": 0, 
    "number_of_episodes_for_baseline": 30, 
    "number_of_epochs_for_optimal_parameters": 15, 
    "number_of_episodes_for_testing": 10, 
    "initial_epsilon": 3.5, 
    "initial_horizon": 10, 
    "reward_discount": 0.98, 
    "acceptable_performance_levels": [0.8, 0.9, 0.95, 0.98, 0.99, ], 
    "acceptable_performance_loss": 1, 
    "acceptable_performance_level": 0.8, 
    "confidence_interval_for_convergence": 90, 
    "min_reward_single_timestep": 18, 
    "max_reward_single_timestep": 110, 
    "horizons": {0.1: 26, 0.01: 16, 0.001: 11, }, 
}
/home/jeffhykin/repos/AFRL/.venv/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(
Traceback (most recent call last):
  File "./main/run/full.py", line 50, in <module>
    full_run(
  File "./main/run/full.py", line 26, in full_run
    results = Tester.smart_load(
TypeError: run_epoch() missing 1 required positional argument: 'method'


-------------------------------------------------------

 Environment: HalfCheetahBulletEnv-v0

-------------------------------------------------------




-----------------------------------------------------------------------------------------------------
 Agent Model Exists, loading: subrepos/rl-trained-agents/sac/HalfCheetahBulletEnv-v0_1/HalfCheetahBulletEnv-v0.zip
-----------------------------------------------------------------------------------------------------


pybullet build time: Oct 11 2021 20:52:37
Wrapping the env with a `Monitor` wrapper
Wrapping the env in a DummyVecEnv.


-----------------------------------------------------------------------------------------------------
 Training Coach Model from scrach
-----------------------------------------------------------------------------------------------------


coach settings = {
    "force_retrain": False, 
    "learning_rate": 0.0001, 
    "hidden_sizes": [64, 64, 64, 64, ], 
    "loss_function": "state_prediction_loss", 
    "consistent_coach_loss": {"scale_future_state_loss": 1000000, }, 
    "delayed_coach_loss": {"epochs_of_delay": 50, }, 
    "value_plus_state_loss": {"value_proportion": 0.5, }, 
    "number_of_episodes": 100, 
    "number_of_epochs": 100, 
    "train_test_split": 0.7, 
    "batch_size": 128, 
    "with_card": True, 
}
Starting Experience Recording
    Episode: 0, Reward: 2775.986, Average Reward: 2775.986
    Episode: 1, Reward: 2818.204, Average Reward: 2797.095
    Episode: 2, Reward: 2818.792, Average Reward: 2804.327
    Episode: 3, Reward: 2805.103, Average Reward: 2804.521
    Episode: 4, Reward: 2797.436, Average Reward: 2803.104
    Episode: 5, Reward: 2786.826, Average Reward: 2800.391
    Episode: 6, Reward: 2785.474, Average Reward: 2798.260
    Episode: 7, Reward: 2792.858, Average Reward: 2797.585
    Episode: 8, Reward: 2809.690, Average Reward: 2798.930
    Episode: 9, Reward: 2781.863, Average Reward: 2797.223
    Episode: 10, Reward: 2789.674, Average Reward: 2796.537
    Episode: 11, Reward: 2805.874, Average Reward: 2797.315
    Episode: 12, Reward: 2801.853, Average Reward: 2797.664
    Episode: 13, Reward: 2790.657, Average Reward: 2797.164
    Episode: 14, Reward: 2795.531, Average Reward: 2797.055
    Episode: 15, Reward: 2790.157, Average Reward: 2796.624
    Episode: 16, Reward: 2801.029, Average Reward: 2796.883
    Episode: 17, Reward: 2776.931, Average Reward: 2795.774
    Episode: 18, Reward: 2776.975, Average Reward: 2794.785
    Episode: 19, Reward: 2804.053, Average Reward: 2795.248
    Episode: 20, Reward: 2797.313, Average Reward: 2795.347
    Episode: 21, Reward: 2797.982, Average Reward: 2795.466
    Episode: 22, Reward: 2790.577, Average Reward: 2795.254
    Episode: 23, Reward: 2794.885, Average Reward: 2795.239
    Episode: 24, Reward: 2781.944, Average Reward: 2794.707
    Episode: 25, Reward: 2790.160, Average Reward: 2794.532
    Episode: 26, Reward: 2796.210, Average Reward: 2794.594
    Episode: 27, Reward: 2806.243, Average Reward: 2795.010
    Episode: 28, Reward: 2776.968, Average Reward: 2794.388
    Episode: 29, Reward: 2784.440, Average Reward: 2794.056
    Episode: 30, Reward: 2802.543, Average Reward: 2794.330
    Episode: 31, Reward: 2797.005, Average Reward: 2794.414
    Episode: 32, Reward: 2789.918, Average Reward: 2794.277
    Episode: 33, Reward: 2810.326, Average Reward: 2794.749
    Episode: 34, Reward: 2789.056, Average Reward: 2794.587
    Episode: 35, Reward: 2794.855, Average Reward: 2794.594
    Episode: 36, Reward: 2804.433, Average Reward: 2794.860
    Episode: 37, Reward: 2800.227, Average Reward: 2795.001
    Episode: 38, Reward: 2813.098, Average Reward: 2795.465
    Episode: 39, Reward: 2791.906, Average Reward: 2795.376
    Episode: 40, Reward: 2787.512, Average Reward: 2795.185
    Episode: 41, Reward: 2803.500, Average Reward: 2795.383
    Episode: 42, Reward: 2785.800, Average Reward: 2795.160
    Episode: 43, Reward: 2791.492, Average Reward: 2795.076
    Episode: 44, Reward: 2809.257, Average Reward: 2795.391
    Episode: 45, Reward: 2798.803, Average Reward: 2795.466
    Episode: 46, Reward: 2793.452, Average Reward: 2795.423
    Episode: 47, Reward: 2797.733, Average Reward: 2795.471
    Episode: 48, Reward: 2775.205, Average Reward: 2795.057
    Episode: 49, Reward: 2772.586, Average Reward: 2794.608
    Episode: 50, Reward: 2798.854, Average Reward: 2794.691
    Episode: 51, Reward: 2802.006, Average Reward: 2794.832
    Episode: 52, Reward: 2787.689, Average Reward: 2794.697
    Episode: 53, Reward: 2783.833, Average Reward: 2794.496
    Episode: 54, Reward: 2789.937, Average Reward: 2794.413
    Episode: 55, Reward: 2777.859, Average Reward: 2794.117
    Episode: 56, Reward: 2799.660, Average Reward: 2794.215
    Episode: 57, Reward: 2801.112, Average Reward: 2794.334
    Episode: 58, Reward: 2795.957, Average Reward: 2794.361
    Episode: 59, Reward: 2790.315, Average Reward: 2794.294
    Episode: 60, Reward: 2773.958, Average Reward: 2793.960
    Episode: 61, Reward: 2807.830, Average Reward: 2794.184
    Episode: 62, Reward: 2807.202, Average Reward: 2794.391
    Episode: 63, Reward: 2795.388, Average Reward: 2794.406
    Episode: 64, Reward: 2790.969, Average Reward: 2794.353
    Episode: 65, Reward: 2831.965, Average Reward: 2794.923
    Episode: 66, Reward: 2780.524, Average Reward: 2794.708
    Episode: 67, Reward: 2801.335, Average Reward: 2794.806
    Episode: 68, Reward: 2819.915, Average Reward: 2795.170
    Episode: 69, Reward: 2790.505, Average Reward: 2795.103
    Episode: 70, Reward: 2790.490, Average Reward: 2795.038
    Episode: 71, Reward: 2770.599, Average Reward: 2794.699
    Episode: 72, Reward: 2803.167, Average Reward: 2794.815
    Episode: 73, Reward: 2797.834, Average Reward: 2794.855
    Episode: 74, Reward: 2795.622, Average Reward: 2794.866
    Episode: 75, Reward: 2780.361, Average Reward: 2794.675
    Episode: 76, Reward: 2785.919, Average Reward: 2794.561
    Episode: 77, Reward: 2805.360, Average Reward: 2794.700
    Episode: 78, Reward: 2790.635, Average Reward: 2794.648
    Episode: 79, Reward: 2804.296, Average Reward: 2794.769
    Episode: 80, Reward: 2789.260, Average Reward: 2794.701
    Episode: 81, Reward: 2769.358, Average Reward: 2794.392
    Episode: 82, Reward: 2809.566, Average Reward: 2794.574
    Episode: 83, Reward: 2778.945, Average Reward: 2794.388
    Episode: 84, Reward: 2794.581, Average Reward: 2794.391
    Episode: 85, Reward: 2788.807, Average Reward: 2794.326
    Episode: 86, Reward: 2777.549, Average Reward: 2794.133
    Episode: 87, Reward: 2782.088, Average Reward: 2793.996
    Episode: 88, Reward: 2795.715, Average Reward: 2794.015
    Episode: 89, Reward: 2792.981, Average Reward: 2794.004
    Episode: 90, Reward: 2783.761, Average Reward: 2793.891
    Episode: 91, Reward: 2802.843, Average Reward: 2793.989
    Episode: 92, Reward: 2787.198, Average Reward: 2793.916
    Episode: 93, Reward: 2788.139, Average Reward: 2793.854
    Episode: 94, Reward: 2810.643, Average Reward: 2794.031
    Episode: 95, Reward: 2785.584, Average Reward: 2793.943
    Episode: 96, Reward: 2796.163, Average Reward: 2793.966
    Episode: 97, Reward: 2783.674, Average Reward: 2793.861
    Episode: 98, Reward: 2791.952, Average Reward: 2793.841
    Episode: 99, Reward: 2787.547, Average Reward: 2793.778

    Max Episode Reward: 2831.9653381676285
    Min Episode Reward: 2769.3580550793117
    Max Timestep Reward: 3.386440528333601
    Min Timestep Reward: 0.43041492644561963
Starting Train/Test
    Epoch: 0
    Epoch: 1
    Epoch: 2
    Epoch: 3
    Epoch: 4
    Epoch: 5
    Epoch: 6
    Epoch: 7
    Epoch: 8
    Epoch: 9
    Epoch: 10
    Epoch: 11
    Epoch: 12
    Epoch: 13
    Epoch: 14
    Epoch: 15
    Epoch: 16
    Epoch: 17
    Epoch: 18
    Epoch: 19
    Epoch: 20
    Epoch: 21
    Epoch: 22
    Epoch: 23
    Epoch: 24
    Epoch: 25
    Epoch: 26
    Epoch: 27
    Epoch: 28
    Epoch: 29
    Epoch: 30
    Epoch: 31
    Epoch: 32
    Epoch: 33
    Epoch: 34
    Epoch: 35
    Epoch: 36
    Epoch: 37
    Epoch: 38
    Epoch: 39
    Epoch: 40
    Epoch: 41
    Epoch: 42
    Epoch: 43
    Epoch: 44
    Epoch: 45
    Epoch: 46
    Epoch: 47
    Epoch: 48
    Epoch: 49
    Epoch: 50
    Epoch: 51
    Epoch: 52
    Epoch: 53
    Epoch: 54
    Epoch: 55
    Epoch: 56
    Epoch: 57
    Epoch: 58
    Epoch: 59
    Epoch: 60
    Epoch: 61
    Epoch: 62
    Epoch: 63
    Epoch: 64
    Epoch: 65
    Epoch: 66
    Epoch: 67
    Epoch: 68
    Epoch: 69
    Epoch: 70
    Epoch: 71
    Epoch: 72
    Epoch: 73
    Epoch: 74
    Epoch: 75
    Epoch: 76
    Epoch: 77
    Epoch: 78
    Epoch: 79
    Epoch: 80
    Epoch: 81
    Epoch: 82
    Epoch: 83
    Epoch: 84
    Epoch: 85
    Epoch: 86
    Epoch: 87
    Epoch: 88
    Epoch: 89
    Epoch: 90
    Epoch: 91
    Epoch: 92
    Epoch: 93
    Epoch: 94
    Epoch: 95
    Epoch: 96
    Epoch: 97
    Epoch: 98
    Epoch: 99
Saving Coach to: models.ignore/coach/HalfCheetahBulletEnv-v0/final_1_98%/


-----------------------------------------------------------------------------------------------------
 Testing Agent+Coach
-----------------------------------------------------------------------------------------------------


test settings = {
    "override_save_path": False, 
    "force_recompute": True, 
    "graph_smoothing": 0, 
    "number_of_episodes_for_baseline": 30, 
    "number_of_epochs_for_optimal_parameters": 15, 
    "number_of_episodes_for_testing": 10, 
    "initial_epsilon": 3.5, 
    "initial_horizon": 10, 
    "reward_discount": 0.98, 
    "acceptable_performance_levels": [0.8, 0.9, 0.95, 0.98, 0.99, ], 
    "acceptable_performance_loss": 1, 
    "acceptable_performance_level": 0.8, 
    "confidence_interval_for_convergence": 90, 
    "min_reward_single_timestep": -60, 
    "max_reward_single_timestep": 100, 
    "horizons": {0: 1, 0.001: 11, 0.01: 13, 0.1: 26, }, 
}
/home/jeffhykin/repos/AFRL/.venv/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(
argv[0]=
argv[0]=
Traceback (most recent call last):
  File "./main/run/full.py", line 50, in <module>
    full_run(
  File "./main/run/full.py", line 26, in full_run
    results = Tester.smart_load(
TypeError: run_epoch() missing 1 required positional argument: 'method'

[silver_spectacle] There were unviewed results
[silver_spectacle] So the display server is still running at: http://localhost:9900
[silver_spectacle] (use the stop server button to kill it now)



-------------------------------------------------------

 Environment: BipedalWalker-v3

-------------------------------------------------------




-----------------------------------------------------------------------------------------------------
 Agent Model Exists, loading: ./subrepos/rl-trained-agents/sac/BipedalWalker-v3_1/BipedalWalker-v3.zip
-----------------------------------------------------------------------------------------------------


Wrapping the env with a `Monitor` wrapper
Wrapping the env in a DummyVecEnv.


-----------------------------------------------------------------------------------------------------
 Training Coach Model from scrach
-----------------------------------------------------------------------------------------------------


coach settings = {
    "force_retrain": False, 
    "learning_rate": 0.0001, 
    "hidden_sizes": [64, 64, 64, 64, ], 
    "loss_function": "state_prediction_loss", 
    "consistent_coach_loss": {"scale_future_state_loss": 1000000, }, 
    "delayed_coach_loss": {"epochs_of_delay": 50, }, 
    "value_plus_state_loss": {"value_proportion": 0.5, }, 
    "number_of_episodes": 100, 
    "number_of_epochs": 100, 
    "train_test_split": 0.7, 
    "batch_size": 128, 
    "with_card": True, 
}
Starting Experience Recording
    Episode: 0, Reward: 296.732, Average Reward: 296.732
    Episode: 1, Reward: 300.756, Average Reward: 298.744
    Episode: 2, Reward: 299.516, Average Reward: 299.002
    Episode: 3, Reward: 300.644, Average Reward: 299.412
    Episode: 4, Reward: 298.960, Average Reward: 299.322
    Episode: 5, Reward: 298.174, Average Reward: 299.130
    Episode: 6, Reward: 301.848, Average Reward: 299.519
    Episode: 7, Reward: 302.958, Average Reward: 299.949
    Episode: 8, Reward: 301.368, Average Reward: 300.106
    Episode: 9, Reward: 298.777, Average Reward: 299.973
    Episode: 10, Reward: 299.658, Average Reward: 299.945
    Episode: 11, Reward: 300.377, Average Reward: 299.981
    Episode: 12, Reward: 302.229, Average Reward: 300.154
    Episode: 13, Reward: 300.492, Average Reward: 300.178
    Episode: 14, Reward: 300.412, Average Reward: 300.193
    Episode: 15, Reward: 302.323, Average Reward: 300.327
    Episode: 16, Reward: 300.102, Average Reward: 300.313
    Episode: 17, Reward: 300.247, Average Reward: 300.310
    Episode: 18, Reward: 301.501, Average Reward: 300.372
    Episode: 19, Reward: 298.139, Average Reward: 300.261
    Episode: 20, Reward: 300.282, Average Reward: 300.262
    Episode: 21, Reward: 301.990, Average Reward: 300.340
    Episode: 22, Reward: 300.584, Average Reward: 300.351
    Episode: 23, Reward: 299.930, Average Reward: 300.333
    Episode: 24, Reward: 301.570, Average Reward: 300.383
    Episode: 25, Reward: 301.496, Average Reward: 300.426
    Episode: 26, Reward: 297.545, Average Reward: 300.319
    Episode: 27, Reward: 300.096, Average Reward: 300.311
    Episode: 28, Reward: 300.531, Average Reward: 300.319
    Episode: 29, Reward: 301.036, Average Reward: 300.343
    Episode: 30, Reward: 300.171, Average Reward: 300.337
    Episode: 31, Reward: 301.786, Average Reward: 300.382
    Episode: 32, Reward: 300.757, Average Reward: 300.394
    Episode: 33, Reward: 300.048, Average Reward: 300.383
    Episode: 34, Reward: 299.013, Average Reward: 300.344
    Episode: 35, Reward: 300.816, Average Reward: 300.357
    Episode: 36, Reward: 301.079, Average Reward: 300.377
    Episode: 37, Reward: 299.581, Average Reward: 300.356
    Episode: 38, Reward: 300.861, Average Reward: 300.369
    Episode: 39, Reward: 297.389, Average Reward: 300.294
    Episode: 40, Reward: 299.926, Average Reward: 300.285
    Episode: 41, Reward: 301.236, Average Reward: 300.308
    Episode: 42, Reward: 298.304, Average Reward: 300.261
    Episode: 43, Reward: 300.691, Average Reward: 300.271
    Episode: 44, Reward: 302.745, Average Reward: 300.326
    Episode: 45, Reward: 301.050, Average Reward: 300.342
    Episode: 46, Reward: 298.081, Average Reward: 300.294
    Episode: 47, Reward: 301.005, Average Reward: 300.309
    Episode: 48, Reward: 301.475, Average Reward: 300.332
    Episode: 49, Reward: 301.056, Average Reward: 300.347
    Episode: 50, Reward: 300.831, Average Reward: 300.356
    Episode: 51, Reward: 300.914, Average Reward: 300.367
    Episode: 52, Reward: 301.052, Average Reward: 300.380
    Episode: 53, Reward: 301.431, Average Reward: 300.400
    Episode: 54, Reward: 298.994, Average Reward: 300.374
    Episode: 55, Reward: 297.915, Average Reward: 300.330
    Episode: 56, Reward: 297.259, Average Reward: 300.276
    Episode: 57, Reward: 301.384, Average Reward: 300.295
    Episode: 58, Reward: 299.744, Average Reward: 300.286
    Episode: 59, Reward: 299.442, Average Reward: 300.272
    Episode: 60, Reward: 299.436, Average Reward: 300.258
    Episode: 61, Reward: 299.113, Average Reward: 300.240
    Episode: 62, Reward: 299.451, Average Reward: 300.227
    Episode: 63, Reward: 299.263, Average Reward: 300.212
    Episode: 64, Reward: 162.054, Average Reward: 298.087
    Episode: 65, Reward: 300.100, Average Reward: 298.117
    Episode: 66, Reward: 301.277, Average Reward: 298.164
    Episode: 67, Reward: 302.201, Average Reward: 298.224
    Episode: 68, Reward: 302.571, Average Reward: 298.287
    Episode: 69, Reward: 299.649, Average Reward: 298.306
    Episode: 70, Reward: 299.960, Average Reward: 298.329
    Episode: 71, Reward: 301.929, Average Reward: 298.379
    Episode: 72, Reward: 297.933, Average Reward: 298.373
    Episode: 73, Reward: 298.462, Average Reward: 298.374
    Episode: 74, Reward: 302.002, Average Reward: 298.423
    Episode: 75, Reward: 299.854, Average Reward: 298.442
    Episode: 76, Reward: 301.051, Average Reward: 298.476
    Episode: 77, Reward: 301.824, Average Reward: 298.519
    Episode: 78, Reward: 302.586, Average Reward: 298.570
    Episode: 79, Reward: -83.809, Average Reward: 293.790
    Episode: 80, Reward: 298.657, Average Reward: 293.850
    Episode: 81, Reward: 301.068, Average Reward: 293.938
    Episode: 82, Reward: 299.835, Average Reward: 294.009
    Episode: 83, Reward: 301.139, Average Reward: 294.094
    Episode: 84, Reward: 300.070, Average Reward: 294.165
    Episode: 85, Reward: 301.151, Average Reward: 294.246
    Episode: 86, Reward: 301.612, Average Reward: 294.330
    Episode: 87, Reward: 302.788, Average Reward: 294.427
    Episode: 88, Reward: 301.862, Average Reward: 294.510
    Episode: 89, Reward: 300.745, Average Reward: 294.579
    Episode: 90, Reward: -85.230, Average Reward: 290.406
    Episode: 91, Reward: 302.321, Average Reward: 290.535
    Episode: 92, Reward: 298.676, Average Reward: 290.623
    Episode: 93, Reward: 297.760, Average Reward: 290.699
    Episode: 94, Reward: 301.766, Average Reward: 290.815
    Episode: 95, Reward: 300.706, Average Reward: 290.918
    Episode: 96, Reward: 301.072, Average Reward: 291.023
    Episode: 97, Reward: 301.378, Average Reward: 291.129
    Episode: 98, Reward: -83.156, Average Reward: 287.348
    Episode: 99, Reward: 301.303, Average Reward: 287.487

    Max Episode Reward: 302.9582045736901
    Min Episode Reward: -85.22963529853088
    Max Timestep Reward: 0.511177140345157
    Min Timestep Reward: -100
Starting Train/Test
    Epoch: 0
    Epoch: 1
    Epoch: 2
    Epoch: 3
    Epoch: 4
    Epoch: 5
    Epoch: 6
    Epoch: 7
    Epoch: 8
    Epoch: 9
    Epoch: 10
    Epoch: 11
    Epoch: 12
    Epoch: 13
    Epoch: 14
    Epoch: 15
    Epoch: 16
    Epoch: 17
    Epoch: 18
    Epoch: 19
    Epoch: 20
    Epoch: 21
    Epoch: 22
    Epoch: 23
    Epoch: 24
    Epoch: 25
    Epoch: 26
    Epoch: 27
    Epoch: 28
    Epoch: 29
    Epoch: 30
    Epoch: 31
    Epoch: 32
    Epoch: 33
    Epoch: 34
    Epoch: 35
    Epoch: 36
    Epoch: 37
    Epoch: 38
    Epoch: 39
    Epoch: 40
    Epoch: 41
    Epoch: 42
    Epoch: 43
    Epoch: 44
    Epoch: 45
    Epoch: 46
    Epoch: 47
    Epoch: 48
    Epoch: 49
    Epoch: 50
    Epoch: 51
    Epoch: 52
    Epoch: 53
    Epoch: 54
    Epoch: 55
    Epoch: 56
    Epoch: 57
    Epoch: 58
    Epoch: 59
    Epoch: 60
    Epoch: 61
    Epoch: 62
    Epoch: 63
    Epoch: 64
    Epoch: 65
    Epoch: 66
    Epoch: 67
    Epoch: 68
    Epoch: 69
    Epoch: 70
    Epoch: 71
    Epoch: 72
    Epoch: 73
    Epoch: 74
    Epoch: 75
    Epoch: 76
    Epoch: 77
    Epoch: 78
    Epoch: 79
    Epoch: 80
    Epoch: 81
    Epoch: 82
    Epoch: 83
    Epoch: 84
    Epoch: 85
    Epoch: 86
    Epoch: 87
    Epoch: 88
    Epoch: 89
    Epoch: 90
    Epoch: 91
    Epoch: 92
    Epoch: 93
    Epoch: 94
    Epoch: 95
    Epoch: 96
    Epoch: 97
    Epoch: 98
    Epoch: 99
Saving Coach to: models.ignore/coach/BipedalWalker-v3/final_1_98%/


-----------------------------------------------------------------------------------------------------
 Testing Agent+Coach
-----------------------------------------------------------------------------------------------------


test settings = {
    "override_save_path": False, 
    "increment_factor": 1.2, 
    "force_recompute": True, 
    "graph_smoothing": 0, 
    "number_of_episodes_for_baseline": 30, 
    "number_of_epochs_for_optimal_parameters": 15, 
    "number_of_episodes_for_testing": 10, 
    "initial_epsilon": 3.5, 
    "initial_horizon": 10, 
    "reward_discount": 0.98, 
    "acceptable_performance_levels": [0.8, 0.9, 0.95, 0.98, 0.99, ], 
    "acceptable_performance_loss": 1, 
    "acceptable_performance_level": 0.8, 
    "confidence_interval_for_convergence": 90, 
    "min_reward_single_timestep": -23, 
    "max_reward_single_timestep": 5, 
    "horizons": {0.1: 26, 0.01: 16, 0.001: 11, }, 
}
----- getting a reward baseline -----------------------------------------------------------------------------------------------------------------------------------
  episode_index=0, episode_discounted_reward_sum=5.004596852368433
  episode_index=1, episode_discounted_reward_sum=4.641344641559698
  episode_index=2, episode_discounted_reward_sum=5.069003548471763
  episode_index=3, episode_discounted_reward_sum=4.4712099021539045
  episode_index=4, episode_discounted_reward_sum=4.902690376392537
  episode_index=5, episode_discounted_reward_sum=-10.702806634521142
  episode_index=6, episode_discounted_reward_sum=4.935355209063155
  episode_index=7, episode_discounted_reward_sum=4.578476610765562
  episode_index=8, episode_discounted_reward_sum=4.95880949607053
  episode_index=9, episode_discounted_reward_sum=4.91677972111129
  episode_index=10, episode_discounted_reward_sum=4.798055657586121
  episode_index=11, episode_discounted_reward_sum=4.7291702129784525
  episode_index=12, episode_discounted_reward_sum=5.153314304285418
  episode_index=13, episode_discounted_reward_sum=5.070673278090445
  episode_index=14, episode_discounted_reward_sum=4.9622866774944985
  episode_index=15, episode_discounted_reward_sum=4.707474015549444
  episode_index=16, episode_discounted_reward_sum=4.963924995297293
  episode_index=17, episode_discounted_reward_sum=5.037549859019525
  episode_index=18, episode_discounted_reward_sum=5.146066755650261
  episode_index=19, episode_discounted_reward_sum=5.1521917806360324
  episode_index=20, episode_discounted_reward_sum=5.08766071398125
  episode_index=21, episode_discounted_reward_sum=5.038714111945687
  episode_index=22, episode_discounted_reward_sum=5.20351230853991
  episode_index=23, episode_discounted_reward_sum=5.207174685518645
  episode_index=24, episode_discounted_reward_sum=5.101446945386207
  episode_index=25, episode_discounted_reward_sum=4.6803100321905875
  episode_index=26, episode_discounted_reward_sum=5.224873560555606
  episode_index=27, episode_discounted_reward_sum=5.034623833017168
  episode_index=28, episode_discounted_reward_sum=4.950135386384794
  episode_index=29, episode_discounted_reward_sum=5.009157613956245
    baseline = {
    "max": 5.224873560555606, 
    "min": -10.702806634521142, 
    "range": 15.927680195076748, 
    "count": 30, 
    "sum": 133.0337764514993, 
    "average": 4.434459215049977, 
    "stdev": 2.8655697989317543, 
    "median": 4.9842609238328635, 
}
    baseline_min = 3.5455115124099996, baseline_max = 5.3234069176899546,
    baseline_confidence_size = 0.8889477026399775
    ----- finding optimal epsilon -------------------------------------------------------------------------------------------------------------------------------------
            reward_single_sum=-16.570546816368335, 
            reward_single_sum=-18.042293779236086, confidence_size=4.646122308104971, confidence_max=-12.660297989697245, new_horizon=2
        episode=0, horizon=20, effective_score=-17.31, baseline_lowerbound=3.55 baseline_stdev=0.52, new_epsilon=2.9167, bad=True, gap_average=0.5899948229108538
            reward_single_sum=-17.033298370630515, 
            reward_single_sum=-16.518528558581135, confidence_size=1.6250643403002831, confidence_max=-15.150849124305543, new_horizon=2
        episode=1, horizon=20, effective_score=-16.78, baseline_lowerbound=3.55 baseline_stdev=0.52, new_epsilon=2.4306, bad=True, gap_average=0.6905327462880625
            reward_single_sum=-15.714797601559468, 
            reward_single_sum=-17.32752025743213, confidence_size=5.091165055734906, confidence_max=-11.429993873760894, new_horizon=4
        episode=2, horizon=20, effective_score=-16.52, baseline_lowerbound=3.55 baseline_stdev=0.52, new_epsilon=2.0255, bad=True, gap_average=0.22241610255321312
            reward_single_sum=-15.472249681603454, 
            reward_single_sum=-18.003044046469167, confidence_size=7.989403377410284, confidence_max=-8.74824348662603, new_horizon=2
        episode=3, horizon=20, effective_score=-16.74, baseline_lowerbound=3.55 baseline_stdev=0.52, new_epsilon=1.6879, bad=True, gap_average=0.4649280671323283
            reward_single_sum=-15.460612095168669, 
            reward_single_sum=-17.027861339121685, confidence_size=4.947611144039488, confidence_max=-11.296625573105693, new_horizon=2
        episode=4, horizon=20, effective_score=-16.24, baseline_lowerbound=3.55 baseline_stdev=0.52, new_epsilon=1.4066, bad=True, gap_average=0.6139095027143784
            reward_single_sum=-16.930140364702655, 
            reward_single_sum=-17.352999810006438, confidence_size=1.3349147316673182, confidence_max=-15.806655355687226, new_horizon=2
        episode=5, horizon=20, effective_score=-17.14, baseline_lowerbound=3.55 baseline_stdev=0.52, new_epsilon=1.1721, bad=True, gap_average=0.5796957910060883
            reward_single_sum=-16.82017162724555, 
            reward_single_sum=-17.199756324265433, confidence_size=1.1983017279022707, confidence_max=-15.81166224785322, new_horizon=3
        episode=6, horizon=20, effective_score=-17.01, baseline_lowerbound=3.55 baseline_stdev=0.52, new_epsilon=0.9768, bad=True, gap_average=0.48892827007357637
            reward_single_sum=-10.966847357438992, 
            reward_single_sum=-16.877090666692467, confidence_size=18.657903823320613, confidence_max=4.735934811254872, new_horizon=2
            reward_single_sum=-16.642544612588168, confidence_size=5.6419283024248825, confidence_max=-9.186899243148327, new_horizon=2
        episode=7, horizon=20, effective_score=-14.83, baseline_lowerbound=3.55 baseline_stdev=0.52, new_epsilon=0.8140, bad=True, gap_average=0.9207911941554997
            reward_single_sum=-16.315964410898648, 
            reward_single_sum=-11.064890006529506, confidence_size=16.576989487459045, confidence_max=2.8865622787449574, new_horizon=2
        episode=8, horizon=20, effective_score=-13.69, baseline_lowerbound=3.55 baseline_stdev=0.52, new_epsilon=0.6783, bad=True, gap_average=1.0367304762008742
            reward_single_sum=-16.87268341825597, 
            reward_single_sum=-16.543066789059893, confidence_size=1.040558745945158, confidence_max=-15.667316357712776, new_horizon=2
        episode=9, horizon=20, effective_score=-16.71, baseline_lowerbound=3.55 baseline_stdev=0.52, new_epsilon=0.5653, bad=True, gap_average=0.13867612790795011
            reward_single_sum=-18.20059953935836, 
            reward_single_sum=-15.508707596056952, confidence_size=8.497968417349849, confidence_max=-8.356685150357812, new_horizon=2
        episode=10, horizon=20, effective_score=-16.85, baseline_lowerbound=3.55 baseline_stdev=0.52, new_epsilon=0.4711, bad=True, gap_average=0.5156685586748176
            reward_single_sum=-15.132747199148646, 
            reward_single_sum=-17.62612560390612, confidence_size=7.871285840004726, confidence_max=-8.508150561522665, new_horizon=2
        episode=11, horizon=20, effective_score=-16.38, baseline_lowerbound=3.55 baseline_stdev=0.52, new_epsilon=0.3925, bad=True, gap_average=1.7349478291428608
            reward_single_sum=4.230659600769675, 
            reward_single_sum=4.000404439073743, confidence_size=0.726886937974212, confidence_max=4.84241895789592, new_horizon=4
        episode=12, horizon=2, effective_score=4.12, baseline_lowerbound=3.55 baseline_stdev=0.52, new_epsilon=0.4711, bad=False, gap_average=0.48304631526964253
            reward_single_sum=-17.943263056684604, 
            reward_single_sum=-16.005561137910806, confidence_size=6.11708421244537, confidence_max=-10.857327884852339, new_horizon=2
        episode=13, horizon=4, effective_score=-16.97, baseline_lowerbound=3.55 baseline_stdev=0.52, new_epsilon=0.3925, bad=True, gap_average=0.5239842661311117
            reward_single_sum=-14.308822472189314, 
            reward_single_sum=4.353195377692533, confidence_size=58.9136717344668, confidence_max=53.935858187218386, new_horizon=4
            reward_single_sum=-10.145072161149793, confidence_size=16.515172285212728, confidence_max=9.814939199997198, new_horizon=4
            reward_single_sum=4.278022864581769, confidence_size=11.41497325432076, confidence_max=7.459304156554555, new_horizon=4
            reward_single_sum=4.325807059419406, confidence_size=8.753479251690027, confidence_max=6.454105385360945, new_horizon=4
            reward_single_sum=4.378674740055147, confidence_size=7.118164162928593, confidence_max=5.93179839766355, new_horizon=4
            reward_single_sum=4.3255670387719345, confidence_size=5.999770933459343, confidence_max=5.600824140199583, new_horizon=4
            reward_single_sum=-10.05517262361695, confidence_size=5.558220395108392, confidence_max=3.952245373053982, new_horizon=4
            reward_single_sum=-12.48498350676733, confidence_size=5.310432299772477, confidence_max=2.495678557194409, new_horizon=4
        episode=14, horizon=20, effective_score=-2.81, baseline_lowerbound=3.55 baseline_stdev=0.52, new_epsilon=0.3271, bad=True, gap_average=0.5380414047633472
    optimal_epsilon = 0.8139881377648227
    optimal_horizon = 2
    scaled_epsilon: 0.8140, forecast_average: 0.5102, episode_reward:-86.05, max_timestep_reward: 0.45, min_timestep_reward: -100.00
    scaled_epsilon: 0.8140, forecast_average: 0.5064, episode_reward:-5.67, max_timestep_reward: 0.45, min_timestep_reward: -100.00
    scaled_epsilon: 0.8140, forecast_average: 0.6705, episode_reward:-73.36, max_timestep_reward: 0.45, min_timestep_reward: -0.25
    scaled_epsilon: 0.8140, forecast_average: 0.6713, episode_reward:297.56, max_timestep_reward: 0.51, min_timestep_reward: -0.25
    scaled_epsilon: 0.8140, forecast_average: 0.6717, episode_reward:39.35, max_timestep_reward: 0.47, min_timestep_reward: -100.00
    scaled_epsilon: 0.8140, forecast_average: 0.7260, episode_reward:-44.37, max_timestep_reward: 0.45, min_timestep_reward: -0.25
    scaled_epsilon: 0.8140, forecast_average: 0.7640, episode_reward:-16.94, max_timestep_reward: 0.47, min_timestep_reward: -0.25
    scaled_epsilon: 0.8140, forecast_average: 0.7502, episode_reward:4.58, max_timestep_reward: 0.45, min_timestep_reward: -100.00
    scaled_epsilon: 0.8140, forecast_average: 0.7559, episode_reward:-71.97, max_timestep_reward: 0.45, min_timestep_reward: -100.00
    scaled_epsilon: 0.8140, forecast_average: 0.7387, episode_reward:149.92, max_timestep_reward: 0.47, min_timestep_reward: -100.00
no data found for: results/False/experiments.csv
no data found for: results/False/experiments.csv


-------------------------------------------------------

 Environment: AntBulletEnv-v0

-------------------------------------------------------




-----------------------------------------------------------------------------------------------------
 Agent Model Exists, loading: subrepos/rl-trained-agents/sac/AntBulletEnv-v0_1/AntBulletEnv-v0.zip
-----------------------------------------------------------------------------------------------------


pybullet build time: Oct 11 2021 20:52:37
Wrapping the env with a `Monitor` wrapper
Wrapping the env in a DummyVecEnv.


-----------------------------------------------------------------------------------------------------
 Training Coach Model from scrach
-----------------------------------------------------------------------------------------------------


coach settings = {
    "force_retrain": False, 
    "learning_rate": 0.0001, 
    "hidden_sizes": [64, 64, 64, 64, ], 
    "loss_function": "state_prediction_loss", 
    "consistent_coach_loss": {"scale_future_state_loss": 1000000, }, 
    "delayed_coach_loss": {"epochs_of_delay": 50, }, 
    "value_plus_state_loss": {"value_proportion": 0.5, }, 
    "number_of_episodes": 100, 
    "number_of_epochs": 100, 
    "train_test_split": 0.7, 
    "batch_size": 128, 
    "with_card": True, 
}
Starting Experience Recording
    Episode: 0, Reward: 3066.252, Average Reward: 3066.252
    Episode: 1, Reward: 3132.444, Average Reward: 3099.348
    Episode: 2, Reward: 3013.863, Average Reward: 3070.853
    Episode: 3, Reward: 3136.498, Average Reward: 3087.264
    Episode: 4, Reward: 3096.575, Average Reward: 3089.126
    Episode: 5, Reward: 3088.160, Average Reward: 3088.965
    Episode: 6, Reward: 3067.856, Average Reward: 3085.950
    Episode: 7, Reward: 3073.539, Average Reward: 3084.398
    Episode: 8, Reward: 3083.106, Average Reward: 3084.255
    Episode: 9, Reward: 3133.873, Average Reward: 3089.217
    Episode: 10, Reward: 3125.453, Average Reward: 3092.511
    Episode: 11, Reward: 3077.896, Average Reward: 3091.293
    Episode: 12, Reward: 3155.685, Average Reward: 3096.246
    Episode: 13, Reward: 3070.345, Average Reward: 3094.396
    Episode: 14, Reward: 3063.283, Average Reward: 3092.322
    Episode: 15, Reward: 3087.933, Average Reward: 3092.048
    Episode: 16, Reward: 3090.155, Average Reward: 3091.936
    Episode: 17, Reward: 2691.257, Average Reward: 3069.676
    Episode: 18, Reward: 3055.151, Average Reward: 3068.912
    Episode: 19, Reward: 3120.517, Average Reward: 3071.492
    Episode: 20, Reward: 3051.713, Average Reward: 3070.550
    Episode: 21, Reward: 3099.371, Average Reward: 3071.860
    Episode: 22, Reward: 3053.842, Average Reward: 3071.077
    Episode: 23, Reward: 3116.837, Average Reward: 3072.984
    Episode: 24, Reward: 3118.062, Average Reward: 3074.787
    Episode: 25, Reward: 3098.570, Average Reward: 3075.701
    Episode: 26, Reward: 3072.368, Average Reward: 3075.578
    Episode: 27, Reward: 3091.334, Average Reward: 3076.141
    Episode: 28, Reward: 3131.258, Average Reward: 3078.041
    Episode: 29, Reward: 3128.633, Average Reward: 3079.728
    Episode: 30, Reward: 3122.556, Average Reward: 3081.109
    Episode: 31, Reward: 3066.562, Average Reward: 3080.655
    Episode: 32, Reward: 3056.058, Average Reward: 3079.909
    Episode: 33, Reward: 3129.038, Average Reward: 3081.354
    Episode: 34, Reward: 3138.501, Average Reward: 3082.987
    Episode: 35, Reward: 3086.715, Average Reward: 3083.091
    Episode: 36, Reward: 3131.621, Average Reward: 3084.402
    Episode: 37, Reward: 3079.922, Average Reward: 3084.284
    Episode: 38, Reward: 3090.422, Average Reward: 3084.442
    Episode: 39, Reward: 3093.802, Average Reward: 3084.676
    Episode: 40, Reward: 3101.518, Average Reward: 3085.086
    Episode: 41, Reward: 3113.956, Average Reward: 3085.774
    Episode: 42, Reward: 3138.333, Average Reward: 3086.996
    Episode: 43, Reward: 3107.057, Average Reward: 3087.452
    Episode: 44, Reward: 3140.781, Average Reward: 3088.637
    Episode: 45, Reward: 3109.661, Average Reward: 3089.094
    Episode: 46, Reward: 3111.257, Average Reward: 3089.566
    Episode: 47, Reward: 3086.232, Average Reward: 3089.496
    Episode: 48, Reward: 3105.541, Average Reward: 3089.824
    Episode: 49, Reward: 3036.837, Average Reward: 3088.764
    Episode: 50, Reward: 3098.449, Average Reward: 3088.954
    Episode: 51, Reward: 3123.479, Average Reward: 3089.618
    Episode: 52, Reward: 3134.450, Average Reward: 3090.464
    Episode: 53, Reward: 3101.592, Average Reward: 3090.670
    Episode: 54, Reward: 3100.038, Average Reward: 3090.840
    Episode: 55, Reward: 3082.226, Average Reward: 3090.686
    Episode: 56, Reward: 3092.218, Average Reward: 3090.713
    Episode: 57, Reward: 3109.198, Average Reward: 3091.032
    Episode: 58, Reward: 3152.304, Average Reward: 3092.070
    Episode: 59, Reward: 3133.761, Average Reward: 3092.765
    Episode: 60, Reward: 3094.342, Average Reward: 3092.791
    Episode: 61, Reward: 3065.515, Average Reward: 3092.351
    Episode: 62, Reward: 3108.888, Average Reward: 3092.614
    Episode: 63, Reward: 3088.621, Average Reward: 3092.551
    Episode: 64, Reward: 3069.282, Average Reward: 3092.193
    Episode: 65, Reward: 3105.585, Average Reward: 3092.396
    Episode: 66, Reward: 2867.884, Average Reward: 3089.045
    Episode: 67, Reward: 3105.404, Average Reward: 3089.286
    Episode: 68, Reward: 3062.010, Average Reward: 3088.891
    Episode: 69, Reward: 3091.492, Average Reward: 3088.928
    Episode: 70, Reward: 3073.192, Average Reward: 3088.706
    Episode: 71, Reward: 3099.052, Average Reward: 3088.850
    Episode: 72, Reward: 3116.202, Average Reward: 3089.224
    Episode: 73, Reward: 3107.093, Average Reward: 3089.466
    Episode: 74, Reward: 3097.040, Average Reward: 3089.567
    Episode: 75, Reward: 3102.580, Average Reward: 3089.738
    Episode: 76, Reward: 3096.141, Average Reward: 3089.821
    Episode: 77, Reward: 3115.608, Average Reward: 3090.152
    Episode: 78, Reward: 3082.294, Average Reward: 3090.052
    Episode: 79, Reward: 3135.205, Average Reward: 3090.617
    Episode: 80, Reward: 3130.403, Average Reward: 3091.108
    Episode: 81, Reward: 3089.405, Average Reward: 3091.087
    Episode: 82, Reward: 3124.428, Average Reward: 3091.489
    Episode: 83, Reward: 3112.077, Average Reward: 3091.734
    Episode: 84, Reward: 3065.104, Average Reward: 3091.421
    Episode: 85, Reward: 3115.418, Average Reward: 3091.700
    Episode: 86, Reward: 3091.122, Average Reward: 3091.693
    Episode: 87, Reward: 3111.159, Average Reward: 3091.914
    Episode: 88, Reward: 3107.110, Average Reward: 3092.085
    Episode: 89, Reward: 3145.693, Average Reward: 3092.681
    Episode: 90, Reward: 3092.102, Average Reward: 3092.674
    Episode: 91, Reward: 3087.678, Average Reward: 3092.620
    Episode: 92, Reward: 3126.484, Average Reward: 3092.984
    Episode: 93, Reward: 3147.899, Average Reward: 3093.568
    Episode: 94, Reward: 3066.020, Average Reward: 3093.278
    Episode: 95, Reward: 3080.564, Average Reward: 3093.146
    Episode: 96, Reward: 3110.309, Average Reward: 3093.323
    Episode: 97, Reward: 3059.878, Average Reward: 3092.982
    Episode: 98, Reward: 3076.971, Average Reward: 3092.820
    Episode: 99, Reward: 3061.664, Average Reward: 3092.508

    Max Episode Reward: 3155.6847359099106
    Min Episode Reward: 2691.2574809845664
    Max Timestep Reward: 3.7966563530488315
    Min Timestep Reward: -0.9498896925681221
Starting Train/Test
    Epoch: 0
    Epoch: 1
    Epoch: 2
    Epoch: 3
    Epoch: 4
    Epoch: 5
    Epoch: 6
    Epoch: 7
    Epoch: 8
    Epoch: 9
    Epoch: 10
    Epoch: 11
    Epoch: 12
    Epoch: 13
    Epoch: 14
    Epoch: 15
    Epoch: 16
    Epoch: 17
    Epoch: 18
    Epoch: 19
    Epoch: 20
    Epoch: 21
    Epoch: 22
    Epoch: 23
    Epoch: 24
    Epoch: 25
    Epoch: 26
    Epoch: 27
    Epoch: 28
    Epoch: 29
    Epoch: 30
    Epoch: 31
    Epoch: 32
    Epoch: 33
    Epoch: 34
    Epoch: 35
    Epoch: 36
    Epoch: 37
    Epoch: 38
    Epoch: 39
    Epoch: 40
    Epoch: 41
    Epoch: 42
    Epoch: 43
    Epoch: 44
    Epoch: 45
    Epoch: 46
    Epoch: 47
    Epoch: 48
    Epoch: 49
    Epoch: 50
    Epoch: 51
    Epoch: 52
    Epoch: 53
    Epoch: 54
    Epoch: 55
    Epoch: 56
    Epoch: 57
    Epoch: 58
    Epoch: 59
    Epoch: 60
    Epoch: 61
    Epoch: 62
    Epoch: 63
    Epoch: 64
    Epoch: 65
    Epoch: 66
    Epoch: 67
    Epoch: 68
    Epoch: 69
    Epoch: 70
    Epoch: 71
    Epoch: 72
    Epoch: 73
    Epoch: 74
    Epoch: 75
    Epoch: 76
    Epoch: 77
    Epoch: 78
    Epoch: 79
    Epoch: 80
    Epoch: 81
    Epoch: 82
    Epoch: 83
    Epoch: 84
    Epoch: 85
    Epoch: 86
    Epoch: 87
    Epoch: 88
    Epoch: 89
    Epoch: 90
    Epoch: 91
    Epoch: 92
    Epoch: 93
    Epoch: 94
    Epoch: 95
    Epoch: 96
    Epoch: 97
    Epoch: 98
    Epoch: 99
Saving Coach to: models.ignore/coach/AntBulletEnv-v0/final_1_98%/


-----------------------------------------------------------------------------------------------------
 Testing Agent+Coach
-----------------------------------------------------------------------------------------------------


test settings = {
    "override_save_path": False, 
    "increment_factor": 1.2, 
    "force_recompute": True, 
    "graph_smoothing": 0, 
    "number_of_episodes_for_baseline": 30, 
    "number_of_epochs_for_optimal_parameters": 15, 
    "number_of_episodes_for_testing": 10, 
    "initial_epsilon": 3.5, 
    "initial_horizon": 10, 
    "reward_discount": 0.98, 
    "acceptable_performance_levels": [0.8, 0.9, 0.95, 0.98, 0.99, ], 
    "acceptable_performance_loss": 1, 
    "acceptable_performance_level": 0.8, 
    "confidence_interval_for_convergence": 90, 
    "min_reward_single_timestep": 19, 
    "max_reward_single_timestep": 105, 
    "horizons": {0: 1, 0.0007: 4, 0.0015: 16, 0.002: 26, }, 
}
----- getting a reward baseline -----------------------------------------------------------------------------------------------------------------------------------
  episode_index=0, episode_discounted_reward_sum=102.67101637487242
  episode_index=1, episode_discounted_reward_sum=103.15677676646196
  episode_index=2, episode_discounted_reward_sum=101.03948561978589
  episode_index=3, episode_discounted_reward_sum=102.94074312274601
  episode_index=4, episode_discounted_reward_sum=100.38806317523381
  episode_index=5, episode_discounted_reward_sum=103.72590067530653
  episode_index=6, episode_discounted_reward_sum=100.4324748519904
  episode_index=7, episode_discounted_reward_sum=103.5036918291085
  episode_index=8, episode_discounted_reward_sum=97.53114556698651
  episode_index=9, episode_discounted_reward_sum=102.887594141065
  episode_index=10, episode_discounted_reward_sum=102.4562771342351
  episode_index=11, episode_discounted_reward_sum=102.67484409382415
  episode_index=12, episode_discounted_reward_sum=102.57163714489839
  episode_index=13, episode_discounted_reward_sum=101.03750104140794
  episode_index=14, episode_discounted_reward_sum=103.8096666298767
  episode_index=15, episode_discounted_reward_sum=101.7144705224521
  episode_index=16, episode_discounted_reward_sum=102.76657407577265
  episode_index=17, episode_discounted_reward_sum=102.41303453432228
  episode_index=18, episode_discounted_reward_sum=101.96483277233068
  episode_index=19, episode_discounted_reward_sum=102.37414961003854
  episode_index=20, episode_discounted_reward_sum=102.95828161031223
  episode_index=21, episode_discounted_reward_sum=102.43713791219787
  episode_index=22, episode_discounted_reward_sum=101.83447354422144
  episode_index=23, episode_discounted_reward_sum=100.37232217949533
  episode_index=24, episode_discounted_reward_sum=103.03388624543358
  episode_index=25, episode_discounted_reward_sum=100.95445327962871
  episode_index=26, episode_discounted_reward_sum=100.34635502476262
  episode_index=27, episode_discounted_reward_sum=99.93437600774008
  episode_index=28, episode_discounted_reward_sum=100.98747851468512
  episode_index=29, episode_discounted_reward_sum=101.55375701091936
    baseline = {
    "max": 103.8096666298767, 
    "min": 97.53114556698651, 
    "range": 6.278521062890192, 
    "count": 30, 
    "sum": 3056.472401012112, 
    "average": 101.88241336707041, 
    "stdev": 1.3719041793954323, 
    "median": 102.39359207218041, 
}
    baseline_min = 101.45682573386377, baseline_max = 102.30800100027703,
    baseline_confidence_size = 0.4255876332066322
    ----- finding optimal epsilon -------------------------------------------------------------------------------------------------------------------------------------
            reward_single_sum=37.91264374777483, 
            reward_single_sum=42.844311530317555, confidence_size=15.568662466262047, confidence_max=55.947140105308236, new_horizon=20
        episode=0, horizon=20, effective_score=40.38, baseline_lowerbound=81.51 baseline_stdev=0.25, new_epsilon=2.9167, bad=True, gap_average=0.4829343709945679
            reward_single_sum=54.51904267054459, 
            reward_single_sum=56.57608232693816, confidence_size=6.4938186232802515, confidence_max=62.041381122021626, new_horizon=14
        episode=1, horizon=20, effective_score=55.55, baseline_lowerbound=81.51 baseline_stdev=0.25, new_epsilon=2.4306, bad=True, gap_average=0.16157280349731445
            reward_single_sum=67.66510038625344, 
            reward_single_sum=62.11412847046789, confidence_size=17.52372867095424, confidence_max=82.41334309931489, new_horizon=6
        episode=2, horizon=20, effective_score=64.89, baseline_lowerbound=81.51 baseline_stdev=0.25, new_epsilon=2.0255, bad=True, gap_average=0.84388880443573
            reward_single_sum=42.970912945750555, 
            reward_single_sum=57.12967298477512, confidence_size=44.69744632204715, confidence_max=94.74773928730997, new_horizon=6
        episode=3, horizon=20, effective_score=50.05, baseline_lowerbound=81.51 baseline_stdev=0.25, new_epsilon=1.6879, bad=True, gap_average=0.5787461395263672
            reward_single_sum=81.73034917147638, 
            reward_single_sum=56.0618275424786, confidence_size=81.03233365889263, confidence_max=149.92842201587007, new_horizon=4
            reward_single_sum=65.87800376961846, confidence_size=21.835201610117036, confidence_max=89.72526177130817, new_horizon=4
        episode=4, horizon=20, effective_score=67.89, baseline_lowerbound=81.51 baseline_stdev=0.25, new_epsilon=1.4066, bad=True, gap_average=0.7067663815816243
            reward_single_sum=45.35407863390526, 
            reward_single_sum=84.14520979527565, confidence_size=122.45878156547201, confidence_max=187.2084257800624, new_horizon=4
            reward_single_sum=50.96497356409201, confidence_size=35.34376940216107, confidence_max=95.4985233999187, new_horizon=4
        episode=5, horizon=20, effective_score=60.15, baseline_lowerbound=81.51 baseline_stdev=0.25, new_epsilon=1.1721, bad=True, gap_average=0.7588898369471232
            reward_single_sum=80.0830648507261, 
            reward_single_sum=70.38041818573193, confidence_size=30.63005003934261, confidence_max=105.8617915575716, new_horizon=10
            reward_single_sum=82.0162452575835, confidence_size=10.51174554571044, confidence_max=88.00498831039096, new_horizon=10
        episode=6, horizon=20, effective_score=77.49, baseline_lowerbound=81.51 baseline_stdev=0.25, new_epsilon=0.9768, bad=True, gap_average=0.6235418761571249
            reward_single_sum=87.2360582121901, 
            reward_single_sum=76.28616218665452, confidence_size=34.567461309069, confidence_max=116.32857150849131, new_horizon=4
            reward_single_sum=73.79977441173546, confidence_size=12.051522344405477, confidence_max=91.15885394793216, new_horizon=2
        episode=7, horizon=20, effective_score=79.11, baseline_lowerbound=81.51 baseline_stdev=0.25, new_epsilon=0.8140, bad=True, gap_average=0.8215192359288533
            reward_single_sum=82.27209236951096, 
            reward_single_sum=72.06286766934927, confidence_size=32.22925395779448, confidence_max=109.39673397722457, new_horizon=6
            reward_single_sum=80.61832488154737, confidence_size=9.237884173148068, confidence_max=87.55564581328393, new_horizon=6
        episode=8, horizon=20, effective_score=78.32, baseline_lowerbound=81.51 baseline_stdev=0.25, new_epsilon=0.6783, bad=True, gap_average=0.4640773140589396
            reward_single_sum=95.65057792586988, 
            reward_single_sum=77.66124802143408, confidence_size=56.79007946719268, confidence_max=143.44599244084463, new_horizon=8
            reward_single_sum=69.81166540271693, confidence_size=22.332336765459793, confidence_max=103.37350054880008, new_horizon=8
            reward_single_sum=82.39958550311749, confidence_size=12.752110942962922, confidence_max=94.13288015624751, new_horizon=8
        episode=9, horizon=20, effective_score=81.38, baseline_lowerbound=81.51 baseline_stdev=0.25, new_epsilon=0.5653, bad=True, gap_average=0.4190893020629883
            reward_single_sum=88.20098234693388, 
            reward_single_sum=96.98414279865376, confidence_size=27.727346303393034, confidence_max=120.31990887618684, new_horizon=8
            reward_single_sum=99.46780800912468, confidence_size=9.979676052403413, confidence_max=104.86398710397418, new_horizon=6
            reward_single_sum=94.97347494621492, confidence_size=5.6875891147174755, confidence_max=100.59419113994929, new_horizon=6
        episode=10, horizon=8, effective_score=94.91, baseline_lowerbound=81.51 baseline_stdev=0.25, new_epsilon=0.6783, bad=False, gap_average=0.3913097292780876
            reward_single_sum=82.28960578419826, 
            reward_single_sum=89.44656016221585, confidence_size=22.59361577278487, confidence_max=108.46169874599191, new_horizon=8
            reward_single_sum=70.63603787289982, confidence_size=16.00620029282748, confidence_max=96.79693489926545, new_horizon=8
        episode=11, horizon=6, effective_score=80.79, baseline_lowerbound=81.51 baseline_stdev=0.25, new_epsilon=0.5653, bad=True, gap_average=0.3875036408106486
            reward_single_sum=95.87960409533494, 
            reward_single_sum=78.26924397163889, confidence_size=55.59371895358797, confidence_max=142.66814298707484, new_horizon=6
            reward_single_sum=96.77404559461878, confidence_size=17.59212208781161, confidence_max=107.89975330834247, new_horizon=6
            reward_single_sum=96.16902087698985, confidence_size=10.602136925329575, confidence_max=102.37511555997519, new_horizon=8
            reward_single_sum=97.94107836017311, confidence_size=7.890532642285777, confidence_max=100.89713122203689, new_horizon=6
        episode=12, horizon=8, effective_score=93.01, baseline_lowerbound=81.51 baseline_stdev=0.25, new_epsilon=0.6783, bad=False, gap_average=0.3815523685216904
            reward_single_sum=81.0140911955953, 
            reward_single_sum=93.09488890170982, confidence_size=38.13757740849212, confidence_max=125.19206745714465, new_horizon=8
            reward_single_sum=95.59039733729504, confidence_size=13.14249320804418, confidence_max=103.04228568624423, new_horizon=8
            reward_single_sum=72.66389358607746, confidence_size=12.606694853061015, confidence_max=98.19751260823043, new_horizon=8
        episode=13, horizon=6, effective_score=85.59, baseline_lowerbound=81.51 baseline_stdev=0.25, new_epsilon=0.8140, bad=False, gap_average=0.4052620803117752
            reward_single_sum=87.61096923546897, 
            reward_single_sum=87.29519138346286, confidence_size=0.9968714457220784, confidence_max=88.449951755188, new_horizon=8
        episode=14, horizon=2, effective_score=87.45, baseline_lowerbound=81.51 baseline_stdev=0.25, new_epsilon=0.9768, bad=False, gap_average=0.42718244409561157
    optimal_epsilon = 0.9767857653177872
    optimal_horizon = 2
    scaled_epsilon: 0.9768, forecast_average: 0.8417, episode_reward:2727.95, max_timestep_reward: 3.48, min_timestep_reward: -0.44
    scaled_epsilon: 0.9768, forecast_average: 0.8362, episode_reward:2678.79, max_timestep_reward: 3.39, min_timestep_reward: -0.27
    scaled_epsilon: 0.9768, forecast_average: 0.6703, episode_reward:792.06, max_timestep_reward: 2.87, min_timestep_reward: -0.72
    scaled_epsilon: 0.9768, forecast_average: 0.7124, episode_reward:2792.37, max_timestep_reward: 3.47, min_timestep_reward: -0.53
    scaled_epsilon: 0.9768, forecast_average: 0.7303, episode_reward:2700.97, max_timestep_reward: 3.51, min_timestep_reward: -0.25
    scaled_epsilon: 0.9768, forecast_average: 0.7428, episode_reward:2682.92, max_timestep_reward: 3.46, min_timestep_reward: -0.26
    scaled_epsilon: 0.9768, forecast_average: 0.7584, episode_reward:2724.70, max_timestep_reward: 3.43, min_timestep_reward: -0.25
    scaled_epsilon: 0.9768, forecast_average: 0.7738, episode_reward:2803.46, max_timestep_reward: 3.45, min_timestep_reward: -0.28
    scaled_epsilon: 0.9768, forecast_average: 0.7828, episode_reward:2750.81, max_timestep_reward: 3.44, min_timestep_reward: -0.19
    scaled_epsilon: 0.9768, forecast_average: 0.7857, episode_reward:2563.12, max_timestep_reward: 3.41, min_timestep_reward: -0.33
no data found for: results/False/experiments.csv
no data found for: results/False/experiments.csv
/home/jeffhykin/repos/AFRL/.venv/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(
argv[0]=
argv[0]=
argv[0]=
argv[0]=


-------------------------------------------------------

 Environment: ReacherBulletEnv-v0

-------------------------------------------------------




-----------------------------------------------------------------------------------------------------
 Agent Model Exists, loading: ./subrepos/rl-trained-agents/sac/ReacherBulletEnv-v0_1/ReacherBulletEnv-v0.zip
-----------------------------------------------------------------------------------------------------


pybullet build time: Oct 11 2021 20:52:37
Wrapping the env with a `Monitor` wrapper
Wrapping the env in a DummyVecEnv.


-----------------------------------------------------------------------------------------------------
 Coach Model Exists, loading: models.ignore/coach/ReacherBulletEnv-v0/final_1_98%/
-----------------------------------------------------------------------------------------------------


coach settings = {
    "force_retrain": False, 
    "learning_rate": 0.0001, 
    "hidden_sizes": [64, 64, 64, 64, ], 
    "loss_function": "state_prediction_loss", 
    "consistent_coach_loss": {"scale_future_state_loss": 1000000, }, 
    "delayed_coach_loss": {"epochs_of_delay": 50, }, 
    "value_plus_state_loss": {"value_proportion": 0.5, }, 
    "number_of_episodes": 100, 
    "number_of_epochs": 100, 
    "train_test_split": 0.7, 
    "batch_size": 128, 
    "with_card": True, 
}


-----------------------------------------------------------------------------------------------------
 Testing Agent+Coach
-----------------------------------------------------------------------------------------------------


test settings = {
    "override_save_path": False, 
    "increment_factor": 1.2, 
    "force_recompute": True, 
    "graph_smoothing": 0, 
    "number_of_episodes_for_baseline": 30, 
    "number_of_epochs_for_optimal_parameters": 15, 
    "number_of_episodes_for_testing": 10, 
    "initial_epsilon": 3.5, 
    "initial_horizon": 10, 
    "reward_discount": 0.98, 
    "acceptable_performance_levels": [0.8, 0.9, 0.95, 0.98, 0.99, ], 
    "acceptable_performance_loss": 1, 
    "acceptable_performance_level": 0.8, 
    "confidence_interval_for_convergence": 90, 
    "min_reward_single_timestep": 0, 
    "max_reward_single_timestep": 100, 
}
----- getting a reward baseline -----------------------------------------------------------------------------------------------------------------------------------
argv[0]=
argv[0]=
  episode_index=0, episode_discounted_reward_sum=16.948507943032755
  episode_index=1, episode_discounted_reward_sum=0.6120949531949594
  episode_index=2, episode_discounted_reward_sum=15.619434030091748
  episode_index=3, episode_discounted_reward_sum=24.403183490418147
  episode_index=4, episode_discounted_reward_sum=12.904186016716718
  episode_index=5, episode_discounted_reward_sum=10.338033296950929
  episode_index=6, episode_discounted_reward_sum=22.61808409942587
  episode_index=7, episode_discounted_reward_sum=22.492455184308568
  episode_index=8, episode_discounted_reward_sum=15.110903241586342
  episode_index=9, episode_discounted_reward_sum=8.881862595962247
  episode_index=10, episode_discounted_reward_sum=3.915429192840222
  episode_index=11, episode_discounted_reward_sum=3.9331540438220736
  episode_index=12, episode_discounted_reward_sum=9.060701858646377
  episode_index=13, episode_discounted_reward_sum=22.433212963661518
  episode_index=14, episode_discounted_reward_sum=13.793149275758926
  episode_index=15, episode_discounted_reward_sum=18.94097413524585
  episode_index=16, episode_discounted_reward_sum=6.513916098593074
  episode_index=17, episode_discounted_reward_sum=13.268079301358595
  episode_index=18, episode_discounted_reward_sum=11.97474976509491
  episode_index=19, episode_discounted_reward_sum=18.16907162912576
  episode_index=20, episode_discounted_reward_sum=21.11875994992781
  episode_index=21, episode_discounted_reward_sum=19.690218037421303
  episode_index=22, episode_discounted_reward_sum=21.652903737519672
  episode_index=23, episode_discounted_reward_sum=9.200416666400185
  episode_index=24, episode_discounted_reward_sum=19.35610538992214
  episode_index=25, episode_discounted_reward_sum=10.940484019782131
  episode_index=26, episode_discounted_reward_sum=27.038339039418773
  episode_index=27, episode_discounted_reward_sum=12.635624245758834
  episode_index=28, episode_discounted_reward_sum=12.570807395959504
  episode_index=29, episode_discounted_reward_sum=12.312874099511275
    baseline = {
    "max": 27.038339039418773, 
    "min": 0.6120949531949594, 
    "range": 26.426244086223814, 
    "count": 30, 
    "sum": 438.4477156974572, 
    "average": 14.614923856581907, 
    "stdev": 6.582094819236027, 
    "median": 13.53061428855876, 
}
    baseline_min = 12.573047902173261, baseline_max = 16.656799810990552,
    baseline_confidence_size = 2.0418759544086456
    ----- finding optimal epsilon -------------------------------------------------------------------------------------------------------------------------------------
            reward_single_sum=5.663726407043921, 
            reward_single_sum=-1.8125943729407634, confidence_size=23.601815824883005, confidence_max=25.52738184193457, new_horizon=2
            reward_single_sum=-2.8311805469363622, confidence_size=7.819903785934697, confidence_max=8.159887614990293, new_horizon=2
        episode=0, horizon=20, effective_score=0.34, baseline_lowerbound=11.69 baseline_stdev=1.20, new_epsilon=2.9167, bad=True, gap_average=0.3932395890023973
            reward_single_sum=-0.761532029244444, 
            reward_single_sum=-1.7450230826894504, confidence_size=3.1047590642407883, confidence_max=1.8514815082738394, new_horizon=20
        episode=1, horizon=20, effective_score=-1.25, baseline_lowerbound=11.69 baseline_stdev=1.20, new_epsilon=2.4306, bad=True, gap_average=0.5755075450738271
            reward_single_sum=-0.704813089240352, 
            reward_single_sum=-5.294351748224967, confidence_size=14.488603330200785, confidence_max=11.489020911468119, new_horizon=11
        episode=2, horizon=20, effective_score=-3.00, baseline_lowerbound=11.69 baseline_stdev=1.20, new_epsilon=2.0255, bad=True, gap_average=0.5678273543715477
            reward_single_sum=4.611361924352258, 
            reward_single_sum=0.43142073504427514, confidence_size=13.195555007886052, confidence_max=15.71694633758431, new_horizon=20
            reward_single_sum=5.221235601615752, confidence_size=4.39542600976673, confidence_max=7.816765430104157, new_horizon=20
        episode=3, horizon=20, effective_score=3.42, baseline_lowerbound=11.69 baseline_stdev=1.20, new_epsilon=1.6879, bad=True, gap_average=0.3797523539596134
            reward_single_sum=-6.49396084311796, 
            reward_single_sum=-0.16906890089218093, confidence_size=19.96689804059012, confidence_max=16.63538316858504, new_horizon=14
            reward_single_sum=2.8902307637080007, confidence_size=8.06826017145685, confidence_max=6.810660511356134, new_horizon=14
        episode=4, horizon=20, effective_score=-1.26, baseline_lowerbound=11.69 baseline_stdev=1.20, new_epsilon=1.4066, bad=True, gap_average=0.47829448170132105
            reward_single_sum=-4.675826638125067, 
            reward_single_sum=0.43446687049694266, confidence_size=16.1325616905698, confidence_max=14.011881806755731, new_horizon=20
            reward_single_sum=8.235455357343477, confidence_size=10.961764924516828, confidence_max=12.29313012108861, new_horizon=22
        episode=5, horizon=20, effective_score=1.33, baseline_lowerbound=11.69 baseline_stdev=1.20, new_epsilon=1.1721, bad=True, gap_average=0.5144828258620369
            reward_single_sum=-1.4766595434056524, 
            reward_single_sum=-2.6598998817548605, confidence_size=3.7353427393129417, confidence_max=1.667063026732683, new_horizon=8
        episode=6, horizon=20, effective_score=-2.07, baseline_lowerbound=11.69 baseline_stdev=1.20, new_epsilon=0.9768, bad=True, gap_average=0.4706777060031891
            reward_single_sum=0.15134154013176868, 
            reward_single_sum=9.609648915631936, confidence_size=29.858701259758526, confidence_max=34.73919648764036, new_horizon=31
            reward_single_sum=4.25872319645188, confidence_size=7.995601148500594, confidence_max=12.668839032572453, new_horizon=8
            reward_single_sum=-2.1696269518782914, confidence_size=6.080387153496584, confidence_max=9.042908828580906, new_horizon=10
        episode=7, horizon=20, effective_score=2.96, baseline_lowerbound=11.69 baseline_stdev=1.20, new_epsilon=0.8140, bad=True, gap_average=0.3511061230798562
            reward_single_sum=-4.940627052273331, 
            reward_single_sum=2.032796635326504, confidence_size=22.014232185466092, confidence_max=20.560316976992663, new_horizon=11
            reward_single_sum=-5.627113431301104, confidence_size=7.144991301747567, confidence_max=4.300010018998254, new_horizon=18
        episode=8, horizon=20, effective_score=-2.84, baseline_lowerbound=11.69 baseline_stdev=1.20, new_epsilon=0.6783, bad=True, gap_average=0.30636242270469666
            reward_single_sum=1.85279121941729, 
            reward_single_sum=7.537467069680518, confidence_size=17.945815380375876, confidence_max=22.64094452492477, new_horizon=20
            reward_single_sum=1.586095795746052, confidence_size=5.667308718508739, confidence_max=9.32609341345669, new_horizon=20
        episode=9, horizon=20, effective_score=3.66, baseline_lowerbound=11.69 baseline_stdev=1.20, new_epsilon=0.5653, bad=True, gap_average=0.22837284922599793
            reward_single_sum=1.8567604200413856, 
            reward_single_sum=6.058706065692556, confidence_size=13.265020342670637, confidence_max=17.2227535855376, new_horizon=24
            reward_single_sum=3.91908808451784, confidence_size=3.542134127677043, confidence_max=7.486985651094303, new_horizon=21
        episode=10, horizon=20, effective_score=3.94, baseline_lowerbound=11.69 baseline_stdev=1.20, new_epsilon=0.4711, bad=True, gap_average=0.20409125506877898
            reward_single_sum=20.195425247150695, 
            reward_single_sum=15.7936545439354, confidence_size=13.895843222615973, confidence_max=31.890383118159015, new_horizon=8
            reward_single_sum=15.167108630378243, confidence_size=4.619575156681813, confidence_max=21.67163796383659, new_horizon=9
            reward_single_sum=18.1452001691258, confidence_size=2.7100815850050397, confidence_max=20.035428732652573, new_horizon=8
            reward_single_sum=3.007187526827204, confidence_size=6.394142747005635, confidence_max=20.8558579704891, new_horizon=9
            reward_single_sum=-4.260788581898329, confidence_size=7.993009375360805, confidence_max=19.334307297947305, new_horizon=10
            reward_single_sum=3.5129842543569803, confidence_size=6.867295751195656, confidence_max=17.09026314974937, new_horizon=10
            reward_single_sum=11.348542511685006, confidence_size=5.804627186354574, confidence_max=16.1682914740497, new_horizon=10
            reward_single_sum=9.08830972714266, confidence_size=5.031451548509724, confidence_max=15.253409773921241, new_horizon=10
            reward_single_sum=16.850855592239345, confidence_size=4.599704231820442, confidence_max=15.484552193914741, new_horizon=10
            reward_single_sum=-6.4312056854719, confidence_size=5.006309623991747, confidence_max=14.316970890852755, new_horizon=10
            reward_single_sum=11.100672406306868, confidence_size=4.536234405016028, confidence_max=13.996063266830857, new_horizon=10
            reward_single_sum=5.357615135244387, confidence_size=4.179151645502634, confidence_max=13.323425605273584, new_horizon=10
            reward_single_sum=3.9378449301063543, confidence_size=3.9004965084316123, confidence_max=12.672882680369376, new_horizon=10
            reward_single_sum=4.037367686977596, confidence_size=3.653980189107472, confidence_max=12.110698462047893, new_horizon=10
        episode=11, horizon=20, effective_score=8.46, baseline_lowerbound=11.69 baseline_stdev=1.20, new_epsilon=0.3925, bad=True, gap_average=0.21156545944346322
            reward_single_sum=20.67106992323361, 
            reward_single_sum=18.99836919475086, confidence_size=5.280508379133291, confidence_max=25.11522793812552, new_horizon=4
            reward_single_sum=14.812369726964686, confidence_size=5.087673366247261, confidence_max=23.248276314563643, new_horizon=4
            reward_single_sum=11.51992249130935, confidence_size=4.8653069015439465, confidence_max=21.36573973560857, new_horizon=6
            reward_single_sum=-5.0519798756382395, confidence_size=9.802950763923402, confidence_max=21.99290105604745, new_horizon=7
            reward_single_sum=16.034196538997133, confidence_size=7.674921557126186, confidence_max=20.50557955706242, new_horizon=8
            reward_single_sum=2.838761469228059, confidence_size=6.8425436909296575, confidence_max=18.24578790076472, new_horizon=9
            reward_single_sum=15.095087812521218, confidence_size=5.843382886547202, confidence_max=17.708107546718036, new_horizon=8
            reward_single_sum=21.395235661268728, confidence_size=5.4278822567884415, confidence_max=18.35155258374793, new_horizon=6
            reward_single_sum=22.702629768759273, confidence_size=5.110533936336519, confidence_max=19.012100207475985, new_horizon=6
            reward_single_sum=-0.6744552968404197, confidence_size=5.163158212064323, confidence_max=17.739631613387438, new_horizon=8
            reward_single_sum=4.766502926782356, confidence_size=4.814230207977431, confidence_max=16.739872736422146, new_horizon=6
            reward_single_sum=19.29733623761879, confidence_size=4.509625869099107, confidence_max=17.002321759787982, new_horizon=6
            reward_single_sum=14.89879684758338, confidence_size=4.159654464093628, confidence_max=16.824214708846398, new_horizon=6
            reward_single_sum=-0.5319929086863433, confidence_size=4.151417138713487, confidence_max=15.936207173236983, new_horizon=8
            reward_single_sum=3.72186111890971, confidence_size=3.964758085153245, confidence_max=15.245615062450879, new_horizon=8
            reward_single_sum=13.25816334233671, confidence_size=3.714571366057311, confidence_max=15.11174048247489, new_horizon=8
            reward_single_sum=15.104271353235664, confidence_size=3.507884054515573, confidence_max=15.111003295200824, new_horizon=8
            reward_single_sum=9.062029118785093, confidence_size=3.31567387991361, confidence_max=14.785051535235693, new_horizon=8
            reward_single_sum=17.246597197460385, confidence_size=3.1761004964133486, confidence_max=14.934339128842348, new_horizon=6
            reward_single_sum=23.259555500654578, confidence_size=3.1579437545842692, confidence_max=15.463864142643057, new_horizon=6
            reward_single_sum=23.65166515338135, confidence_size=3.132374020454307, confidence_max=15.954010079664123, new_horizon=6
            reward_single_sum=4.340922596623184, confidence_size=3.0531995143946506, confidence_max=15.50610890131809, new_horizon=6
            reward_single_sum=11.20544555360032, confidence_size=2.919004030651992, confidence_max=15.319935757853635, new_horizon=6
            reward_single_sum=20.625643889692462, confidence_size=2.8510396668860283, confidence_max=15.580959880587303, new_horizon=8
            reward_single_sum=16.441352897620053, confidence_size=2.7456497941811024, confidence_max=15.61831741880233, new_horizon=8
            reward_single_sum=9.001980952657822, confidence_size=2.6494079878383516, confidence_max=15.378716846831306, new_horizon=6
            reward_single_sum=21.283509699836326, confidence_size=2.6021086406943468, confidence_max=15.636924672574565, new_horizon=6
            reward_single_sum=5.879004244246446, confidence_size=2.5424874348240536, confidence_max=15.33055133609621, new_horizon=6
            reward_single_sum=10.467614295520407, confidence_size=2.4569003652681873, confidence_max=15.167615946348619, new_horizon=6
        hit cap of: 30 iterations
        episode=12, horizon=10, effective_score=12.71, baseline_lowerbound=11.69 baseline_stdev=1.20, new_epsilon=0.4711, bad=False, gap_average=0.17948082648714384
            reward_single_sum=4.085954955258069, 
            reward_single_sum=12.5233861520456, confidence_size=26.635921999872973, confidence_max=34.940592553524795, new_horizon=10
            reward_single_sum=16.227961375635903, confidence_size=10.4908029503162, confidence_max=21.43657044462939, new_horizon=10
            reward_single_sum=11.211873296818466, confidence_size=5.98068436826556, confidence_max=16.992978313205068, new_horizon=10
            reward_single_sum=6.247924343615006, confidence_size=4.662363149442912, confidence_max=14.721783174117519, new_horizon=10
            reward_single_sum=2.895947153310411, confidence_size=4.3284119384120325, confidence_max=13.193919817859275, new_horizon=10
            reward_single_sum=-0.06630911525864167, confidence_size=4.31188554237928, confidence_max=11.901419565439967, new_horizon=10
        episode=13, horizon=6, effective_score=7.59, baseline_lowerbound=11.69 baseline_stdev=1.20, new_epsilon=0.3925, bad=True, gap_average=0.19505352044389362
            reward_single_sum=16.86959232756867, 
            reward_single_sum=20.060743312256886, confidence_size=10.074067181766855, confidence_max=28.53923500167963, new_horizon=6
            reward_single_sum=15.962353048096382, confidence_size=3.6290492757618384, confidence_max=21.25994550506915, new_horizon=8
            reward_single_sum=9.19552387871256, confidence_size=5.376564532455005, confidence_max=20.898617674113627, new_horizon=8
            reward_single_sum=16.719098414004687, confidence_size=3.807025359360158, confidence_max=19.568487555487994, new_horizon=8
            reward_single_sum=7.030730224905637, confidence_size=4.150904133629109, confidence_max=18.457244334553245, new_horizon=8
            reward_single_sum=6.20922079063897, confidence_size=4.061679454893056, confidence_max=17.21128831149074, new_horizon=8
            reward_single_sum=3.7628098731905197, confidence_size=4.0869898680187, confidence_max=16.06324885169049, new_horizon=8
            reward_single_sum=17.25725882775051, confidence_size=3.702189183080197, confidence_max=16.265225927205176, new_horizon=8
            reward_single_sum=20.27321148482905, confidence_size=3.5571082379135577, confidence_max=16.891162456108944, new_horizon=8
            reward_single_sum=10.759912953293673, confidence_size=3.209426227018347, confidence_max=16.30946760294994, new_horizon=8
            reward_single_sum=21.431821567231612, confidence_size=3.1594579171751747, confidence_max=16.953814309048436, new_horizon=8
            reward_single_sum=18.850078795466846, confidence_size=2.966390578588322, confidence_max=17.14964869381494, new_horizon=8
            reward_single_sum=4.351151123243195, confidence_size=2.998906363804962, confidence_max=16.479871122461333, new_horizon=8
            reward_single_sum=-2.80365654845067, confidence_size=3.3713733047929804, confidence_max=15.76669664297555, new_horizon=8
            reward_single_sum=6.1706652392954116, confidence_size=3.2120808962455554, confidence_max=15.218363103247675, new_horizon=8
            reward_single_sum=11.051592089303762, confidence_size=3.00648897738337, confidence_max=14.95661294216794, new_horizon=8
            reward_single_sum=12.925810444348452, confidence_size=2.8259276410025613, confidence_max=14.830256410207348, new_horizon=8
            reward_single_sum=10.406553232377975, confidence_size=2.668529728594417, confidence_max=14.588765048492528, new_horizon=8
            reward_single_sum=14.134577123142813, confidence_size=2.5316400004021036, confidence_max=14.56259241046245, new_horizon=8
            reward_single_sum=18.602403878733465, confidence_size=2.4618115222322494, confidence_max=14.805690192705603, new_horizon=8
            reward_single_sum=3.505299857091704, confidence_size=2.4417426050951274, confidence_max=14.383867693142042, new_horizon=8
            reward_single_sum=9.612132357286646, confidence_size=2.33477584981369, confidence_max=14.175596906088419, new_horizon=8
            reward_single_sum=6.676611533336601, confidence_size=2.261389926656662, confidence_max=13.887035586142302, new_horizon=8
            reward_single_sum=7.197389071797261, confidence_size=2.1863700285090655, confidence_max=13.63488542448717, new_horizon=8
            reward_single_sum=13.204262211922009, confidence_size=2.1004003400908102, confidence_max=13.616444459759064, new_horizon=8
            reward_single_sum=17.083101515350332, confidence_size=2.0485383693903216, confidence_max=13.770769800009765, new_horizon=8
            reward_single_sum=4.158340999366407, confidence_size=2.0243113697224064, confidence_max=13.476403856368528, new_horizon=8
        episode=14, horizon=20, effective_score=11.45, baseline_lowerbound=11.69 baseline_stdev=1.20, new_epsilon=0.3271, bad=True, gap_average=0.1683655371587901
    optimal_epsilon = 0.8139881377648227
    optimal_horizon = 18
    scaled_epsilon: 0.8140, forecast_average: 7.8636, episode_reward:9.49, max_timestep_reward: 1.72, min_timestep_reward: -0.96
    scaled_epsilon: 0.8140, forecast_average: 8.6780, episode_reward:8.06, max_timestep_reward: 1.30, min_timestep_reward: -1.23
    scaled_epsilon: 0.8140, forecast_average: 7.3788, episode_reward:5.26, max_timestep_reward: 0.81, min_timestep_reward: -0.69
    scaled_epsilon: 0.8140, forecast_average: 7.4205, episode_reward:13.25, max_timestep_reward: 0.89, min_timestep_reward: -1.20
    scaled_epsilon: 0.8140, forecast_average: 7.4061, episode_reward:31.11, max_timestep_reward: 2.02, min_timestep_reward: -0.81
    scaled_epsilon: 0.8140, forecast_average: 6.5568, episode_reward:0.65, max_timestep_reward: 1.45, min_timestep_reward: -1.16
    scaled_epsilon: 0.8140, forecast_average: 6.7543, episode_reward:12.09, max_timestep_reward: 1.06, min_timestep_reward: -0.65
    scaled_epsilon: 0.8140, forecast_average: 6.9886, episode_reward:21.58, max_timestep_reward: 1.54, min_timestep_reward: -1.38
    scaled_epsilon: 0.8140, forecast_average: 7.3721, episode_reward:11.11, max_timestep_reward: 1.58, min_timestep_reward: -0.98
    scaled_epsilon: 0.8140, forecast_average: 7.6773, episode_reward:8.22, max_timestep_reward: 1.39, min_timestep_reward: -1.25
no data found for: results/False/experiments.csv
no data found for: results/False/experiments.csv
/home/jeffhykin/repos/AFRL/.venv/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(


-------------------------------------------------------

 Environment: LunarLanderContinuous-v2

-------------------------------------------------------




-----------------------------------------------------------------------------------------------------
 Agent Model Exists, loading: ./subrepos/rl-trained-agents/sac/LunarLanderContinuous-v2_1/LunarLanderContinuous-v2.zip
-----------------------------------------------------------------------------------------------------


Wrapping the env with a `Monitor` wrapper
Wrapping the env in a DummyVecEnv.


-----------------------------------------------------------------------------------------------------
 Coach Model Exists, loading: models.ignore/coach/LunarLanderContinuous-v2/final_1_99%/
-----------------------------------------------------------------------------------------------------


coach settings = {
    "force_retrain": False, 
    "learning_rate": 0.0001, 
    "hidden_sizes": [64, 64, 64, 64, ], 
    "loss_function": "state_prediction_loss", 
    "consistent_coach_loss": {"scale_future_state_loss": 1000000, }, 
    "delayed_coach_loss": {"epochs_of_delay": 50, }, 
    "value_plus_state_loss": {"value_proportion": 0.5, }, 
    "number_of_episodes": 100, 
    "number_of_epochs": 100, 
    "train_test_split": 0.7, 
    "batch_size": 128, 
    "with_card": True, 
}


-----------------------------------------------------------------------------------------------------
 Testing Agent+Coach
-----------------------------------------------------------------------------------------------------


test settings = {
    "override_save_path": False, 
    "increment_factor": 1.2, 
    "force_recompute": True, 
    "graph_smoothing": 0, 
    "number_of_episodes_for_baseline": 30, 
    "number_of_epochs_for_optimal_parameters": 15, 
    "number_of_episodes_for_testing": 10, 
    "initial_epsilon": 3.5, 
    "initial_horizon": 10, 
    "reward_discount": 0.98, 
    "acceptable_performance_levels": [0.8, 0.9, 0.95, 0.98, 0.99, ], 
    "acceptable_performance_loss": 1, 
    "acceptable_performance_level": 0.8, 
    "confidence_interval_for_convergence": 90, 
    "min_reward_single_timestep": -100, 
    "max_reward_single_timestep": 70, 
    "horizons": {0.0: 1, 0.005: 10, 0.01: 20, }, 
}
----- getting a reward baseline -----------------------------------------------------------------------------------------------------------------------------------
  episode_index=0, episode_discounted_reward_sum=17.55732115691156
  episode_index=1, episode_discounted_reward_sum=16.72968913982887
  episode_index=2, episode_discounted_reward_sum=50.018691407718606
  episode_index=3, episode_discounted_reward_sum=50.19764786917698
  episode_index=4, episode_discounted_reward_sum=23.402358005411845
  episode_index=5, episode_discounted_reward_sum=0.4547709543565124
  episode_index=6, episode_discounted_reward_sum=15.633074401370953
  episode_index=7, episode_discounted_reward_sum=30.210948341199675
  episode_index=8, episode_discounted_reward_sum=32.10748836620017
  episode_index=9, episode_discounted_reward_sum=17.43274024908214
  episode_index=10, episode_discounted_reward_sum=20.656661818393534
  episode_index=11, episode_discounted_reward_sum=48.42380829178708
  episode_index=12, episode_discounted_reward_sum=25.137253937163273
  episode_index=13, episode_discounted_reward_sum=40.97817977960896
  episode_index=14, episode_discounted_reward_sum=10.235613819531517
  episode_index=15, episode_discounted_reward_sum=23.417497799808004
  episode_index=16, episode_discounted_reward_sum=16.206850015372105
  episode_index=17, episode_discounted_reward_sum=20.45320357962721
  episode_index=18, episode_discounted_reward_sum=24.21563938911496
  episode_index=19, episode_discounted_reward_sum=22.32694898561452
  episode_index=20, episode_discounted_reward_sum=21.01773952556459
  episode_index=21, episode_discounted_reward_sum=27.610294963147112
  episode_index=22, episode_discounted_reward_sum=35.468677895998404
  episode_index=23, episode_discounted_reward_sum=41.70794681750561
  episode_index=24, episode_discounted_reward_sum=28.784033376313094
  episode_index=25, episode_discounted_reward_sum=14.555430671304215
  episode_index=26, episode_discounted_reward_sum=20.53454298502784
  episode_index=27, episode_discounted_reward_sum=17.86597974082547
  episode_index=28, episode_discounted_reward_sum=24.178198129279814
  episode_index=29, episode_discounted_reward_sum=20.81199497433067
    baseline = {
    "max": 50.19764786917698, 
    "min": 0.4547709543565124, 
    "range": 49.74287691482047, 
    "count": 30, 
    "sum": 758.3312263865754, 
    "average": 25.27770754621918, 
    "stdev": 11.719437384086557, 
    "median": 22.86465349551318, 
}
    baseline_min = 21.64214196247367, baseline_max = 28.91327312996468,
    baseline_confidence_size = 3.635565583745505
    ----- finding optimal epsilon -------------------------------------------------------------------------------------------------------------------------------------
            reward_single_sum=-2.781480439074807, 
            reward_single_sum=44.449542871908264, confidence_size=149.10247248765887, confidence_max=169.9365037040755, new_horizon=40
            reward_single_sum=9.19729774511982, confidence_size=41.39211295737862, confidence_max=58.347233016696364, new_horizon=19
            reward_single_sum=-1.5633688562183492, confidence_size=25.983637940580913, confidence_max=38.30913577101464, new_horizon=19
            reward_single_sum=31.984470396354894, confidence_size=20.06679120546357, confidence_max=36.32408354908153, new_horizon=19
            reward_single_sum=-3.040817626405401, confidence_size=16.788263659143105, confidence_max=29.829204341090506, new_horizon=20
            reward_single_sum=15.780615228137, confidence_size=13.703743641884916, confidence_max=27.136066401859402, new_horizon=18
            reward_single_sum=13.085440253571884, confidence_size=11.571251588754876, confidence_max=24.96021403542904, new_horizon=18
            reward_single_sum=30.17911613692202, confidence_size=10.599953643751308, confidence_max=25.85448872267523, new_horizon=16
            reward_single_sum=-3.796826119915165, confidence_size=9.977278919523865, confidence_max=23.326677878563878, new_horizon=11
            reward_single_sum=-7.082714844851911, confidence_size=9.537078522607757, confidence_max=21.02901259038487, new_horizon=11
        episode=0, horizon=20, effective_score=11.49, baseline_lowerbound=20.22 baseline_stdev=2.14, new_epsilon=2.9167, bad=True, gap_average=0.8136197760571281
            reward_single_sum=31.423670325437904, 
            reward_single_sum=17.719747753785015, confidence_size=43.26158094774408, confidence_max=67.83328998735551, new_horizon=9
            reward_single_sum=-2.272211794381642, confidence_size=28.567552177014527, confidence_max=44.191287605294946, new_horizon=8
            reward_single_sum=20.282537668336683, confidence_size=16.50956807498006, confidence_max=33.29800406327455, new_horizon=8
            reward_single_sum=7.805619749871746, confidence_size=12.201235917336295, confidence_max=27.193108657946233, new_horizon=8
            reward_single_sum=16.17519072698044, confidence_size=9.424842617121028, confidence_max=24.61393502212605, new_horizon=8
            reward_single_sum=-1.4378916923197593, confidence_size=8.961416139771206, confidence_max=21.77522510230126, new_horizon=6
            reward_single_sum=43.92391815839149, confidence_size=10.5610648610499, confidence_max=27.26363747306263, new_horizon=6
            reward_single_sum=11.02318836074097, confidence_size=9.21677477694358, confidence_max=25.28830469437056, new_horizon=6
            reward_single_sum=-48.69228228685523, confidence_size=14.386924688672803, confidence_max=23.982073385671562, new_horizon=6
            reward_single_sum=15.552843003137685, confidence_size=12.904245361177033, confidence_max=23.041002631461147, new_horizon=6
            reward_single_sum=52.20466645276872, confidence_size=13.261834300532154, confidence_max=26.904250669356653, new_horizon=8
            reward_single_sum=9.916440666066869, confidence_size=12.117521622180522, confidence_max=25.473324475408283, new_horizon=8
            reward_single_sum=54.832977441507175, confidence_size=12.320187950222873, confidence_max=28.638646131184878, new_horizon=8
            reward_single_sum=23.242131287368085, confidence_size=11.436079395586274, confidence_max=28.216115783642017, new_horizon=8
            reward_single_sum=20.83231374852988, confidence_size=10.656559363817912, confidence_max=27.689863086903287, new_horizon=8
            reward_single_sum=29.026126556439582, confidence_size=10.044966711725067, confidence_max=27.78373060147834, new_horizon=8
            reward_single_sum=18.632408075469478, confidence_size=9.4368315599432, confidence_max=27.225242348902924, new_horizon=8
            reward_single_sum=25.0227139037612, confidence_size=8.92236846679108, confidence_max=27.09153205126667, new_horizon=8
            reward_single_sum=23.393782906803658, confidence_size=8.452511361023143, confidence_max=26.88290591161514, new_horizon=8
            reward_single_sum=25.359727192824224, confidence_size=8.03958357188889, confidence_max=26.799946343539567, new_horizon=8
            reward_single_sum=0.3439065895602571, confidence_size=7.78224604681061, confidence_max=25.70549717382081, new_horizon=8
            reward_single_sum=49.36727912305032, confidence_size=7.783121799547077, confidence_max=27.07350457855902, new_horizon=8
            reward_single_sum=34.40633794357476, confidence_size=7.515493540464414, confidence_max=27.435707784666477, new_horizon=8
            reward_single_sum=0.38060036770454, confidence_size=7.3192221448336685, confidence_max=26.457851833975827, new_horizon=8
            reward_single_sum=14.254633579466677, confidence_size=7.028142152801047, confidence_max=25.97892583772492, new_horizon=8
            reward_single_sum=11.20858001818416, confidence_size=6.770532613256067, confidence_max=25.434568014226617, new_horizon=8
            reward_single_sum=22.20713454017392, confidence_size=6.518901910805199, confidence_max=25.3094765667473, new_horizon=8
            reward_single_sum=13.455468542492623, confidence_size=6.289919699913575, confidence_max=24.89652517952983, new_horizon=8
            reward_single_sum=-14.128678523978829, confidence_size=6.346345087652244, confidence_max=23.861774433815327, new_horizon=8
        hit cap of: 30 iterations
        episode=1, horizon=20, effective_score=17.52, baseline_lowerbound=20.22 baseline_stdev=2.14, new_epsilon=2.4306, bad=True, gap_average=1.048603993650259
            reward_single_sum=7.195325051802853, 
            reward_single_sum=11.50690981620311, confidence_size=13.611137418712381, confidence_max=22.962254852715354, new_horizon=8
            reward_single_sum=-7.053635475479095, confidence_size=16.375604974482073, confidence_max=20.258471438657693, new_horizon=10
        episode=2, horizon=20, effective_score=3.88, baseline_lowerbound=20.22 baseline_stdev=2.14, new_epsilon=2.0255, bad=True, gap_average=0.23697294010835535
            reward_single_sum=23.775964401880003, 
            reward_single_sum=18.598460380121388, confidence_size=16.344736930133195, confidence_max=37.53194932113388, new_horizon=14
            reward_single_sum=38.134811835676665, confidence_size=17.06314693234787, confidence_max=43.89955913824055, new_horizon=14
            reward_single_sum=16.28402257266951, confidence_size=11.53705712305294, confidence_max=35.73537192063983, new_horizon=6
            reward_single_sum=5.981953305758595, confidence_size=11.218734878702953, confidence_max=31.773777377924183, new_horizon=6
            reward_single_sum=28.80090969318808, confidence_size=9.090297804225317, confidence_max=31.01965150244102, new_horizon=6
            reward_single_sum=-50.34374954185903, confidence_size=21.387031116214207, confidence_max=32.99165578013352, new_horizon=6
            reward_single_sum=41.50339257664771, confidence_size=19.397014283301946, confidence_max=34.738984936312306, new_horizon=6
            reward_single_sum=36.06302518918398, confidence_size=17.32750576093256, confidence_max=34.9718158068511, new_horizon=6
            reward_single_sum=34.26015092315569, confidence_size=15.57853337970137, confidence_max=34.884427513343624, new_horizon=6
            reward_single_sum=20.112170112143158, confidence_size=13.933194566242786, confidence_max=33.3123865161124, new_horizon=6
            reward_single_sum=30.119544667962906, confidence_size=12.704970027649802, confidence_max=32.979191370693854, new_horizon=6
            reward_single_sum=36.63215693640556, confidence_size=11.81321716327675, confidence_max=33.345741244271686, new_horizon=8
            reward_single_sum=29.895813425423142, confidence_size=10.918611879853955, confidence_max=33.04851377116519, new_horizon=8
            reward_single_sum=4.203608980969372, confidence_size=10.326250349843066, confidence_max=31.261066047131514, new_horizon=6
            reward_single_sum=-6.902471234647489, confidence_size=10.086229920268497, confidence_max=29.281215184310945, new_horizon=6
            reward_single_sum=20.121206474799315, confidence_size=9.436110400307172, confidence_max=28.68557926498238, new_horizon=10
            reward_single_sum=-9.387440551362587, confidence_size=9.28646312838935, confidence_max=26.944992581062458, new_horizon=10
            reward_single_sum=38.548842154320425, confidence_size=8.961294081854934, confidence_max=27.719313676720006, new_horizon=10
            reward_single_sum=18.57991760550182, confidence_size=8.477270265061104, confidence_max=27.22638476045801, new_horizon=10
            reward_single_sum=-1.3186124820335454, confidence_size=8.210038829853747, confidence_max=26.00354727870635, new_horizon=10
            reward_single_sum=49.74781477111072, confidence_size=8.2000918724692, confidence_max=27.44606879051535, new_horizon=10
            reward_single_sum=19.74097446282197, confidence_size=7.8191604072864624, confidence_max=27.08665895771417, new_horizon=10
            reward_single_sum=16.41846267765814, confidence_size=7.474776801899376, confidence_max=26.62356552429502, new_horizon=10
            reward_single_sum=19.896203272323536, confidence_size=7.1572312076018445, confidence_max=26.335916511994604, new_horizon=10
            reward_single_sum=25.36052815839088, confidence_size=6.877429463713172, confidence_max=26.293877954798166, new_horizon=10
            reward_single_sum=6.894208683029147, confidence_size=6.655214544905562, confidence_max=25.607876376432934, new_horizon=10
            reward_single_sum=13.404666579614473, confidence_size=6.413252764100802, confidence_max=25.167771908059855, new_horizon=10
            reward_single_sum=22.509560488903887, confidence_size=6.184241444512314, confidence_max=25.06824477277981, new_horizon=10
            reward_single_sum=32.85747770770797, confidence_size=6.01975830994275, confidence_max=25.369544117524928, new_horizon=10
        hit cap of: 30 iterations
        episode=3, horizon=20, effective_score=19.35, baseline_lowerbound=20.22 baseline_stdev=2.14, new_epsilon=1.6879, bad=True, gap_average=0.7425340738894405
            reward_single_sum=12.725944595065029, 
            reward_single_sum=38.56032573930159, confidence_size=81.55593154168416, confidence_max=107.19906670886743, new_horizon=8
            reward_single_sum=2.6698210477300144, confidence_size=31.212364935364256, confidence_max=49.19772872939646, new_horizon=10
            reward_single_sum=5.998369152790209, confidence_size=19.134769935189308, confidence_max=34.12338506891101, new_horizon=10
            reward_single_sum=35.45430421142368, confidence_size=16.012979179716837, confidence_max=35.09473212897893, new_horizon=9
            reward_single_sum=19.874079023390816, confidence_size=12.361086582426488, confidence_max=31.574893877376706, new_horizon=10
            reward_single_sum=9.574070113668789, confidence_size=10.423761935026917, confidence_max=28.260463918365502, new_horizon=10
            reward_single_sum=19.11253113077916, confidence_size=8.80664321772636, confidence_max=26.802823844495023, new_horizon=12
            reward_single_sum=28.22479703173488, confidence_size=7.910655417396869, confidence_max=27.043348978050663, new_horizon=12
            reward_single_sum=17.370745813095915, confidence_size=6.982394959223198, confidence_max=25.938893745121206, new_horizon=12
            reward_single_sum=28.328617536033423, confidence_size=6.432763436481094, confidence_max=26.241273017845955, new_horizon=12
            reward_single_sum=42.72549952492487, confidence_size=6.754154346723647, confidence_max=28.472413090051845, new_horizon=10
            reward_single_sum=-8.232892192633983, confidence_size=7.408072718396058, confidence_max=26.822396774342547, new_horizon=12
            reward_single_sum=24.21545061780335, confidence_size=6.841857672331786, confidence_max=26.59911933983948, new_horizon=16
            reward_single_sum=-5.66964377872176, confidence_size=7.003135575484226, confidence_max=25.06527021324329, new_horizon=14
            reward_single_sum=23.488704336086634, confidence_size=6.5471658609470165, confidence_max=24.948461104851553, new_horizon=14
            reward_single_sum=32.15080021825258, confidence_size=6.285514947508561, confidence_max=25.495604601668866, new_horizon=14
            reward_single_sum=20.40044803703932, confidence_size=5.905855092159969, confidence_max=25.18207576759133, new_horizon=14
            reward_single_sum=11.986120313304294, confidence_size=5.608186254574576, confidence_max=24.50071743726241, new_horizon=14
            reward_single_sum=7.29952019855781, confidence_size=5.399112744670534, confidence_max=23.711993378151863, new_horizon=14
            reward_single_sum=3.435909636997275, confidence_size=5.2661728959393805, confidence_max=22.870626339111947, new_horizon=14
            reward_single_sum=34.71614784488702, confidence_size=5.18523525096891, confidence_max=23.567492985128496, new_horizon=14
            reward_single_sum=46.18532718978571, confidence_size=5.362350004246435, confidence_max=24.953436845172373, new_horizon=14
            reward_single_sum=-10.71373873843136, confidence_size=5.562512304820112, confidence_max=23.890898079939497, new_horizon=14
            reward_single_sum=6.967504018093776, confidence_size=5.382516526091142, confidence_max=23.256467030929503, new_horizon=14
            reward_single_sum=57.61592605788558, confidence_size=5.785704398312307, confidence_max=25.188192424421715, new_horizon=14
            reward_single_sum=37.086926125191546, confidence_size=5.670212982426964, confidence_max=25.727680197391265, new_horizon=14
            reward_single_sum=36.50178909170584, confidence_size=5.547429347296161, confidence_max=26.192193772144087, new_horizon=14
            reward_single_sum=5.04530205965828, confidence_size=5.423691077163161, confidence_max=25.53054300665972, new_horizon=14
            reward_single_sum=23.84622339389342, confidence_size=5.237894479576537, confidence_max=25.469392124552993, new_horizon=14
        hit cap of: 30 iterations
        episode=4, horizon=10, effective_score=20.23, baseline_lowerbound=20.22 baseline_stdev=2.14, new_epsilon=2.0255, bad=False, gap_average=0.5917040457744372
            reward_single_sum=13.931272511123806, 
            reward_single_sum=40.94890173304618, confidence_size=85.29129871312121, confidence_max=112.73138583520615, new_horizon=10
            reward_single_sum=12.578312800705712, confidence_size=26.979580920323237, confidence_max=49.46574326861513, new_horizon=12
            reward_single_sum=3.115191438602289, confidence_size=19.138723055684466, confidence_max=36.78214267655396, new_horizon=10
            reward_single_sum=39.35341125020679, confidence_size=16.310433877106444, confidence_max=38.29585182384339, new_horizon=10
            reward_single_sum=3.7796556445675087, confidence_size=13.994156213616932, confidence_max=32.94528044332564, new_horizon=10
            reward_single_sum=36.94723681942965, confidence_size=12.451495809728529, confidence_max=33.97349326654023, new_horizon=10
            reward_single_sum=5.6067220478777, confidence_size=11.168794341429603, confidence_max=30.701382372124556, new_horizon=10
            reward_single_sum=-14.229765184091379, confidence_size=11.921805684320548, confidence_max=27.703021135594796, new_horizon=10
            reward_single_sum=41.13101388930032, confidence_size=11.492931197146794, confidence_max=29.809126492223648, new_horizon=10
            reward_single_sum=11.332320880912674, confidence_size=10.342842487765353, confidence_max=28.02414010882728, new_horizon=10
            reward_single_sum=17.36786521864309, confidence_size=9.35544664415983, confidence_max=27.01062489835352, new_horizon=10
            reward_single_sum=24.373568147327685, confidence_size=8.59012696399683, confidence_max=26.762104440739293, new_horizon=10
            reward_single_sum=10.504158815259789, confidence_size=7.961551859764588, confidence_max=25.585828003544, new_horizon=10
            reward_single_sum=12.192024353429378, confidence_size=7.399071170994905, confidence_max=24.66119719541765, new_horizon=10
            reward_single_sum=8.984938911286413, confidence_size=6.948179387709193, confidence_max=23.692981217560916, new_horizon=10
            reward_single_sum=43.62162903010556, confidence_size=7.061783725780472, confidence_max=25.387575390941247, new_horizon=10
            reward_single_sum=12.237211183053315, confidence_size=6.660022002086113, confidence_max=24.647559196018694, new_horizon=10
            reward_single_sum=25.29504883462669, confidence_size=6.314992766021751, confidence_max=24.687135835780335, new_horizon=10
            reward_single_sum=12.763021763093223, confidence_size=5.993544037021227, confidence_max=24.085231041446548, new_horizon=10
            reward_single_sum=6.686848850704496, confidence_size=5.763070846698266, confidence_max=23.311670319994022, new_horizon=10
            reward_single_sum=25.22115860513365, confidence_size=5.514955328335, confidence_max=23.41230748944157, new_horizon=10
            reward_single_sum=19.248396431516767, confidence_size=5.259669172746291, confidence_max=23.215762389088084, new_horizon=10
            reward_single_sum=11.822935538472988, confidence_size=5.045198269288954, confidence_max=22.745743249052882, new_horizon=10
            reward_single_sum=30.023474268432313, confidence_size=4.90380212840179, confidence_max=23.097264279712455, new_horizon=10
            reward_single_sum=20.040506192416704, confidence_size=4.705436594617547, confidence_max=22.969938901355363, new_horizon=10
            reward_single_sum=-9.834613776520392, confidence_size=4.857091666736638, confidence_max=22.080885970390817, new_horizon=10
            reward_single_sum=13.585559506878372, confidence_size=4.679255246066238, confidence_max=21.773112592692712, new_horizon=10
            reward_single_sum=20.262517619144898, confidence_size=4.513129518011913, confidence_max=21.71625101196661, new_horizon=10
            reward_single_sum=32.39008426857738, confidence_size=4.439093342155962, confidence_max=22.14844692859808, new_horizon=10
        hit cap of: 30 iterations
        episode=5, horizon=14, effective_score=17.71, baseline_lowerbound=20.22 baseline_stdev=2.14, new_epsilon=1.6879, bad=True, gap_average=0.7906676997601146
            reward_single_sum=14.510043069107482, 
            reward_single_sum=11.360090229067133, confidence_size=9.944009757678131, confidence_max=22.879076406765435, new_horizon=14
            reward_single_sum=34.124335486240454, confidence_size=20.794333474647708, confidence_max=40.79248973611939, new_horizon=14
            reward_single_sum=22.13604968878614, confidence_size=11.917109057155491, confidence_max=32.44973867545579, new_horizon=12
            reward_single_sum=23.24904261033953, confidence_size=8.441892146750927, confidence_max=29.51780436345907, new_horizon=12
            reward_single_sum=9.236431336270023, confidence_size=7.6326338822854165, confidence_max=26.735299285587207, new_horizon=12
            reward_single_sum=6.41378770027495, confidence_size=7.1487179649637955, confidence_max=24.438686553547466, new_horizon=12
            reward_single_sum=42.340471694456625, confidence_size=8.463427032226509, confidence_max=28.8847085090443, new_horizon=12
            reward_single_sum=-5.121539720983227, confidence_size=9.029033933664197, confidence_max=26.612224166281873, new_horizon=12
            reward_single_sum=21.884420534022553, confidence_size=7.999958439866711, confidence_max=26.013271702624877, new_horizon=12
            reward_single_sum=19.896897486540237, confidence_size=7.1614395699654185, confidence_max=25.345987762158316, new_horizon=12
            reward_single_sum=8.204535682004174, confidence_size=6.64763997872811, confidence_max=24.000520461738613, new_horizon=8
            reward_single_sum=11.236570300842217, confidence_size=6.126300221758621, confidence_max=23.008695306140794, new_horizon=8
            reward_single_sum=29.393561414486506, confidence_size=5.853713474350133, confidence_max=23.629763296596906, new_horizon=10
            reward_single_sum=20.743251593463494, confidence_size=5.4310850226370615, confidence_max=23.40494829629828, new_horizon=12
            reward_single_sum=24.688771260682117, confidence_size=5.109734201937574, confidence_max=23.5032792247876, new_horizon=12
            reward_single_sum=46.0100696056086, confidence_size=5.558209392254655, confidence_max=25.5762558611493, new_horizon=12
            reward_single_sum=37.99996546833473, confidence_size=5.503099892600372, confidence_max=26.520141861463912, new_horizon=12
            reward_single_sum=13.972408977685062, confidence_size=5.2285057878223515, confidence_max=25.874777599255445, new_horizon=12
            reward_single_sum=10.713418549374037, confidence_size=5.0200891576892275, confidence_max=25.16971830601937, new_horizon=12
            reward_single_sum=19.87359705602616, confidence_size=4.762920232905408, confidence_max=24.899404995887743, new_horizon=12
            reward_single_sum=50.51798058909382, confidence_size=5.11614817894522, confidence_max=26.633610024932622, new_horizon=12
            reward_single_sum=51.480087938832156, confidence_size=5.366846278880049, confidence_max=28.187030998469393, new_horizon=12
            reward_single_sum=17.656198463961466, confidence_size=5.141811383103546, confidence_max=27.746830008708397, new_horizon=12
            reward_single_sum=28.629885390264764, confidence_size=4.940484855308421, confidence_max=27.786498151499668, new_horizon=12
            reward_single_sum=13.228091381714103, confidence_size=4.780998338908423, confidence_max=27.257091561465934, new_horizon=12
            reward_single_sum=47.61271407289907, confidence_size=4.860425702970428, confidence_max=28.267504882947996, new_horizon=12
            reward_single_sum=17.529967658005596, confidence_size=4.6908705154296655, confidence_max=27.888052855336806, new_horizon=12
            reward_single_sum=25.132556363897418, confidence_size=4.521919077609466, confidence_max=27.785838452826614, new_horizon=12
            reward_single_sum=8.867753036226512, confidence_size=4.438969548697505, confidence_max=27.223016712614967, new_horizon=12
        hit cap of: 30 iterations
        episode=6, horizon=10, effective_score=22.78, baseline_lowerbound=20.22 baseline_stdev=2.14, new_epsilon=2.0255, bad=False, gap_average=0.6898425264850724
            reward_single_sum=0.8361306828428763, 
            reward_single_sum=39.75495583789069, confidence_size=122.86189563847793, confidence_max=143.15743889884465, new_horizon=10
            reward_single_sum=5.162060578949585, confidence_size=35.96088021420799, confidence_max=51.21192924743569, new_horizon=10
            reward_single_sum=22.999315445765575, confidence_size=20.994738149403993, confidence_max=38.18285378576617, new_horizon=10
            reward_single_sum=28.941417444965467, confidence_size=15.560713246609037, confidence_max=35.09948924469187, new_horizon=10
            reward_single_sum=4.207747830865626, confidence_size=13.066387615490257, confidence_max=30.049992252370224, new_horizon=10
            reward_single_sum=18.269606786878875, confidence_size=10.655234774798073, confidence_max=27.82255400453503, new_horizon=10
            reward_single_sum=10.584884701163014, confidence_size=9.130957648603413, confidence_max=25.475472562268628, new_horizon=10
            reward_single_sum=31.881870690547576, confidence_size=8.530929043580736, confidence_max=26.601816821343988, new_horizon=10
            reward_single_sum=36.15763444366454, confidence_size=8.220121820738175, confidence_max=28.099684265091557, new_horizon=10
            reward_single_sum=0.6580237132487312, confidence_size=8.0048015762565, confidence_max=26.136951408691274, new_horizon=10
            reward_single_sum=18.618754749933807, confidence_size=7.240885715284635, confidence_max=25.413585957510996, new_horizon=10
            reward_single_sum=55.900989241348086, confidence_size=8.393440290455004, confidence_max=29.468316609536885, new_horizon=10
            reward_single_sum=1.6166750356351063, confidence_size=8.104134148517941, confidence_max=27.78913894735362, new_horizon=10
            reward_single_sum=10.722003037196979, confidence_size=7.576989169903198, confidence_max=26.664460517962965, new_horizon=10
            reward_single_sum=12.188388888937551, confidence_size=7.094769712401069, confidence_max=25.751048406765698, new_horizon=10
            reward_single_sum=26.77074099884065, confidence_size=6.689243237704315, confidence_max=25.822843244096944, new_horizon=10
            reward_single_sum=51.775469523054994, confidence_size=7.03140400655909, confidence_max=27.97844120832185, new_horizon=10
            reward_single_sum=0.8077764095869409, confidence_size=6.879919537010833, confidence_max=26.76699564444855, new_horizon=8
            reward_single_sum=24.50611403653485, confidence_size=6.520546016509103, confidence_max=26.638574020401677, new_horizon=8
            reward_single_sum=26.019713613532108, confidence_size=6.205400606418236, confidence_max=26.604461258388884, new_horizon=8
            reward_single_sum=7.977261194979776, confidence_size=5.982402295674115, confidence_max=25.81683569959972, new_horizon=8
            reward_single_sum=26.114251714726056, confidence_size=5.723663245754983, confidence_max=25.831132228411043, new_horizon=8
            reward_single_sum=19.23634076704314, confidence_size=5.469899338301403, confidence_max=25.541071311973596, new_horizon=8
            reward_single_sum=22.45584627668032, confidence_size=5.239933723300711, confidence_max=25.406492669093225, new_horizon=10
            reward_single_sum=9.430336905030817, confidence_size=5.0755486042655615, confidence_max=24.829175933105702, new_horizon=10
            reward_single_sum=34.78282061800024, confidence_size=4.968292313317975, confidence_max=25.27855643064553, new_horizon=10
            reward_single_sum=16.115110823487374, confidence_size=4.787833633928578, confidence_max=24.9482708479047, new_horizon=10
            reward_single_sum=37.48330405108043, confidence_size=4.7245068996254656, confidence_max=25.482284349363802, new_horizon=10
            reward_single_sum=-12.16185070544921, confidence_size=4.925460640660965, confidence_max=24.58591715189305, new_horizon=10
        hit cap of: 30 iterations
        episode=7, horizon=12, effective_score=19.66, baseline_lowerbound=20.22 baseline_stdev=2.14, new_epsilon=1.6879, bad=True, gap_average=0.8089685360958822
            reward_single_sum=17.76453302870689, 
            reward_single_sum=-8.728532107894562, confidence_size=83.63531506896865, confidence_max=88.15331552937477, new_horizon=12
            reward_single_sum=5.457682417680099, confidence_size=22.3504477686823, confidence_max=27.18167554817977, new_horizon=12
            reward_single_sum=32.4761033477913, confidence_size=20.658604706781723, confidence_max=32.40105137835265, new_horizon=12
            reward_single_sum=19.26242956984394, confidence_size=14.846207552231022, confidence_max=28.09265080345655, new_horizon=12
            reward_single_sum=20.59167817202197, confidence_size=11.720295709015236, confidence_max=26.190944780373506, new_horizon=12
            reward_single_sum=10.110938722170383, confidence_size=9.628535396749832, confidence_max=23.476368703938405, new_horizon=12
            reward_single_sum=9.61067604384339, confidence_size=8.191689300580896, confidence_max=21.509877949851322, new_horizon=12
        episode=8, horizon=20, effective_score=13.32, baseline_lowerbound=20.22 baseline_stdev=2.14, new_epsilon=1.4066, bad=True, gap_average=0.7381186723387354
            reward_single_sum=20.77397613754158, 
            reward_single_sum=21.948485620339103, confidence_size=3.707780513080456, confidence_max=25.069011392020794, new_horizon=18
            reward_single_sum=26.00128446367058, confidence_size=4.623536069511873, confidence_max=27.531451476695626, new_horizon=18
            reward_single_sum=19.37841744176182, confidence_size=3.35482608254169, confidence_max=25.380366998369958, new_horizon=18
        episode=9, horizon=12, effective_score=22.03, baseline_lowerbound=20.22 baseline_stdev=2.14, new_epsilon=1.6879, bad=False, gap_average=0.44140121578115277
            reward_single_sum=20.10947364488854, 
            reward_single_sum=4.503641738729325, confidence_size=49.26567241862075, confidence_max=61.572230110429665, new_horizon=12
            reward_single_sum=7.531493706586229, confidence_size=13.951493236742426, confidence_max=24.666362933477117, new_horizon=12
            reward_single_sum=49.51545055963181, confidence_size=24.172964670709476, confidence_max=44.58797958316845, new_horizon=12
            reward_single_sum=7.889975363172519, confidence_size=17.782636825537296, confidence_max=35.692643828138976, new_horizon=12
            reward_single_sum=19.228005127421024, confidence_size=13.731114467934113, confidence_max=31.86078782467235, new_horizon=12
            reward_single_sum=32.87010726950028, confidence_size=11.915638236758546, confidence_max=32.151087866748504, new_horizon=12
            reward_single_sum=28.350122326326012, confidence_size=10.243033630310869, confidence_max=31.492817347342836, new_horizon=8
            reward_single_sum=25.004256347443366, confidence_size=8.900348130742257, confidence_max=30.567295473375488, new_horizon=8
            reward_single_sum=20.490564759489523, confidence_size=7.8505071664317345, confidence_max=29.399816250750597, new_horizon=8
            reward_single_sum=14.539899066120775, confidence_size=7.115407678577013, confidence_max=28.027497670332412, new_horizon=8
            reward_single_sum=48.62244300989536, confidence_size=7.65641405831243, confidence_max=30.877700134912825, new_horizon=8
            reward_single_sum=34.38105324153403, confidence_size=7.155048884834816, confidence_max=31.23477858950703, new_horizon=8
            reward_single_sum=51.947314421271216, confidence_size=7.466614184365135, confidence_max=33.53688565450871, new_horizon=8
            reward_single_sum=15.687760730344314, confidence_size=7.019937685554089, confidence_max=32.398041773044376, new_horizon=8
            reward_single_sum=22.653439384337375, confidence_size=6.542570476367299, confidence_max=31.75038301991053, new_horizon=8
            reward_single_sum=18.830524478865904, confidence_size=6.1554924774821576, confidence_max=30.988170428985544, new_horizon=8
            reward_single_sum=27.568056381932553, confidence_size=5.788628555561056, confidence_max=30.77327197542162, new_horizon=8
            reward_single_sum=11.459635751778464, confidence_size=5.595889015073611, confidence_max=29.868689926087747, new_horizon=8
            reward_single_sum=25.578549393479946, confidence_size=5.294834729440604, confidence_max=29.63292306457803, new_horizon=8
            reward_single_sum=13.391387709160592, confidence_size=5.103349950387008, confidence_max=28.920166827144584, new_horizon=8
            reward_single_sum=48.00954418719452, confidence_size=5.210385059916064, confidence_max=30.126871359875324, new_horizon=8
            reward_single_sum=21.50893006552852, confidence_size=4.97479268111646, confidence_max=29.743124362187423, new_horizon=8
            reward_single_sum=38.44603368972915, confidence_size=4.853226943986368, confidence_max=30.19146287541809, new_horizon=8
            reward_single_sum=15.542801214186994, confidence_size=4.695034781686887, confidence_max=29.64145332442882, new_horizon=8
            reward_single_sum=7.37977253852147, confidence_size=4.649137160645122, confidence_max=28.9199154724555, new_horizon=8
            reward_single_sum=24.349627041185713, confidence_size=4.467029753616622, confidence_max=28.740728388737196, new_horizon=8
            reward_single_sum=48.88894700658241, confidence_size=4.551992633247055, confidence_max=29.70480728163412, new_horizon=8
            reward_single_sum=22.210298076420184, confidence_size=4.390054583026393, confidence_max=29.441403142724948, new_horizon=8
            reward_single_sum=-9.629755266880485, confidence_size=4.669440892977805, confidence_max=28.56475265845706, new_horizon=8
        hit cap of: 30 iterations
        episode=10, horizon=10, effective_score=23.90, baseline_lowerbound=20.22 baseline_stdev=2.14, new_epsilon=2.0255, bad=False, gap_average=0.7579821415817379
            reward_single_sum=4.980929204734801, 
            reward_single_sum=43.2411954024646, confidence_size=120.78290683130177, confidence_max=144.8939691349014, new_horizon=10
            reward_single_sum=51.22761537369489, confidence_size=41.673847469830434, confidence_max=74.82376079679518, new_horizon=10
            reward_single_sum=18.82528968660774, confidence_size=25.200639033545457, confidence_max=54.769396450420956, new_horizon=10
            reward_single_sum=-9.103715685005913, confidence_size=24.177780019517602, confidence_max=46.012042816016816, new_horizon=10
            reward_single_sum=23.490549301460288, confidence_size=18.66780114968049, confidence_max=40.77811169700655, new_horizon=10
            reward_single_sum=-1.714666474765858, confidence_size=16.58980634667649, confidence_max=35.29654874798942, new_horizon=10
            reward_single_sum=44.10319678429542, confidence_size=15.244456002004805, confidence_max=37.12575520119055, new_horizon=10
            reward_single_sum=29.316387441472703, confidence_size=13.284881529571814, confidence_max=35.99230164456722, new_horizon=10
            reward_single_sum=23.54887263493519, confidence_size=11.714456676890695, confidence_max=34.50602204388008, new_horizon=10
            reward_single_sum=28.74760657023843, confidence_size=10.522611924840819, confidence_max=33.85563558303466, new_horizon=10
            reward_single_sum=37.455624153894334, confidence_size=9.749777299725455, confidence_max=34.25968433256101, new_horizon=10
            reward_single_sum=21.682387925362715, confidence_size=8.90902489530596, confidence_max=33.201430458335906, new_horizon=10
            reward_single_sum=18.308853603132206, confidence_size=8.230485446785266, confidence_max=32.09549444125109, new_horizon=10
            reward_single_sum=18.20021510111185, confidence_size=7.649503322706629, confidence_max=31.136859390948853, new_horizon=10
            reward_single_sum=23.81404773511987, confidence_size=7.1219891231316215, confidence_max=30.629763420553697, new_horizon=10
            reward_single_sum=9.36548714617648, confidence_size=6.819063754343504, confidence_max=29.494938807574666, new_horizon=10
            reward_single_sum=17.79865765010231, confidence_size=6.423278711986013, confidence_max=28.828197242821126, new_horizon=10
            reward_single_sum=-13.040950441590292, confidence_size=6.866293382081052, confidence_max=27.405639861735878, new_horizon=10
            reward_single_sum=17.59229382387852, confidence_size=6.500410905107634, confidence_max=26.892404751973647, new_horizon=10
            reward_single_sum=-2.9225295482531646, confidence_size=6.45775144402078, confidence_max=25.739529891119215, new_horizon=10
            reward_single_sum=21.596302084033084, confidence_size=6.1456993695381605, confidence_max=25.532683436497262, new_horizon=10
            reward_single_sum=-30.936739873591385, confidence_size=6.961103060398644, confidence_max=24.160099129942505, new_horizon=10
            reward_single_sum=1.6234932722951694, confidence_size=6.744394152943941, confidence_max=23.294410939269106, new_horizon=10
            reward_single_sum=22.494424890477674, confidence_size=6.470512808014272, confidence_max=23.258305918505535, new_horizon=10
            reward_single_sum=15.89907665744257, confidence_size=6.206981371082767, confidence_max=22.960593079533698, new_horizon=10
            reward_single_sum=21.667939925294437, confidence_size=5.971923727433657, confidence_max=22.907547592063978, new_horizon=8
            reward_single_sum=5.770968403661574, confidence_size=5.786823150804107, confidence_max=22.32370932039983, new_horizon=8
            reward_single_sum=18.563667759852123, confidence_size=5.577907517203, confidence_max=22.18468270715239, new_horizon=8
            reward_single_sum=1.6729442479006238, confidence_size=5.4484749111540385, confidence_max=21.55745573636847, new_horizon=8
        episode=11, horizon=8, effective_score=16.11, baseline_lowerbound=20.22 baseline_stdev=2.14, new_epsilon=1.6879, bad=True, gap_average=0.7605978998888989
            reward_single_sum=20.198522005738766, 
            reward_single_sum=18.571002316430608, confidence_size=5.137877451868866, confidence_max=24.52263961295355, new_horizon=8
            reward_single_sum=-11.735285421987184, confidence_size=30.32108141290038, confidence_max=39.33249437962777, new_horizon=8
            reward_single_sum=29.321144958163288, confidence_size=21.008811646330415, confidence_max=35.09765761091678, new_horizon=8
            reward_single_sum=4.833325588596914, confidence_size=15.260645325484631, confidence_max=27.498387214873105, new_horizon=8
            reward_single_sum=22.610660969806386, confidence_size=12.282006790401343, confidence_max=26.248568526526135, new_horizon=8
            reward_single_sum=37.36194325371382, confidence_size=11.932225215611478, confidence_max=29.24098431139185, new_horizon=8
            reward_single_sum=21.134281193108247, confidence_size=10.115802888687124, confidence_max=27.90275224663348, new_horizon=8
            reward_single_sum=17.22583464420506, confidence_size=8.757113124106375, confidence_max=26.48171640274814, new_horizon=8
            reward_single_sum=24.279244936768468, confidence_size=7.814182175893969, confidence_max=26.194249620348405, new_horizon=8
            reward_single_sum=29.238655109016396, confidence_size=7.2139529682026, confidence_max=26.581164745799033, new_horizon=8
            reward_single_sum=33.636309022514716, confidence_size=6.865726836413773, confidence_max=27.42203005108673, new_horizon=8
            reward_single_sum=19.698708760367726, confidence_size=6.2688356153070455, confidence_max=26.759170025802675, new_horizon=8
            reward_single_sum=11.452853903415406, confidence_size=5.879060444380917, confidence_max=25.723860532942243, new_horizon=8
            reward_single_sum=-6.6633937779475865, confidence_size=6.27045296524895, confidence_max=24.34804012937635, new_horizon=8
            reward_single_sum=37.97893166619693, confidence_size=6.231888430637559, confidence_max=25.553309626144305, new_horizon=8
            reward_single_sum=9.13424958655829, confidence_size=5.9230394442225425, confidence_max=24.645215250967617, new_horizon=8
            reward_single_sum=21.394371471488224, confidence_size=5.570207992733576, confidence_max=24.440839114186602, new_horizon=8
            reward_single_sum=30.31118151800367, confidence_size=5.354884196944493, confidence_max=24.82764954979492, new_horizon=8
            reward_single_sum=18.36494890732056, confidence_size=5.066549386011124, confidence_max=24.483923916585056, new_horizon=8
            reward_single_sum=38.50514100888717, confidence_size=5.056116192580424, confidence_max=25.382431984026415, new_horizon=8
            reward_single_sum=20.013830056275598, confidence_size=4.809768131010337, confidence_max=25.12188002540313, new_horizon=8
            reward_single_sum=11.353774203720736, confidence_size=4.634791367812735, confidence_max=24.55741031913283, new_horizon=8
            reward_single_sum=23.20409818886357, confidence_size=4.435212018886571, confidence_max=24.494559271770978, new_horizon=8
            reward_single_sum=30.82948172004333, confidence_size=4.3101731010969875, confidence_max=24.80032573266775, new_horizon=8
            reward_single_sum=22.66689852685349, confidence_size=4.136917696955658, confidence_max=24.71079132449883, new_horizon=8
            reward_single_sum=23.919983736967865, confidence_size=3.9804879146543968, confidence_max=24.67829154625034, new_horizon=8
            reward_single_sum=15.942919805247667, confidence_size=3.8413608812524647, confidence_max=24.36934723333597, new_horizon=8
            reward_single_sum=5.017999599688798, confidence_size=3.812003210446857, confidence_max=23.80516243313744, new_horizon=8
            reward_single_sum=17.494532780863985, confidence_size=3.6811279646017994, confidence_max=23.590999639231498, new_horizon=8
        hit cap of: 30 iterations
        episode=12, horizon=18, effective_score=19.91, baseline_lowerbound=20.22 baseline_stdev=2.14, new_epsilon=1.4066, bad=True, gap_average=0.5982555844062982
            reward_single_sum=22.228507514423193, 
            reward_single_sum=36.121724282993604, confidence_size=43.859159209009604, confidence_max=73.03427510771797, new_horizon=14
            reward_single_sum=16.854668798669337, confidence_size=16.761596733108327, confidence_max=41.8298969318037, new_horizon=12
            reward_single_sum=40.447339872735554, confidence_size=13.157323857589219, confidence_max=42.07038397479464, new_horizon=12
            reward_single_sum=46.57423666827873, confidence_size=11.91382167444559, confidence_max=44.35911710186567, new_horizon=13
            reward_single_sum=18.336663410186183, confidence_size=10.343723941692497, confidence_max=40.437580699573594, new_horizon=14
            reward_single_sum=15.815167300902626, confidence_size=9.31559150241727, confidence_max=37.36963548073002, new_horizon=11
            reward_single_sum=23.463316776476425, confidence_size=7.940537126048449, confidence_max=35.420740204131654, new_horizon=12
            reward_single_sum=35.41371926377521, confidence_size=7.066170235903449, confidence_max=35.427875112396876, new_horizon=12
            reward_single_sum=22.94163563856657, confidence_size=6.30905285433392, confidence_max=34.12875080703466, new_horizon=12
            reward_single_sum=42.52955367828388, confidence_size=6.1409964132058, confidence_max=35.29795397732319, new_horizon=12
            reward_single_sum=47.19686188790639, confidence_size=6.17602114332383, confidence_max=36.83630406775697, new_horizon=12
            reward_single_sum=54.33388176612546, confidence_size=6.505556503278166, confidence_max=38.98688549245687, new_horizon=12
            reward_single_sum=47.189642987725634, confidence_size=6.2671427908399355, confidence_max=39.7990656370577, new_horizon=12
            reward_single_sum=24.57000069488535, confidence_size=5.897334850733502, confidence_max=38.83179622019578, new_horizon=12
            reward_single_sum=56.06410292246437, confidence_size=6.047208491086259, confidence_max=40.42727245761117, new_horizon=12
            reward_single_sum=12.944072306353378, confidence_size=6.070391653716975, confidence_max=39.18951493434944, new_horizon=12
            reward_single_sum=18.253778120588777, confidence_size=5.880827413572435, confidence_max=38.17409818531358, new_horizon=12
            reward_single_sum=31.049069498420586, confidence_size=5.546143308541179, confidence_max=37.77392980273913, new_horizon=12
            reward_single_sum=15.236448913541428, confidence_size=5.448350574011016, confidence_max=36.826570189176145, new_horizon=12
            reward_single_sum=27.392183767881768, confidence_size=5.179540165099276, confidence_max=36.367948549441394, new_horizon=12
            reward_single_sum=14.449832648116818, confidence_size=5.098089055466309, confidence_max=35.52565308816182, new_horizon=12
            reward_single_sum=19.827714373138015, confidence_size=4.9251986546686926, confidence_max=34.89189965868779, new_horizon=12
            reward_single_sum=4.799254222023926, confidence_size=5.038006283948725, confidence_max=33.95606367205136, new_horizon=12
            reward_single_sum=15.898467375270002, confidence_size=4.905453793191137, confidence_max=33.30272758078046, new_horizon=12
            reward_single_sum=-2.565436846550183, confidence_size=5.1263244535581585, confidence_max=32.33272475521904, new_horizon=12
            reward_single_sum=31.43032454840595, confidence_size=4.932744654665791, confidence_max=32.29558659509501, new_horizon=12
            reward_single_sum=50.974267618202816, confidence_size=4.95936732594253, confidence_max=33.16547446914938, new_horizon=12
            reward_single_sum=17.9930466303222, confidence_size=4.81664147657761, confidence_max=32.67057411934016, new_horizon=12
            reward_single_sum=12.840897691730703, confidence_size=4.724975615392179, confidence_max=32.078473759787, new_horizon=12
        hit cap of: 30 iterations
        episode=13, horizon=8, effective_score=27.35, baseline_lowerbound=20.22 baseline_stdev=2.14, new_epsilon=1.6879, bad=False, gap_average=0.5315968138332537
            reward_single_sum=18.13747094826671, 
            reward_single_sum=23.918153737450037, confidence_size=18.248897358394963, confidence_max=39.276709701253324, new_horizon=8
            reward_single_sum=-7.054524177480152, confidence_size=27.76426821648298, confidence_max=39.4313017192285, new_horizon=8
            reward_single_sum=22.566167889024594, confidence_size=17.072655274997054, confidence_max=31.464472374312344, new_horizon=8
            reward_single_sum=16.729568060257, confidence_size=12.021032416663527, confidence_max=26.88039970816716, new_horizon=8
            reward_single_sum=30.286824851254103, confidence_size=10.626123735734048, confidence_max=28.05673395386276, new_horizon=8
            reward_single_sum=6.2497834475720255, confidence_size=9.199786439488566, confidence_max=25.033135690394893, new_horizon=8
            reward_single_sum=9.175492928482651, confidence_size=7.926382257903978, confidence_max=22.92749946850735, new_horizon=8
            reward_single_sum=13.820574872408201, confidence_size=6.865494460729964, confidence_max=21.73544030042276, new_horizon=8
            reward_single_sum=21.293574528777764, confidence_size=6.166854058625862, confidence_max=21.679162767227155, new_horizon=8
            reward_single_sum=24.280066095694753, confidence_size=5.701352011829691, confidence_max=22.01072957380312, new_horizon=8
            reward_single_sum=24.621597698948342, confidence_size=5.304915350441588, confidence_max=22.30697792382959, new_horizon=8
            reward_single_sum=25.665699393675805, confidence_size=4.986397516660198, confidence_max=22.6548936916088, new_horizon=8
            reward_single_sum=19.34345179055069, confidence_size=4.591987863735332, confidence_max=22.380123725512654, new_horizon=8
            reward_single_sum=28.66140413957057, confidence_size=4.439239690026121, confidence_max=22.95226010365633, new_horizon=8
            reward_single_sum=53.27519311344697, confidence_size=5.6203774905211645, confidence_max=26.30603369788992, new_horizon=8
            reward_single_sum=14.32990412756167, confidence_size=5.298203764193243, confidence_max=25.609992202161582, new_horizon=8
            reward_single_sum=41.932732620903785, confidence_size=5.398064406511141, confidence_max=26.911016410198112, new_horizon=8
            reward_single_sum=22.888502237369238, confidence_size=5.091335730291558, confidence_max=26.67668511469865, new_horizon=8
            reward_single_sum=10.522192156797923, confidence_size=4.910387364449656, confidence_max=25.942578887476287, new_horizon=8
            reward_single_sum=12.949315412291687, confidence_size=4.705844060928609, confidence_max=25.353136721539293, new_horizon=8
            reward_single_sum=17.627475106181684, confidence_size=4.482731408654658, confidence_max=24.992759634973115, new_horizon=8
            reward_single_sum=31.109428736409342, confidence_size=4.3470739621653145, confidence_max=25.317945688922507, new_horizon=8
            reward_single_sum=18.240984515213043, confidence_size=4.158645468094933, confidence_max=25.015771894371117, new_horizon=8
            reward_single_sum=13.097057959108183, confidence_size=4.017132813756168, confidence_max=24.563856501345633, new_horizon=8
            reward_single_sum=9.278522723450719, confidence_size=3.923819666918872, confidence_max=24.03715100973377, new_horizon=8
            reward_single_sum=18.011179054983142, confidence_size=3.772458711844127, confidence_max=23.80793256251711, new_horizon=8
            reward_single_sum=19.924758280579614, confidence_size=3.63027354797698, confidence_max=23.661793271146628, new_horizon=8
        episode=14, horizon=12, effective_score=20.03, baseline_lowerbound=20.22 baseline_stdev=2.14, new_epsilon=1.4066, bad=True, gap_average=0.6647888965410474
    optimal_epsilon = 1.687885802469136
    optimal_horizon = 8
    scaled_epsilon: 1.6879, forecast_average: 5.5941, episode_reward:245.25, max_timestep_reward: 100.00, min_timestep_reward: -4.26
    scaled_epsilon: 1.6879, forecast_average: 5.6204, episode_reward:284.05, max_timestep_reward: 100.00, min_timestep_reward: -2.36
    scaled_epsilon: 1.6879, forecast_average: 5.7982, episode_reward:240.00, max_timestep_reward: 100.00, min_timestep_reward: -2.45
    scaled_epsilon: 1.6879, forecast_average: 5.6012, episode_reward:316.70, max_timestep_reward: 100.00, min_timestep_reward: -9.34
    scaled_epsilon: 1.6879, forecast_average: 5.5515, episode_reward:256.73, max_timestep_reward: 100.00, min_timestep_reward: -10.06
    scaled_epsilon: 1.6879, forecast_average: 5.5664, episode_reward:279.98, max_timestep_reward: 100.00, min_timestep_reward: -9.81
    scaled_epsilon: 1.6879, forecast_average: 5.0758, episode_reward:248.10, max_timestep_reward: 100.00, min_timestep_reward: -10.92
    scaled_epsilon: 1.6879, forecast_average: 5.0895, episode_reward:286.03, max_timestep_reward: 100.00, min_timestep_reward: -7.62
    scaled_epsilon: 1.6879, forecast_average: 5.1785, episode_reward:254.66, max_timestep_reward: 100.00, min_timestep_reward: -2.52
    scaled_epsilon: 1.6879, forecast_average: 5.2723, episode_reward:251.86, max_timestep_reward: 100.00, min_timestep_reward: -2.31
no data found for: results/False/experiments.csv
no data found for: results/False/experiments.csv


-------------------------------------------------------

 Environment: HopperBulletEnv-v0

-------------------------------------------------------




-----------------------------------------------------------------------------------------------------
 Agent Model Exists, loading: ./subrepos/rl-trained-agents/sac/HopperBulletEnv-v0_1/HopperBulletEnv-v0.zip
-----------------------------------------------------------------------------------------------------


pybullet build time: Oct 11 2021 20:52:37
Wrapping the env with a `Monitor` wrapper
Wrapping the env in a DummyVecEnv.


-----------------------------------------------------------------------------------------------------
 Coach Model Exists, loading: models.ignore/coach/HopperBulletEnv-v0/final_1_99%/
-----------------------------------------------------------------------------------------------------


coach settings = {
    "force_retrain": False, 
    "learning_rate": 0.0001, 
    "hidden_sizes": [64, 64, 64, 64, ], 
    "loss_function": "state_prediction_loss", 
    "consistent_coach_loss": {"scale_future_state_loss": 1000000, }, 
    "delayed_coach_loss": {"epochs_of_delay": 50, }, 
    "value_plus_state_loss": {"value_proportion": 0.5, }, 
    "number_of_episodes": 100, 
    "number_of_epochs": 100, 
    "train_test_split": 0.7, 
    "batch_size": 128, 
    "with_card": True, 
}


-----------------------------------------------------------------------------------------------------
 Testing Agent+Coach
-----------------------------------------------------------------------------------------------------


test settings = {
    "override_save_path": False, 
    "increment_factor": 1.2, 
    "force_recompute": True, 
    "graph_smoothing": 0, 
    "number_of_episodes_for_baseline": 30, 
    "number_of_epochs_for_optimal_parameters": 15, 
    "number_of_episodes_for_testing": 10, 
    "initial_epsilon": 3.5, 
    "initial_horizon": 10, 
    "reward_discount": 0.98, 
    "acceptable_performance_levels": [0.8, 0.9, 0.95, 0.98, 0.99, ], 
    "acceptable_performance_loss": 1, 
    "acceptable_performance_level": 0.8, 
    "confidence_interval_for_convergence": 90, 
    "min_reward_single_timestep": 18, 
    "max_reward_single_timestep": 110, 
    "horizons": {0.1: 26, 0.01: 16, 0.001: 11, }, 
}
----- getting a reward baseline -----------------------------------------------------------------------------------------------------------------------------------
  episode_index=0, episode_discounted_reward_sum=110.11968485394878
  episode_index=1, episode_discounted_reward_sum=110.55466453727954
  episode_index=2, episode_discounted_reward_sum=111.76757877699877
  episode_index=3, episode_discounted_reward_sum=111.62889378792175
  episode_index=4, episode_discounted_reward_sum=112.07099354858335
  episode_index=5, episode_discounted_reward_sum=109.01959089487431
  episode_index=6, episode_discounted_reward_sum=111.66735900701326
  episode_index=7, episode_discounted_reward_sum=110.33561498161673
  episode_index=8, episode_discounted_reward_sum=109.06665316787837
  episode_index=9, episode_discounted_reward_sum=110.99926063513531
  episode_index=10, episode_discounted_reward_sum=109.7052524747483
  episode_index=11, episode_discounted_reward_sum=110.25582955998082
  episode_index=12, episode_discounted_reward_sum=111.41403307304526
  episode_index=13, episode_discounted_reward_sum=112.25356944197921
  episode_index=14, episode_discounted_reward_sum=110.76021938773228
  episode_index=15, episode_discounted_reward_sum=110.59348404075577
  episode_index=16, episode_discounted_reward_sum=111.40044397587295
  episode_index=17, episode_discounted_reward_sum=109.57456048192493
  episode_index=18, episode_discounted_reward_sum=109.34888205918433
  episode_index=19, episode_discounted_reward_sum=112.01490438839079
  episode_index=20, episode_discounted_reward_sum=107.31435768287157
  episode_index=21, episode_discounted_reward_sum=111.4926732632837
  episode_index=22, episode_discounted_reward_sum=109.44349526677144
  episode_index=23, episode_discounted_reward_sum=107.03469613680413
  episode_index=24, episode_discounted_reward_sum=112.70217735441446
  episode_index=25, episode_discounted_reward_sum=111.9221378605973
  episode_index=26, episode_discounted_reward_sum=110.68100822833478
  episode_index=27, episode_discounted_reward_sum=112.37015211926534
  episode_index=28, episode_discounted_reward_sum=110.37112732999414
  episode_index=29, episode_discounted_reward_sum=111.57131689121152
    baseline = {
    "max": 112.70217735441446, 
    "min": 107.03469613680413, 
    "range": 5.66748121761033, 
    "count": 30, 
    "sum": 3319.4546152084126, 
    "average": 110.64848717361376, 
    "stdev": 1.3959391080977344, 
    "median": 110.72061380803353, 
}
    baseline_min = 110.21544350333522, baseline_max = 111.08153084389232,
    baseline_confidence_size = 0.4330436702785505
    ----- finding optimal epsilon -------------------------------------------------------------------------------------------------------------------------------------
            reward_single_sum=21.677721024977295, 
            reward_single_sum=21.552309767255952, confidence_size=0.3959077592056097, confidence_max=22.010923155322235, new_horizon=4
        episode=0, horizon=20, effective_score=21.62, baseline_lowerbound=88.52 baseline_stdev=0.25, new_epsilon=2.9167, bad=True, gap_average=2.9650976181030275
            reward_single_sum=54.896479245552115, 
            reward_single_sum=23.088344522389036, confidence_size=100.41432939568156, confidence_max=139.4067412796521, new_horizon=8
            reward_single_sum=22.518674906198097, confidence_size=31.240693390534823, confidence_max=64.74185961524789, new_horizon=8
        episode=1, horizon=20, effective_score=33.50, baseline_lowerbound=88.52 baseline_stdev=0.25, new_epsilon=2.4306, bad=True, gap_average=2.818587636947632
            reward_single_sum=89.40037550393117, 
            reward_single_sum=96.25348089962479, confidence_size=21.634402286575522, confidence_max=114.46133048835348, new_horizon=8
            reward_single_sum=97.82126057531569, confidence_size=7.549864510999271, confidence_max=102.04157017062316, new_horizon=8
        episode=2, horizon=8, effective_score=94.49, baseline_lowerbound=88.52 baseline_stdev=0.25, new_epsilon=2.9167, bad=False, gap_average=1.3468734294176101
            reward_single_sum=70.44116683746833, 
            reward_single_sum=95.49258922238438, confidence_size=79.0842280153409, confidence_max=162.0511060452672, new_horizon=6
            reward_single_sum=24.22083371228597, confidence_size=60.953824119445024, confidence_max=124.33868737682457, new_horizon=6
            reward_single_sum=22.863889036910937, confidence_size=42.131015692699386, confidence_max=95.38563539496178, new_horizon=6
        episode=3, horizon=8, effective_score=53.25, baseline_lowerbound=88.52 baseline_stdev=0.25, new_epsilon=2.4306, bad=True, gap_average=1.8704951765890023
            reward_single_sum=103.81864309953268, 
            reward_single_sum=24.673270185806903, confidence_size=249.85210906176056, confidence_max=314.0980657044302, new_horizon=8
            reward_single_sum=101.52340276378874, confidence_size=75.94208649812276, confidence_max=152.61385851449884, new_horizon=8
            reward_single_sum=91.4697356742209, confidence_size=44.145877878854904, confidence_max=124.5171408096922, new_horizon=8
            reward_single_sum=91.99015425634659, confidence_size=31.37016000488652, confidence_max=114.06520120082567, new_horizon=8
            reward_single_sum=95.98499481517028, confidence_size=24.61830534606191, confidence_max=109.52833881187291, new_horizon=8
        episode=4, horizon=20, effective_score=84.91, baseline_lowerbound=88.52 baseline_stdev=0.25, new_epsilon=2.0255, bad=True, gap_average=1.318618910594637
            reward_single_sum=102.75043295294489, 
            reward_single_sum=23.015439786023283, confidence_size=251.71346694514676, confidence_max=314.5964033146307, new_horizon=10
            reward_single_sum=103.15697053583062, confidence_size=77.80694537786928, confidence_max=154.1145598028022, new_horizon=8
            reward_single_sum=88.37399595500528, confidence_size=44.90632575761168, confidence_max=124.2305355650627, new_horizon=8
            reward_single_sum=100.05575948930117, confidence_size=32.7264601862931, confidence_max=116.19697993011414, new_horizon=8
            reward_single_sum=99.99254727664292, confidence_size=25.859395363332034, confidence_max=112.0835863626234, new_horizon=8
            reward_single_sum=98.162948420269, confidence_size=21.33468393030116, confidence_max=109.26441170401789, new_horizon=8
        episode=5, horizon=20, effective_score=87.93, baseline_lowerbound=88.52 baseline_stdev=0.25, new_epsilon=1.6879, bad=True, gap_average=1.218407500316115
            reward_single_sum=105.32934620469571, 
            reward_single_sum=97.58171442846604, confidence_size=24.45831093164496, confidence_max=125.91384124822582, new_horizon=8
            reward_single_sum=101.96238015741919, confidence_size=6.549296533507402, confidence_max=108.17377679703438, new_horizon=8
        episode=6, horizon=8, effective_score=101.62, baseline_lowerbound=88.52 baseline_stdev=0.25, new_epsilon=2.0255, bad=False, gap_average=1.083945416378138
            reward_single_sum=104.36322435166475, 
            reward_single_sum=100.64636988307873, confidence_size=11.733647765664806, confidence_max=114.23844488303654, new_horizon=6
            reward_single_sum=101.5293112508401, confidence_size=3.273746482190589, confidence_max=105.45338164405179, new_horizon=6
        episode=7, horizon=8, effective_score=102.18, baseline_lowerbound=88.52 baseline_stdev=0.25, new_epsilon=2.4306, bad=False, gap_average=1.1476591736671629
            reward_single_sum=100.93527680273439, 
            reward_single_sum=84.04151727837196, confidence_size=53.33149989381303, confidence_max=145.81989693436617, new_horizon=8
            reward_single_sum=100.47124738788926, confidence_size=16.222067800463414, confidence_max=111.37141495679528, new_horizon=8
            reward_single_sum=21.435393714802064, confidence_size=44.343333849757514, confidence_max=121.06419264570692, new_horizon=8
            reward_single_sum=97.49645502019557, confidence_size=32.35141236942726, confidence_max=113.2273904102259, new_horizon=8
            reward_single_sum=104.4054160965905, confidence_size=26.18828077291034, confidence_max=110.98583182300763, new_horizon=8
            reward_single_sum=95.3135970466437, confidence_size=21.542452302182852, confidence_max=107.84229563750105, new_horizon=8
        episode=8, horizon=6, effective_score=86.30, baseline_lowerbound=88.52 baseline_stdev=0.25, new_epsilon=2.0255, bad=True, gap_average=1.3589768546044558
            reward_single_sum=100.18610697694504, 
            reward_single_sum=101.35269190825817, confidence_size=3.6827636886110895, confidence_max=104.4521631312127, new_horizon=8
        episode=9, horizon=8, effective_score=100.77, baseline_lowerbound=88.52 baseline_stdev=0.25, new_epsilon=2.4306, bad=False, gap_average=1.2799135521522857
            reward_single_sum=100.94721531545422, 
            reward_single_sum=96.32363089145817, confidence_size=14.596081580407535, confidence_max=113.23150468386372, new_horizon=8
            reward_single_sum=23.976704471957206, confidence_size=72.771897957057, confidence_max=146.52108151668017, new_horizon=8
            reward_single_sum=95.88468489142433, confidence_size=43.46892026649587, confidence_max=122.75197915906934, new_horizon=8
            reward_single_sum=98.45857003330308, confidence_size=31.57827062906844, confidence_max=114.69643174978782, new_horizon=8
            reward_single_sum=105.87044676383191, confidence_size=25.540748129480956, confidence_max=112.45095685738576, new_horizon=8
            reward_single_sum=101.55042889672171, confidence_size=21.209019433376845, confidence_max=110.21068818539835, new_horizon=8
        episode=10, horizon=6, effective_score=89.00, baseline_lowerbound=88.52 baseline_stdev=0.25, new_epsilon=2.9167, bad=False, gap_average=1.407932974159433
            reward_single_sum=22.385804994500297, 
            reward_single_sum=105.24589994194945, confidence_size=261.57902499550323, confidence_max=325.39487746372794, new_horizon=6
            reward_single_sum=103.21567169950127, confidence_size=79.68043430737711, confidence_max=156.6295598526941, new_horizon=8
            reward_single_sum=92.23178861513985, confidence_size=46.2909473015886, confidence_max=127.0607386143613, new_horizon=6
            reward_single_sum=22.47172074777389, confidence_size=40.901181179977186, confidence_max=110.01135837975012, new_horizon=6
        episode=11, horizon=8, effective_score=69.11, baseline_lowerbound=88.52 baseline_stdev=0.25, new_epsilon=2.4306, bad=True, gap_average=1.5437797124172836
            reward_single_sum=95.41540952532849, 
            reward_single_sum=102.0217237261641, confidence_size=20.855313146388383, confidence_max=119.57387977213466, new_horizon=8
            reward_single_sum=104.42471724147582, confidence_size=7.864873660151197, confidence_max=108.48549049114065, new_horizon=8
        episode=12, horizon=6, effective_score=100.62, baseline_lowerbound=88.52 baseline_stdev=0.25, new_epsilon=2.9167, bad=False, gap_average=1.2823471815355363
            reward_single_sum=92.55180235709274, 
            reward_single_sum=97.53089507877415, confidence_size=15.718377106925153, confidence_max=110.75972582485859, new_horizon=8
            reward_single_sum=21.769978553503503, confidence_size=71.44050515620499, confidence_max=142.05806381932842, new_horizon=8
            reward_single_sum=101.30364105661381, confidence_size=44.536802200235556, confidence_max=122.8258814617316, new_horizon=8
            reward_single_sum=95.96983616736192, confidence_size=32.14722762062625, confidence_max=113.97245826329547, new_horizon=8
            reward_single_sum=97.38570878612612, confidence_size=25.354435359312482, confidence_max=109.77307902589118, new_horizon=8
        episode=13, horizon=8, effective_score=84.42, baseline_lowerbound=88.52 baseline_stdev=0.25, new_epsilon=2.4306, bad=True, gap_average=1.6542512190940082
            reward_single_sum=103.06471397205405, 
            reward_single_sum=21.784754020437944, confidence_size=256.5907351337378, confidence_max=319.0154691299836, new_horizon=8
            reward_single_sum=102.87640471999576, confidence_size=79.02061971734574, confidence_max=154.92924395484164, new_horizon=8
            reward_single_sum=94.41794728307038, confidence_size=46.33125552723634, confidence_max=126.86721052612586, new_horizon=8
            reward_single_sum=97.73109505243058, confidence_size=33.32640556364716, confidence_max=117.30138857324488, new_horizon=8
            reward_single_sum=97.9786391308505, confidence_size=26.146526544428074, confidence_max=112.45545224090127, new_horizon=8
            reward_single_sum=104.01547190572404, confidence_size=21.86924606831846, confidence_max=110.70767836611321, new_horizon=8
            reward_single_sum=99.46351341829752, confidence_size=18.636276413521642, confidence_max=108.80284385137924, new_horizon=8
        episode=14, horizon=8, effective_score=90.17, baseline_lowerbound=88.52 baseline_stdev=0.25, new_epsilon=2.9167, bad=False, gap_average=1.3463539834746756
    optimal_epsilon = 2.430555555555556
    optimal_horizon = 8
    scaled_epsilon: 2.4306, forecast_average: 3.3435, episode_reward:583.75, max_timestep_reward: 4.96, min_timestep_reward: -0.17
    scaled_epsilon: 2.4306, forecast_average: 3.2190, episode_reward:321.09, max_timestep_reward: 4.52, min_timestep_reward: 0.17
    scaled_epsilon: 2.4306, forecast_average: 3.2964, episode_reward:851.33, max_timestep_reward: 5.03, min_timestep_reward: -0.98
    scaled_epsilon: 2.4306, forecast_average: 3.3644, episode_reward:1507.56, max_timestep_reward: 5.10, min_timestep_reward: 0.10
    scaled_epsilon: 2.4306, forecast_average: 3.3287, episode_reward:497.83, max_timestep_reward: 4.67, min_timestep_reward: -0.17
    scaled_epsilon: 2.4306, forecast_average: 3.4100, episode_reward:688.20, max_timestep_reward: 4.88, min_timestep_reward: -0.59
    scaled_epsilon: 2.4306, forecast_average: 3.4026, episode_reward:659.47, max_timestep_reward: 4.58, min_timestep_reward: -0.04
    scaled_epsilon: 2.4306, forecast_average: 3.4755, episode_reward:766.34, max_timestep_reward: 4.87, min_timestep_reward: 0.61
    scaled_epsilon: 2.4306, forecast_average: 3.3902, episode_reward:399.48, max_timestep_reward: 4.72, min_timestep_reward: 0.86
    scaled_epsilon: 2.4306, forecast_average: 3.3615, episode_reward:517.38, max_timestep_reward: 5.48, min_timestep_reward: -0.68
no data found for: results/False/experiments.csv
no data found for: results/False/experiments.csv
/home/jeffhykin/repos/AFRL/.venv/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(
argv[0]=
argv[0]=


-------------------------------------------------------

 Environment: HalfCheetahBulletEnv-v0

-------------------------------------------------------




-----------------------------------------------------------------------------------------------------
 Agent Model Exists, loading: subrepos/rl-trained-agents/sac/HalfCheetahBulletEnv-v0_1/HalfCheetahBulletEnv-v0.zip
-----------------------------------------------------------------------------------------------------


pybullet build time: Oct 11 2021 20:52:37
Wrapping the env with a `Monitor` wrapper
Wrapping the env in a DummyVecEnv.


-----------------------------------------------------------------------------------------------------
 Training Coach Model from scrach
-----------------------------------------------------------------------------------------------------


coach settings = {
    "force_retrain": False, 
    "learning_rate": 0.0001, 
    "hidden_sizes": [64, 64, 64, 64, ], 
    "loss_function": "state_prediction_loss", 
    "consistent_coach_loss": {"scale_future_state_loss": 1000000, }, 
    "delayed_coach_loss": {"epochs_of_delay": 50, }, 
    "value_plus_state_loss": {"value_proportion": 0.5, }, 
    "number_of_episodes": 100, 
    "number_of_epochs": 100, 
    "train_test_split": 0.7, 
    "batch_size": 128, 
    "with_card": True, 
}
Starting Experience Recording
    Episode: 0, Reward: 2786.718, Average Reward: 2786.718
    Episode: 1, Reward: 2813.239, Average Reward: 2799.979
    Episode: 2, Reward: 2781.614, Average Reward: 2793.857
    Episode: 3, Reward: 2798.515, Average Reward: 2795.021
