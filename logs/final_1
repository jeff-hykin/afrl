

-------------------------------------------------------

 Environment: LunarLanderContinuous-v2

-------------------------------------------------------




-----------------------------------------------------------------------------------------------------
 Agent Model Exists, loading: ./subrepos/rl-trained-agents/sac/LunarLanderContinuous-v2_1/LunarLanderContinuous-v2.zip
-----------------------------------------------------------------------------------------------------


Wrapping the env with a `Monitor` wrapper
Wrapping the env in a DummyVecEnv.


-----------------------------------------------------------------------------------------------------
 Training Coach Model from scrach
-----------------------------------------------------------------------------------------------------


coach settings = {
    "force_retrain": False, 
    "learning_rate": 0.0001, 
    "hidden_sizes": [64, 64, 64, 64, ], 
    "loss_function": "state_prediction_loss", 
    "consistent_coach_loss": {"scale_future_state_loss": 1000000, }, 
    "delayed_coach_loss": {"epochs_of_delay": 50, }, 
    "value_plus_state_loss": {"value_proportion": 0.5, }, 
    "number_of_episodes": 100, 
    "number_of_epochs": 100, 
    "train_test_split": 0.7, 
    "batch_size": 128, 
    "with_card": True, 
}
Starting Train/Test
    Epoch: 0
    Epoch: 1
    Epoch: 2
    Epoch: 3
    Epoch: 4
    Epoch: 5
    Epoch: 6
    Epoch: 7
    Epoch: 8
    Epoch: 9
    Epoch: 10
    Epoch: 11
    Epoch: 12
    Epoch: 13
    Epoch: 14
    Epoch: 15
    Epoch: 16
    Epoch: 17
    Epoch: 18
    Epoch: 19
    Epoch: 20
    Epoch: 21
    Epoch: 22
    Epoch: 23
    Epoch: 24
    Epoch: 25
    Epoch: 26
    Epoch: 27
    Epoch: 28
    Epoch: 29
    Epoch: 30
    Epoch: 31
    Epoch: 32
    Epoch: 33
    Epoch: 34
    Epoch: 35
    Epoch: 36
    Epoch: 37
    Epoch: 38
    Epoch: 39
    Epoch: 40
    Epoch: 41
    Epoch: 42
    Epoch: 43
    Epoch: 44
    Epoch: 45
    Epoch: 46
    Epoch: 47
    Epoch: 48
    Epoch: 49
    Epoch: 50
    Epoch: 51
    Epoch: 52
    Epoch: 53
    Epoch: 54
    Epoch: 55
    Epoch: 56
    Epoch: 57
    Epoch: 58
    Epoch: 59
    Epoch: 60
    Epoch: 61
    Epoch: 62
    Epoch: 63
    Epoch: 64
    Epoch: 65
    Epoch: 66
    Epoch: 67
    Epoch: 68
    Epoch: 69
    Epoch: 70
    Epoch: 71
    Epoch: 72
    Epoch: 73
    Epoch: 74
    Epoch: 75
    Epoch: 76
    Epoch: 77
    Epoch: 78
    Epoch: 79
    Epoch: 80
    Epoch: 81
    Epoch: 82
    Epoch: 83
    Epoch: 84
    Epoch: 85
    Epoch: 86
    Epoch: 87
    Epoch: 88
    Epoch: 89
    Epoch: 90
    Epoch: 91
    Epoch: 92
    Epoch: 93
    Epoch: 94
    Epoch: 95
    Epoch: 96
    Epoch: 97
    Epoch: 98
    Epoch: 99
Saving Coach to: models.ignore/coach/LunarLanderContinuous-v2/final_1_80%/


-----------------------------------------------------------------------------------------------------
 Testing Agent+Coach
-----------------------------------------------------------------------------------------------------


test settings = {
    "force_recompute": False, 
    "graph_smoothing": 0, 
    "number_of_episodes_for_baseline": 30, 
    "number_of_epochs_for_optimal_parameters": 15, 
    "number_of_episodes_for_testing": 10, 
    "initial_epsilon": 100.1, 
    "initial_horizon": 16, 
    "reward_discount": 0.98, 
    "acceptable_performance_loss": 1, 
    "acceptable_performance_level": 0.8, 
    "confidence_interval_for_convergence": 90, 
    "min_reward_single_timestep": -100, 
    "max_reward_single_timestep": 70, 
    "horizons": {0.0: 1, 0.005: 10, 0.01: 20, }, 
}

[silver_spectacle] There were unviewed results
[silver_spectacle] So the display server is still running at: http://localhost:9900
[silver_spectacle] (use the stop server button to kill it now)



-------------------------------------------------------

 Environment: HopperBulletEnv-v0

-------------------------------------------------------




-----------------------------------------------------------------------------------------------------
 Agent Model Exists, loading: ./subrepos/rl-trained-agents/sac/HopperBulletEnv-v0_1/HopperBulletEnv-v0.zip
-----------------------------------------------------------------------------------------------------


Wrapping the env with a `Monitor` wrapper
Wrapping the env in a DummyVecEnv.


-----------------------------------------------------------------------------------------------------
 Training Coach Model from scrach
-----------------------------------------------------------------------------------------------------


coach settings = {
    "force_retrain": False, 
    "learning_rate": 0.0001, 
    "hidden_sizes": [64, 64, 64, 64, ], 
    "loss_function": "state_prediction_loss", 
    "consistent_coach_loss": {"scale_future_state_loss": 1000000, }, 
    "delayed_coach_loss": {"epochs_of_delay": 50, }, 
    "value_plus_state_loss": {"value_proportion": 0.5, }, 
    "number_of_episodes": 100, 
    "number_of_epochs": 100, 
    "train_test_split": 0.7, 
    "batch_size": 128, 
    "with_card": True, 
}
Starting Train/Test
    Epoch: 0
    Epoch: 1
    Epoch: 2
    Epoch: 3
    Epoch: 4
    Epoch: 5
    Epoch: 6
    Epoch: 7
    Epoch: 8
    Epoch: 9
    Epoch: 10
    Epoch: 11
    Epoch: 12
    Epoch: 13
    Epoch: 14
    Epoch: 15
    Epoch: 16
    Epoch: 17
    Epoch: 18
    Epoch: 19
    Epoch: 20
    Epoch: 21
    Epoch: 22
    Epoch: 23
    Epoch: 24
    Epoch: 25
    Epoch: 26
    Epoch: 27
    Epoch: 28
    Epoch: 29
    Epoch: 30
    Epoch: 31
    Epoch: 32
    Epoch: 33
    Epoch: 34
    Epoch: 35
    Epoch: 36
    Epoch: 37
    Epoch: 38
    Epoch: 39
    Epoch: 40
    Epoch: 41
    Epoch: 42
    Epoch: 43
    Epoch: 44
    Epoch: 45
    Epoch: 46
    Epoch: 47
    Epoch: 48
    Epoch: 49
    Epoch: 50
    Epoch: 51
    Epoch: 52
    Epoch: 53
    Epoch: 54
    Epoch: 55
    Epoch: 56
    Epoch: 57
    Epoch: 58
    Epoch: 59
    Epoch: 60
    Epoch: 61
    Epoch: 62
    Epoch: 63
    Epoch: 64
    Epoch: 65
    Epoch: 66
    Epoch: 67
    Epoch: 68
    Epoch: 69
    Epoch: 70
    Epoch: 71
    Epoch: 72
    Epoch: 73
    Epoch: 74
    Epoch: 75
    Epoch: 76
    Epoch: 77
    Epoch: 78
    Epoch: 79
    Epoch: 80
    Epoch: 81
    Epoch: 82
    Epoch: 83
    Epoch: 84
    Epoch: 85
    Epoch: 86
    Epoch: 87
    Epoch: 88
    Epoch: 89
    Epoch: 90
    Epoch: 91
    Epoch: 92
    Epoch: 93
    Epoch: 94
    Epoch: 95
    Epoch: 96
    Epoch: 97
    Epoch: 98
    Epoch: 99
Saving Coach to: models.ignore/coach/HopperBulletEnv-v0/final_1_80%/


-----------------------------------------------------------------------------------------------------
 Testing Agent+Coach
-----------------------------------------------------------------------------------------------------


test settings = {
    "force_recompute": False, 
    "graph_smoothing": 0, 
    "number_of_episodes_for_baseline": 30, 
    "number_of_epochs_for_optimal_parameters": 15, 
    "number_of_episodes_for_testing": 10, 
    "initial_epsilon": 100.1, 
    "initial_horizon": 16, 
    "reward_discount": 0.98, 
    "acceptable_performance_loss": 1, 
    "acceptable_performance_level": 0.8, 
    "confidence_interval_for_convergence": 90, 
    "min_reward_single_timestep": 18, 
    "max_reward_single_timestep": 110, 
    "horizons": {0.1: 26, 0.01: 16, 0.001: 11, }, 
}
----- getting a reward baseline -----------------------------------------------------------------------------------------------------------------------------------
  episode_index=0, episode_discounted_reward_sum=112.2204289156404
  episode_index=1, episode_discounted_reward_sum=109.22420425194095
  episode_index=2, episode_discounted_reward_sum=112.79861896992979
  episode_index=3, episode_discounted_reward_sum=111.5092922853988
  episode_index=4, episode_discounted_reward_sum=109.88948479706457
  episode_index=5, episode_discounted_reward_sum=111.96355321807728
  episode_index=6, episode_discounted_reward_sum=111.68228949575698
  episode_index=7, episode_discounted_reward_sum=110.87176452989053
  episode_index=8, episode_discounted_reward_sum=110.76339415315906
  episode_index=9, episode_discounted_reward_sum=110.23904050528931
  episode_index=10, episode_discounted_reward_sum=108.46144514285483
  episode_index=11, episode_discounted_reward_sum=111.11094126454962
  episode_index=12, episode_discounted_reward_sum=111.83001785169992
  episode_index=13, episode_discounted_reward_sum=109.87450059367481
  episode_index=14, episode_discounted_reward_sum=110.06943986243823
  episode_index=15, episode_discounted_reward_sum=110.85856601298913
  episode_index=16, episode_discounted_reward_sum=112.17161501551986
  episode_index=17, episode_discounted_reward_sum=109.41423662444832
  episode_index=18, episode_discounted_reward_sum=107.61325171050676
  episode_index=19, episode_discounted_reward_sum=111.4678163181514
  episode_index=20, episode_discounted_reward_sum=109.728139166574
  episode_index=21, episode_discounted_reward_sum=109.82296946504313
  episode_index=22, episode_discounted_reward_sum=110.55935140384875
  episode_index=23, episode_discounted_reward_sum=110.91368563741615
  episode_index=24, episode_discounted_reward_sum=105.7972408403108
  episode_index=25, episode_discounted_reward_sum=111.35585709145323
  episode_index=26, episode_discounted_reward_sum=112.4569220309779
  episode_index=27, episode_discounted_reward_sum=111.64912520557004
  episode_index=28, episode_discounted_reward_sum=111.2961062234029
  episode_index=29, episode_discounted_reward_sum=110.579624288314
baseline = {
    "max": 112.79861896992979, 
    "min": 105.7972408403108, 
    "range": 7.001378129618985, 
    "count": 30, 
    "sum": 3318.192922871891, 
    "average": 110.60643076239636, 
    "stdev": 1.49541705880447, 
    "median": 110.86516527143982, 
}
baseline_min = 110.14252736719435, baseline_max = 111.07033415759842,
baseline_confidence_size = 0.4639033952020384
----- finding optimal epsilon -------------------------------------------------------------------------------------------------------------------------------------
            reward_single_sum=22.796092322892576, 
            reward_single_sum=24.77871569466261, confidence_size=6.258895658396387, confidence_max=30.046299667173976, new_horizon=2
        episode=0, horizon=32, effective_score=23.79, baseline_lowerbound=88.49 baseline_stdev=0.27, new_epsilon=66.7333, bad=True, gap_average=5.060235118865966
            reward_single_sum=22.49260139901757, 
            reward_single_sum=23.62768516513916, confidence_size=3.583318423888068, confidence_max=26.64346170596643, new_horizon=2
        episode=1, horizon=32, effective_score=23.06, baseline_lowerbound=88.49 baseline_stdev=0.27, new_epsilon=44.4889, bad=True, gap_average=5.157432556152344
            reward_single_sum=21.853771760227126, 
            reward_single_sum=21.786416331732667, confidence_size=0.21263271934347827, confidence_max=22.032726765323375, new_horizon=18
        episode=2, horizon=32, effective_score=21.82, baseline_lowerbound=88.49 baseline_stdev=0.27, new_epsilon=29.6593, bad=True, gap_average=6.608934593200684
            reward_single_sum=21.152587935131557, 
            reward_single_sum=21.85447405943609, confidence_size=2.215767290272751, confidence_max=23.719298287556576, new_horizon=9
        episode=3, horizon=32, effective_score=21.50, baseline_lowerbound=88.49 baseline_stdev=0.27, new_epsilon=19.7728, bad=True, gap_average=7.5578247873406665
            reward_single_sum=22.57927847699014, 
            reward_single_sum=22.50819029956623, confidence_size=0.22441654394732424, confidence_max=22.76815093222551, new_horizon=20
        episode=4, horizon=32, effective_score=22.54, baseline_lowerbound=88.49 baseline_stdev=0.27, new_epsilon=13.1819, bad=True, gap_average=6.0926454717462715
            reward_single_sum=22.235401203919245, 
            reward_single_sum=24.036751718040687, confidence_size=5.686639768610849, confidence_max=28.82271622959081, new_horizon=10
        episode=5, horizon=32, effective_score=23.14, baseline_lowerbound=88.49 baseline_stdev=0.27, new_epsilon=8.7879, bad=True, gap_average=3.00148191906157
            reward_single_sum=21.53498974898434, 
            reward_single_sum=21.860750761478386, confidence_size=1.0283870430486814, confidence_max=22.726257298280046, new_horizon=2
        episode=6, horizon=32, effective_score=21.70, baseline_lowerbound=88.49 baseline_stdev=0.27, new_epsilon=5.8586, bad=True, gap_average=5.920244789123535
            reward_single_sum=22.23533282394064, 
            reward_single_sum=23.442094724401237, confidence_size=3.8095973885185703, confidence_max=26.64831116268951, new_horizon=9
        episode=7, horizon=32, effective_score=22.84, baseline_lowerbound=88.49 baseline_stdev=0.27, new_epsilon=3.9057, bad=True, gap_average=2.904590061732701
            reward_single_sum=21.432797921034336, 
            reward_single_sum=82.73999645329746, confidence_size=193.53920880063902, confidence_max=245.62560598780482, new_horizon=12
            reward_single_sum=36.264940717411484, confidence_size=53.92317279299914, confidence_max=100.73575115691355, new_horizon=12
        episode=8, horizon=32, effective_score=46.81, baseline_lowerbound=88.49 baseline_stdev=0.27, new_epsilon=2.6038, bad=True, gap_average=1.946641767528695
            reward_single_sum=106.1272077030804, 
            reward_single_sum=102.14930064935403, confidence_size=12.557758343101092, confidence_max=116.6960125193183, new_horizon=8
            reward_single_sum=80.02582321294739, confidence_size=23.707636385100223, confidence_max=119.80841357356083, new_horizon=6
            reward_single_sum=98.88594616103636, confidence_size=13.609821156221209, confidence_max=110.40689058782574, new_horizon=6
            reward_single_sum=22.53843779849156, confidence_size=33.07047362457356, confidence_max=115.0158167295555, new_horizon=6
            reward_single_sum=23.02249011613771, confidence_size=32.295435418364896, confidence_max=104.42030302520612, new_horizon=6
        episode=9, horizon=32, effective_score=72.12, baseline_lowerbound=88.49 baseline_stdev=0.27, new_epsilon=1.7359, bad=True, gap_average=1.3651871002326577
            reward_single_sum=101.53694360648153, 
            reward_single_sum=102.72451117764108, confidence_size=3.749003275668528, confidence_max=105.87973066772983, new_horizon=6
        episode=10, horizon=6, effective_score=102.13, baseline_lowerbound=88.49 baseline_stdev=0.27, new_epsilon=2.6038, bad=False, gap_average=0.9643313531769054
            reward_single_sum=95.71420058919618, 
            reward_single_sum=96.59476817351702, confidence_size=2.7798424596951605, confidence_max=98.93432684105177, new_horizon=8
        episode=11, horizon=12, effective_score=96.15, baseline_lowerbound=88.49 baseline_stdev=0.27, new_epsilon=3.9057, bad=False, gap_average=1.391286144873962
            reward_single_sum=22.260560785545135, 
            reward_single_sum=93.5110300343447, confidence_size=224.92887907494287, confidence_max=282.8146744848877, new_horizon=12
            reward_single_sum=104.44034554923658, confidence_size=75.2351985675852, confidence_max=148.639177357294, new_horizon=12
            reward_single_sum=22.08747435076467, confidence_size=52.43934362560595, confidence_max=113.01419630557871, new_horizon=12
            reward_single_sum=22.473950307443676, confidence_size=40.22241684177713, confidence_max=93.17708904724407, new_horizon=12
        episode=12, horizon=8, effective_score=52.95, baseline_lowerbound=88.49 baseline_stdev=0.27, new_epsilon=2.6038, bad=True, gap_average=1.8371738379686036
            reward_single_sum=85.94587979547744, 
            reward_single_sum=21.78839712601763, confidence_size=202.53720169505834, confidence_max=256.40434015580576, new_horizon=8
            reward_single_sum=102.70705719766892, confidence_size=72.00312229478291, confidence_max=142.15023366783754, new_horizon=8
            reward_single_sum=106.85993501230298, confidence_size=46.3717608288558, confidence_max=125.69707811172253, new_horizon=8
            reward_single_sum=22.643101818294266, confidence_size=40.53169678539108, confidence_max=108.5205709753433, new_horizon=8
        episode=13, horizon=6, effective_score=67.99, baseline_lowerbound=88.49 baseline_stdev=0.27, new_epsilon=1.7359, bad=True, gap_average=1.3073319974753688
            reward_single_sum=102.08728517932613, 
            reward_single_sum=101.69803634937107, confidence_size=1.2288101948816461, confidence_max=103.12147095923024, new_horizon=6
        episode=14, horizon=8, effective_score=101.89, baseline_lowerbound=88.49 baseline_stdev=0.27, new_epsilon=2.6038, bad=False, gap_average=1.2491122842889972
optimal_epsilon = 3.9057460752934006
optimal_horizon = 12
    scaled_epsilon: 3.9057, forecast_average: 3.9928, episode_reward:661.71, max_timestep_reward: 4.93, min_timestep_reward: -0.27
    scaled_epsilon: 3.9057, forecast_average: 4.3104, episode_reward:1351.61, max_timestep_reward: 5.49, min_timestep_reward: -0.38
argv[0]=
argv[0]=

[silver_spectacle] There were unviewed results
[silver_spectacle] So the display server is still running at: http://localhost:9900
[silver_spectacle] (use the stop server button to kill it now)



-------------------------------------------------------

 Environment: HalfCheetahBulletEnv-v0

-------------------------------------------------------




-----------------------------------------------------------------------------------------------------
 Agent Model Exists, loading: subrepos/rl-trained-agents/sac/HalfCheetahBulletEnv-v0_1/HalfCheetahBulletEnv-v0.zip
-----------------------------------------------------------------------------------------------------


Wrapping the env with a `Monitor` wrapper
Wrapping the env in a DummyVecEnv.

[silver_spectacle] There were unviewed results
[silver_spectacle] So the display server is still running at: http://localhost:9900
[silver_spectacle] (use the stop server button to kill it now)



-------------------------------------------------------

 Environment: BipedalWalker-v3

-------------------------------------------------------




-----------------------------------------------------------------------------------------------------
 Agent Model Exists, loading: ./subrepos/rl-trained-agents/sac/Walker2d-v3_1/Walker2d-v3.zip
-----------------------------------------------------------------------------------------------------


Wrapping the env with a `Monitor` wrapper
Wrapping the env in a DummyVecEnv.

[silver_spectacle] There were unviewed results
[silver_spectacle] So the display server is still running at: http://localhost:9900
[silver_spectacle] (use the stop server button to kill it now)



-------------------------------------------------------

 Environment: AntBulletEnv-v0

-------------------------------------------------------




-----------------------------------------------------------------------------------------------------
 Agent Model Exists, loading: subrepos/rl-trained-agents/sac/AntBulletEnv-v0_1/AntBulletEnv-v0.zip
-----------------------------------------------------------------------------------------------------


Wrapping the env with a `Monitor` wrapper
Wrapping the env in a DummyVecEnv.

[silver_spectacle] There were unviewed results
[silver_spectacle] So the display server is still running at: http://localhost:9900
[silver_spectacle] (use the stop server button to kill it now)



-------------------------------------------------------

 Environment: ReacherBulletEnv-v0

-------------------------------------------------------




-----------------------------------------------------------------------------------------------------
 Agent Model Exists, loading: ./subrepos/rl-trained-agents/sac/ReacherBulletEnv-v0_1/ReacherBulletEnv-v0.zip
-----------------------------------------------------------------------------------------------------


Wrapping the env with a `Monitor` wrapper
Wrapping the env in a DummyVecEnv.


-----------------------------------------------------------------------------------------------------
 Training Coach Model from scrach
-----------------------------------------------------------------------------------------------------


coach settings = {
    "force_retrain": False, 
    "learning_rate": 0.0001, 
    "hidden_sizes": [64, 64, 64, 64, ], 
    "loss_function": "state_prediction_loss", 
    "consistent_coach_loss": {"scale_future_state_loss": 1000000, }, 
    "delayed_coach_loss": {"epochs_of_delay": 50, }, 
    "value_plus_state_loss": {"value_proportion": 0.5, }, 
    "number_of_episodes": 100, 
    "number_of_epochs": 100, 
    "train_test_split": 0.7, 
    "batch_size": 128, 
    "with_card": True, 
}
Starting Experience Recording
argv[0]=
argv[0]=
    Episode: 0, Reward: 14.514, Average Reward: 14.514
    Episode: 1, Reward: 35.108, Average Reward: 24.811
    Episode: 2, Reward: 31.443, Average Reward: 27.022
    Episode: 3, Reward: 20.700, Average Reward: 25.441
    Episode: 4, Reward: -1.490, Average Reward: 20.055
    Episode: 5, Reward: 14.891, Average Reward: 19.194
    Episode: 6, Reward: 13.082, Average Reward: 18.321
    Episode: 7, Reward: 1.827, Average Reward: 16.259
    Episode: 8, Reward: 17.885, Average Reward: 16.440
    Episode: 9, Reward: 12.157, Average Reward: 16.012
    Episode: 10, Reward: 19.761, Average Reward: 16.352
    Episode: 11, Reward: 10.177, Average Reward: 15.838
    Episode: 12, Reward: 29.770, Average Reward: 16.909
    Episode: 13, Reward: 26.744, Average Reward: 17.612
    Episode: 14, Reward: 29.198, Average Reward: 18.384
    Episode: 15, Reward: 11.420, Average Reward: 17.949
    Episode: 16, Reward: 21.422, Average Reward: 18.153
    Episode: 17, Reward: 29.356, Average Reward: 18.776
    Episode: 18, Reward: 25.018, Average Reward: 19.104
    Episode: 19, Reward: 34.921, Average Reward: 19.895
    Episode: 20, Reward: 13.318, Average Reward: 19.582
    Episode: 21, Reward: 3.122, Average Reward: 18.834
    Episode: 22, Reward: 30.142, Average Reward: 19.325
    Episode: 23, Reward: 2.692, Average Reward: 18.632
    Episode: 24, Reward: 22.401, Average Reward: 18.783
    Episode: 25, Reward: 26.333, Average Reward: 19.073
    Episode: 26, Reward: 21.407, Average Reward: 19.160
    Episode: 27, Reward: 24.642, Average Reward: 19.356
    Episode: 28, Reward: 24.565, Average Reward: 19.535
    Episode: 29, Reward: 22.150, Average Reward: 19.622
    Episode: 30, Reward: 28.709, Average Reward: 19.916
    Episode: 31, Reward: 13.685, Average Reward: 19.721
    Episode: 32, Reward: 19.404, Average Reward: 19.711
    Episode: 33, Reward: 10.867, Average Reward: 19.451
    Episode: 34, Reward: 17.099, Average Reward: 19.384
    Episode: 35, Reward: 11.557, Average Reward: 19.167
    Episode: 36, Reward: 18.018, Average Reward: 19.135
    Episode: 37, Reward: 21.000, Average Reward: 19.185
    Episode: 38, Reward: 16.790, Average Reward: 19.123
    Episode: 39, Reward: 0.489, Average Reward: 18.657
    Episode: 40, Reward: 13.515, Average Reward: 18.532
    Episode: 41, Reward: 21.299, Average Reward: 18.598
    Episode: 42, Reward: 31.101, Average Reward: 18.889
    Episode: 43, Reward: 32.986, Average Reward: 19.209
    Episode: 44, Reward: 18.391, Average Reward: 19.191
    Episode: 45, Reward: 12.080, Average Reward: 19.036
    Episode: 46, Reward: 1.117, Average Reward: 18.655
    Episode: 47, Reward: 18.367, Average Reward: 18.649
    Episode: 48, Reward: 9.932, Average Reward: 18.471
    Episode: 49, Reward: 24.659, Average Reward: 18.595
    Episode: 50, Reward: 12.281, Average Reward: 18.471
    Episode: 51, Reward: 7.139, Average Reward: 18.253
    Episode: 52, Reward: 17.719, Average Reward: 18.243
    Episode: 53, Reward: -0.885, Average Reward: 17.889
    Episode: 54, Reward: 7.209, Average Reward: 17.695
    Episode: 55, Reward: 38.295, Average Reward: 18.062
    Episode: 56, Reward: 13.013, Average Reward: 17.974
    Episode: 57, Reward: 18.824, Average Reward: 17.989
    Episode: 58, Reward: 6.765, Average Reward: 17.798
    Episode: 59, Reward: 19.877, Average Reward: 17.833
    Episode: 60, Reward: 10.494, Average Reward: 17.713
    Episode: 61, Reward: 29.913, Average Reward: 17.909
    Episode: 62, Reward: 21.988, Average Reward: 17.974
    Episode: 63, Reward: 36.148, Average Reward: 18.258
    Episode: 64, Reward: 21.889, Average Reward: 18.314
    Episode: 65, Reward: 26.970, Average Reward: 18.445
    Episode: 66, Reward: 34.363, Average Reward: 18.683
    Episode: 67, Reward: 32.367, Average Reward: 18.884
    Episode: 68, Reward: 0.573, Average Reward: 18.619
    Episode: 69, Reward: 4.477, Average Reward: 18.417
    Episode: 70, Reward: 14.471, Average Reward: 18.361
    Episode: 71, Reward: 23.051, Average Reward: 18.426
    Episode: 72, Reward: 12.411, Average Reward: 18.344
    Episode: 73, Reward: 0.983, Average Reward: 18.109
    Episode: 74, Reward: 25.250, Average Reward: 18.204
    Episode: 75, Reward: -0.095, Average Reward: 17.964
    Episode: 76, Reward: 14.756, Average Reward: 17.922
    Episode: 77, Reward: 21.170, Average Reward: 17.964
    Episode: 78, Reward: 19.859, Average Reward: 17.988
    Episode: 79, Reward: 20.744, Average Reward: 18.022
    Episode: 80, Reward: 21.417, Average Reward: 18.064
    Episode: 81, Reward: -0.370, Average Reward: 17.839
    Episode: 82, Reward: 7.508, Average Reward: 17.715
    Episode: 83, Reward: 8.691, Average Reward: 17.607
    Episode: 84, Reward: 24.795, Average Reward: 17.692
    Episode: 85, Reward: 27.776, Average Reward: 17.809
    Episode: 86, Reward: 14.908, Average Reward: 17.776
    Episode: 87, Reward: 7.840, Average Reward: 17.663
    Episode: 88, Reward: 24.933, Average Reward: 17.744
    Episode: 89, Reward: 12.402, Average Reward: 17.685
    Episode: 90, Reward: 25.122, Average Reward: 17.767
    Episode: 91, Reward: 30.954, Average Reward: 17.910
    Episode: 92, Reward: 25.875, Average Reward: 17.996
    Episode: 93, Reward: 2.317, Average Reward: 17.829
    Episode: 94, Reward: 15.175, Average Reward: 17.801
    Episode: 95, Reward: 20.886, Average Reward: 17.833
    Episode: 96, Reward: 25.721, Average Reward: 17.914
    Episode: 97, Reward: 19.951, Average Reward: 17.935
    Episode: 98, Reward: 9.807, Average Reward: 17.853
    Episode: 99, Reward: 27.088, Average Reward: 17.946

    Max Episode Reward: 38.29521868185669
    Min Episode Reward: -1.4903442682367605
    Max Timestep Reward: 3.4768505902556184
    Min Timestep Reward: -1.8515267649024414
Starting Train/Test
    Epoch: 0
    Epoch: 1
    Epoch: 2
    Epoch: 3
    Epoch: 4
    Epoch: 5
    Epoch: 6
    Epoch: 7
    Epoch: 8
    Epoch: 9
    Epoch: 10
    Epoch: 11
    Epoch: 12
    Epoch: 13
    Epoch: 14
    Epoch: 15
    Epoch: 16
    Epoch: 17
    Epoch: 18
    Epoch: 19
    Epoch: 20
    Epoch: 21
    Epoch: 22
    Epoch: 23
    Epoch: 24
    Epoch: 25
    Epoch: 26
    Epoch: 27
    Epoch: 28
    Epoch: 29
    Epoch: 30
    Epoch: 31
    Epoch: 32
    Epoch: 33
    Epoch: 34
    Epoch: 35
    Epoch: 36
    Epoch: 37
    Epoch: 38
    Epoch: 39
    Epoch: 40
    Epoch: 41
    Epoch: 42
    Epoch: 43
    Epoch: 44
    Epoch: 45
    Epoch: 46
    Epoch: 47
    Epoch: 48
    Epoch: 49
    Epoch: 50
    Epoch: 51
    Epoch: 52
    Epoch: 53
    Epoch: 54
    Epoch: 55
    Epoch: 56
    Epoch: 57
    Epoch: 58
    Epoch: 59
    Epoch: 60
    Epoch: 61
    Epoch: 62
    Epoch: 63
    Epoch: 64
    Epoch: 65
    Epoch: 66
    Epoch: 67
    Epoch: 68
    Epoch: 69
    Epoch: 70
    Epoch: 71
    Epoch: 72
    Epoch: 73
    Epoch: 74
    Epoch: 75
    Epoch: 76
    Epoch: 77
    Epoch: 78
    Epoch: 79
    Epoch: 80
    Epoch: 81
    Epoch: 82
    Epoch: 83
    Epoch: 84
    Epoch: 85
    Epoch: 86
    Epoch: 87
    Epoch: 88
    Epoch: 89
    Epoch: 90
    Epoch: 91
    Epoch: 92
    Epoch: 93
    Epoch: 94
    Epoch: 95
    Epoch: 96
    Epoch: 97
    Epoch: 98
    Epoch: 99
Saving Coach to: models.ignore/coach/ReacherBulletEnv-v0/final_1_80%/


-----------------------------------------------------------------------------------------------------
 Testing Agent+Coach
-----------------------------------------------------------------------------------------------------


test settings = {
    "force_recompute": False, 
    "graph_smoothing": 0, 
    "number_of_episodes_for_baseline": 30, 
    "number_of_epochs_for_optimal_parameters": 15, 
    "number_of_episodes_for_testing": 10, 
    "initial_epsilon": 100.1, 
    "initial_horizon": 16, 
    "reward_discount": 0.98, 
    "acceptable_performance_loss": 1, 
    "acceptable_performance_level": 0.8, 
    "confidence_interval_for_convergence": 90, 
}

[silver_spectacle] There were unviewed results
[silver_spectacle] So the display server is still running at: http://localhost:9900
[silver_spectacle] (use the stop server button to kill it now)



-------------------------------------------------------

 Environment: LunarLanderContinuous-v2

-------------------------------------------------------




-----------------------------------------------------------------------------------------------------
 Agent Model Exists, loading: ./subrepos/rl-trained-agents/sac/LunarLanderContinuous-v2_1/LunarLanderContinuous-v2.zip
-----------------------------------------------------------------------------------------------------


Wrapping the env with a `Monitor` wrapper
Wrapping the env in a DummyVecEnv.


-----------------------------------------------------------------------------------------------------
 Training Coach Model from scrach
-----------------------------------------------------------------------------------------------------


coach settings = {
    "force_retrain": False, 
    "learning_rate": 0.0001, 
    "hidden_sizes": [64, 64, 64, 64, ], 
    "loss_function": "state_prediction_loss", 
    "consistent_coach_loss": {"scale_future_state_loss": 1000000, }, 
    "delayed_coach_loss": {"epochs_of_delay": 50, }, 
    "value_plus_state_loss": {"value_proportion": 0.5, }, 
    "number_of_episodes": 100, 
    "number_of_epochs": 100, 
    "train_test_split": 0.7, 
    "batch_size": 128, 
    "with_card": True, 
}
Starting Train/Test
    Epoch: 0
    Epoch: 1
    Epoch: 2
    Epoch: 3
    Epoch: 4
    Epoch: 5
    Epoch: 6
    Epoch: 7
    Epoch: 8
    Epoch: 9
    Epoch: 10
    Epoch: 11
    Epoch: 12
    Epoch: 13
    Epoch: 14
    Epoch: 15
    Epoch: 16
    Epoch: 17
    Epoch: 18
    Epoch: 19
    Epoch: 20
    Epoch: 21
    Epoch: 22
    Epoch: 23
    Epoch: 24
    Epoch: 25
    Epoch: 26
    Epoch: 27
    Epoch: 28
    Epoch: 29
    Epoch: 30
    Epoch: 31
    Epoch: 32
    Epoch: 33
    Epoch: 34
    Epoch: 35
    Epoch: 36
    Epoch: 37
    Epoch: 38
    Epoch: 39
    Epoch: 40
    Epoch: 41
    Epoch: 42
    Epoch: 43
    Epoch: 44
    Epoch: 45
    Epoch: 46
    Epoch: 47
    Epoch: 48
    Epoch: 49
    Epoch: 50
    Epoch: 51
    Epoch: 52
    Epoch: 53
    Epoch: 54
    Epoch: 55
    Epoch: 56
    Epoch: 57
    Epoch: 58
    Epoch: 59
    Epoch: 60
    Epoch: 61
    Epoch: 62
    Epoch: 63
    Epoch: 64
    Epoch: 65
    Epoch: 66
    Epoch: 67
    Epoch: 68
    Epoch: 69
    Epoch: 70
    Epoch: 71
    Epoch: 72
    Epoch: 73
    Epoch: 74
    Epoch: 75
    Epoch: 76
    Epoch: 77
    Epoch: 78
    Epoch: 79
    Epoch: 80
    Epoch: 81
    Epoch: 82
    Epoch: 83
    Epoch: 84
    Epoch: 85
    Epoch: 86
    Epoch: 87
    Epoch: 88
    Epoch: 89
    Epoch: 90
    Epoch: 91
    Epoch: 92
    Epoch: 93
    Epoch: 94
    Epoch: 95
    Epoch: 96
    Epoch: 97
    Epoch: 98
    Epoch: 99
Saving Coach to: models.ignore/coach/LunarLanderContinuous-v2/final_1_90%/


-----------------------------------------------------------------------------------------------------
 Testing Agent+Coach
-----------------------------------------------------------------------------------------------------


test settings = {
    "force_recompute": False, 
    "graph_smoothing": 0, 
    "number_of_episodes_for_baseline": 30, 
    "number_of_epochs_for_optimal_parameters": 15, 
    "number_of_episodes_for_testing": 10, 
    "initial_epsilon": 100.1, 
    "initial_horizon": 16, 
    "reward_discount": 0.98, 
    "acceptable_performance_loss": 1, 
    "acceptable_performance_level": 0.8, 
    "confidence_interval_for_convergence": 90, 
    "min_reward_single_timestep": -100, 
    "max_reward_single_timestep": 70, 
    "horizons": {0.0: 1, 0.005: 10, 0.01: 20, }, 
}

[silver_spectacle] There were unviewed results
[silver_spectacle] So the display server is still running at: http://localhost:9900
[silver_spectacle] (use the stop server button to kill it now)



-------------------------------------------------------

 Environment: HopperBulletEnv-v0

-------------------------------------------------------




-----------------------------------------------------------------------------------------------------
 Agent Model Exists, loading: ./subrepos/rl-trained-agents/sac/HopperBulletEnv-v0_1/HopperBulletEnv-v0.zip
-----------------------------------------------------------------------------------------------------


Wrapping the env with a `Monitor` wrapper
Wrapping the env in a DummyVecEnv.


-----------------------------------------------------------------------------------------------------
 Training Coach Model from scrach
-----------------------------------------------------------------------------------------------------


coach settings = {
    "force_retrain": False, 
    "learning_rate": 0.0001, 
    "hidden_sizes": [64, 64, 64, 64, ], 
    "loss_function": "state_prediction_loss", 
    "consistent_coach_loss": {"scale_future_state_loss": 1000000, }, 
    "delayed_coach_loss": {"epochs_of_delay": 50, }, 
    "value_plus_state_loss": {"value_proportion": 0.5, }, 
    "number_of_episodes": 100, 
    "number_of_epochs": 100, 
    "train_test_split": 0.7, 
    "batch_size": 128, 
    "with_card": True, 
}
Starting Train/Test
    Epoch: 0
    Epoch: 1
    Epoch: 2
    Epoch: 3
    Epoch: 4
    Epoch: 5
    Epoch: 6
    Epoch: 7
    Epoch: 8
    Epoch: 9
    Epoch: 10
    Epoch: 11
    Epoch: 12
    Epoch: 13
    Epoch: 14
    Epoch: 15
    Epoch: 16
    Epoch: 17
    Epoch: 18
    Epoch: 19
    Epoch: 20
    Epoch: 21
    Epoch: 22
    Epoch: 23
    Epoch: 24
    Epoch: 25
    Epoch: 26
    Epoch: 27
    Epoch: 28
    Epoch: 29
    Epoch: 30
    Epoch: 31
    Epoch: 32
    Epoch: 33
    Epoch: 34
    Epoch: 35
    Epoch: 36
    Epoch: 37
    Epoch: 38
    Epoch: 39
    Epoch: 40
    Epoch: 41
    Epoch: 42
    Epoch: 43
    Epoch: 44
    Epoch: 45
    Epoch: 46
    Epoch: 47
    Epoch: 48
    Epoch: 49
    Epoch: 50
    Epoch: 51
    Epoch: 52
    Epoch: 53
    Epoch: 54
    Epoch: 55
    Epoch: 56
    Epoch: 57
    Epoch: 58
    Epoch: 59
    Epoch: 60
    Epoch: 61
    Epoch: 62
    Epoch: 63
    Epoch: 64
    Epoch: 65
    Epoch: 66
    Epoch: 67
    Epoch: 68
    Epoch: 69
    Epoch: 70
    Epoch: 71
    Epoch: 72
    Epoch: 73
    Epoch: 74
    Epoch: 75
    Epoch: 76
    Epoch: 77
    Epoch: 78
    Epoch: 79
    Epoch: 80
    Epoch: 81
    Epoch: 82
    Epoch: 83
    Epoch: 84
    Epoch: 85
    Epoch: 86
    Epoch: 87
    Epoch: 88
    Epoch: 89
    Epoch: 90
    Epoch: 91
    Epoch: 92
    Epoch: 93
    Epoch: 94
    Epoch: 95
    Epoch: 96
    Epoch: 97
    Epoch: 98
    Epoch: 99
Saving Coach to: models.ignore/coach/HopperBulletEnv-v0/final_1_90%/


-----------------------------------------------------------------------------------------------------
 Testing Agent+Coach
-----------------------------------------------------------------------------------------------------


test settings = {
    "force_recompute": False, 
    "graph_smoothing": 0, 
    "number_of_episodes_for_baseline": 30, 
    "number_of_epochs_for_optimal_parameters": 15, 
    "number_of_episodes_for_testing": 10, 
    "initial_epsilon": 100.1, 
    "initial_horizon": 16, 
    "reward_discount": 0.98, 
    "acceptable_performance_loss": 1, 
    "acceptable_performance_level": 0.8, 
    "confidence_interval_for_convergence": 90, 
    "min_reward_single_timestep": 18, 
    "max_reward_single_timestep": 110, 
    "horizons": {0.1: 26, 0.01: 16, 0.001: 11, }, 
}
baseline = {
    "max": 112.79861896992979, 
    "min": 105.7972408403108, 
    "range": 7.001378129618985, 
    "count": 30, 
    "sum": 3318.192922871891, 
    "average": 110.60643076239636, 
    "stdev": 1.49541705880447, 
    "median": 110.86516527143982, 
}
baseline_min = 110.14252736719435, baseline_max = 111.07033415759842,
baseline_confidence_size = 0.4639033952020384
----- finding optimal epsilon -------------------------------------------------------------------------------------------------------------------------------------
            reward_single_sum=22.184333727574696, 
            reward_single_sum=25.10172322201677, confidence_size=9.209836169898988, confidence_max=32.85286464469472, new_horizon=2
        episode=0, horizon=32, effective_score=23.64, baseline_lowerbound=88.49 baseline_stdev=0.27, new_epsilon=66.7333, bad=True, gap_average=6.052084922790527
            reward_single_sum=23.94565636415873, 
            reward_single_sum=23.242080097449122, confidence_size=2.2211028598578846, confidence_max=25.81497109066181, new_horizon=2
        episode=1, horizon=32, effective_score=23.59, baseline_lowerbound=88.49 baseline_stdev=0.27, new_epsilon=44.4889, bad=True, gap_average=4.44140396118164
            reward_single_sum=22.891324555359727, 
            reward_single_sum=23.432125242608738, confidence_size=1.7072405791619136, confidence_max=24.868965478146144, new_horizon=11
        episode=2, horizon=32, effective_score=23.16, baseline_lowerbound=88.49 baseline_stdev=0.27, new_epsilon=29.6593, bad=True, gap_average=5.3395640055338545
            reward_single_sum=20.988038045921776, 
            reward_single_sum=22.973903063247878, confidence_size=6.269129130666432, confidence_max=28.250099685251257, new_horizon=16
        episode=3, horizon=32, effective_score=21.98, baseline_lowerbound=88.49 baseline_stdev=0.27, new_epsilon=19.7728, bad=True, gap_average=5.615761003996196
            reward_single_sum=21.391772355121716, 
            reward_single_sum=21.73736302116842, confidence_size=1.0909867956267192, confidence_max=22.655554483771787, new_horizon=16
        episode=4, horizon=32, effective_score=21.56, baseline_lowerbound=88.49 baseline_stdev=0.27, new_epsilon=13.1819, bad=True, gap_average=9.767500224866366
            reward_single_sum=21.394909747368178, 
            reward_single_sum=22.486250781962486, confidence_size=3.4452280551671155, confidence_max=25.385808319832446, new_horizon=16
        episode=5, horizon=32, effective_score=21.94, baseline_lowerbound=88.49 baseline_stdev=0.27, new_epsilon=8.7879, bad=True, gap_average=5.345513725280762
            reward_single_sum=23.298653586387644, 
            reward_single_sum=22.128920774474505, confidence_size=3.6927011565644694, confidence_max=26.406488336995544, new_horizon=9
        episode=6, horizon=32, effective_score=22.71, baseline_lowerbound=88.49 baseline_stdev=0.27, new_epsilon=5.8586, bad=True, gap_average=4.418510610407049
            reward_single_sum=58.509046502015025, 
            reward_single_sum=21.9369887583674, confidence_size=115.45344248917124, confidence_max=155.6764601193624, new_horizon=13
            reward_single_sum=25.031216816153933, confidence_size=34.19040540869814, confidence_max=69.34948943421024, new_horizon=13
        episode=7, horizon=32, effective_score=35.16, baseline_lowerbound=88.49 baseline_stdev=0.27, new_epsilon=3.9057, bad=True, gap_average=3.0467761482779436
            reward_single_sum=59.35691447088228, 
            reward_single_sum=21.955313158755825, confidence_size=118.07220847020955, confidence_max=158.72832228502853, new_horizon=13
            reward_single_sum=22.497076952768506, confidence_size=36.143273520855445, confidence_max=70.74637504832431, new_horizon=13
        episode=8, horizon=32, effective_score=34.60, baseline_lowerbound=88.49 baseline_stdev=0.27, new_epsilon=2.6038, bad=True, gap_average=3.0063522338867186
            reward_single_sum=92.91918375423317, 
            reward_single_sum=22.884877762812994, confidence_size=221.08960277068073, confidence_max=278.9916335292037, new_horizon=5
            reward_single_sum=99.36288506220988, confidence_size=71.50888723740836, confidence_max=143.231202763827, new_horizon=6
            reward_single_sum=99.02363393987817, confidence_size=43.803679882906806, confidence_max=122.35132501269035, new_horizon=8
            reward_single_sum=95.530367992805, confidence_size=31.577808821828675, confidence_max=113.52199852421651, new_horizon=8
            reward_single_sum=106.13381197471708, confidence_size=25.68895669879068, confidence_max=111.6647501132334, new_horizon=8
            reward_single_sum=102.57883344793882, confidence_size=21.43808763330061, confidence_max=109.78574390967134, new_horizon=8
        episode=9, horizon=32, effective_score=88.35, baseline_lowerbound=88.49 baseline_stdev=0.27, new_epsilon=1.7359, bad=True, gap_average=1.5303179404400518
            reward_single_sum=102.50249753951915, 
            reward_single_sum=104.23256322787398, confidence_size=5.461602430277708, confidence_max=108.82913281397427, new_horizon=6
        episode=10, horizon=8, effective_score=103.37, baseline_lowerbound=88.49 baseline_stdev=0.27, new_epsilon=2.6038, bad=False, gap_average=1.0922077045462648
            reward_single_sum=82.8216793625221, 
            reward_single_sum=104.24357367122447, confidence_size=67.62625882073755, confidence_max=161.1588853376108, new_horizon=7
            reward_single_sum=94.78241347689556, confidence_size=18.098026087429048, confidence_max=112.04724825764308, new_horizon=8
            reward_single_sum=99.79957883391556, confidence_size=10.873118359746385, confidence_max=106.28492969588581, new_horizon=8
        episode=11, horizon=13, effective_score=95.41, baseline_lowerbound=88.49 baseline_stdev=0.27, new_epsilon=3.9057, bad=False, gap_average=1.3013606232752204
            reward_single_sum=71.1231864941113, 
            reward_single_sum=23.872681822805752, confidence_size=149.163972721782, confidence_max=196.66190688024042, new_horizon=16
            reward_single_sum=98.33388201644084, confidence_size=63.51855190709056, confidence_max=127.96180201820984, new_horizon=11
            reward_single_sum=97.34484619534896, confidence_size=41.049464145914676, confidence_max=113.71811327809138, new_horizon=10
            reward_single_sum=47.45253982962516, confidence_size=30.744954054645213, confidence_max=98.3703813263116, new_horizon=10
        episode=12, horizon=8, effective_score=67.63, baseline_lowerbound=88.49 baseline_stdev=0.27, new_epsilon=2.6038, bad=True, gap_average=1.5969426653641385
            reward_single_sum=99.91646183035897, 
            reward_single_sum=21.72955845613027, confidence_size=246.82633980831545, confidence_max=307.64934995155994, new_horizon=8
            reward_single_sum=104.12921774219011, confidence_size=78.23237558335396, confidence_max=153.49078825958037, new_horizon=8
            reward_single_sum=93.81919053098136, confidence_size=45.90193936273154, confidence_max=125.8005465026467, new_horizon=8
            reward_single_sum=98.22609912673933, confidence_size=33.143106303920185, confidence_max=116.70721184120018, new_horizon=8
            reward_single_sum=23.017566024416787, confidence_size=32.67627009018584, confidence_max=106.1492857086553, new_horizon=8
        episode=13, horizon=6, effective_score=73.47, baseline_lowerbound=88.49 baseline_stdev=0.27, new_epsilon=1.7359, bad=True, gap_average=1.456279840582182
            reward_single_sum=102.1451605057394, 
            reward_single_sum=99.34199486689732, confidence_size=8.849245649238533, confidence_max=109.59282333555689, new_horizon=8
        episode=14, horizon=8, effective_score=100.74, baseline_lowerbound=88.49 baseline_stdev=0.27, new_epsilon=2.6038, bad=False, gap_average=1.0711677624320342
optimal_epsilon = 3.9057460752934006
optimal_horizon = 10
    scaled_epsilon: 3.9057, forecast_average: 4.0547, episode_reward:430.40, max_timestep_reward: 5.52, min_timestep_reward: -1.70
    scaled_epsilon: 3.9057, forecast_average: 4.0602, episode_reward:265.96, max_timestep_reward: 4.60, min_timestep_reward: -1.19
    scaled_epsilon: 3.9057, forecast_average: 2.7068, episode_reward:24.42, max_timestep_reward: 3.17, min_timestep_reward: -2.62
argv[0]=
argv[0]=

[silver_spectacle] There were unviewed results
[silver_spectacle] So the display server is still running at: http://localhost:9900
[silver_spectacle] (use the stop server button to kill it now)



-------------------------------------------------------

 Environment: HalfCheetahBulletEnv-v0

-------------------------------------------------------




-----------------------------------------------------------------------------------------------------
 Agent Model Exists, loading: subrepos/rl-trained-agents/sac/HalfCheetahBulletEnv-v0_1/HalfCheetahBulletEnv-v0.zip
-----------------------------------------------------------------------------------------------------


Wrapping the env with a `Monitor` wrapper
Wrapping the env in a DummyVecEnv.

[silver_spectacle] There were unviewed results
[silver_spectacle] So the display server is still running at: http://localhost:9900
[silver_spectacle] (use the stop server button to kill it now)



-------------------------------------------------------

 Environment: BipedalWalker-v3

-------------------------------------------------------




-----------------------------------------------------------------------------------------------------
 Agent Model Exists, loading: ./subrepos/rl-trained-agents/sac/Walker2d-v3_1/Walker2d-v3.zip
-----------------------------------------------------------------------------------------------------


Wrapping the env with a `Monitor` wrapper
Wrapping the env in a DummyVecEnv.

[silver_spectacle] There were unviewed results
[silver_spectacle] So the display server is still running at: http://localhost:9900
[silver_spectacle] (use the stop server button to kill it now)



-------------------------------------------------------

 Environment: AntBulletEnv-v0

-------------------------------------------------------




-----------------------------------------------------------------------------------------------------
 Agent Model Exists, loading: subrepos/rl-trained-agents/sac/AntBulletEnv-v0_1/AntBulletEnv-v0.zip
-----------------------------------------------------------------------------------------------------


Wrapping the env with a `Monitor` wrapper
Wrapping the env in a DummyVecEnv.

[silver_spectacle] There were unviewed results
[silver_spectacle] So the display server is still running at: http://localhost:9900
[silver_spectacle] (use the stop server button to kill it now)



-------------------------------------------------------

 Environment: ReacherBulletEnv-v0

-------------------------------------------------------




-----------------------------------------------------------------------------------------------------
 Agent Model Exists, loading: ./subrepos/rl-trained-agents/sac/ReacherBulletEnv-v0_1/ReacherBulletEnv-v0.zip
-----------------------------------------------------------------------------------------------------


Wrapping the env with a `Monitor` wrapper
Wrapping the env in a DummyVecEnv.


-----------------------------------------------------------------------------------------------------
 Training Coach Model from scrach
-----------------------------------------------------------------------------------------------------


coach settings = {
    "force_retrain": False, 
    "learning_rate": 0.0001, 
    "hidden_sizes": [64, 64, 64, 64, ], 
    "loss_function": "state_prediction_loss", 
    "consistent_coach_loss": {"scale_future_state_loss": 1000000, }, 
    "delayed_coach_loss": {"epochs_of_delay": 50, }, 
    "value_plus_state_loss": {"value_proportion": 0.5, }, 
    "number_of_episodes": 100, 
    "number_of_epochs": 100, 
    "train_test_split": 0.7, 
    "batch_size": 128, 
    "with_card": True, 
}
Starting Train/Test
    Epoch: 0
    Epoch: 1
    Epoch: 2
    Epoch: 3
    Epoch: 4
    Epoch: 5
    Epoch: 6
    Epoch: 7
    Epoch: 8
    Epoch: 9
    Epoch: 10
    Epoch: 11
    Epoch: 12
    Epoch: 13
    Epoch: 14
    Epoch: 15
    Epoch: 16
    Epoch: 17
    Epoch: 18
    Epoch: 19
    Epoch: 20
    Epoch: 21
    Epoch: 22
    Epoch: 23
    Epoch: 24
    Epoch: 25
    Epoch: 26
    Epoch: 27
    Epoch: 28
    Epoch: 29
    Epoch: 30
    Epoch: 31
    Epoch: 32
    Epoch: 33
    Epoch: 34
    Epoch: 35
    Epoch: 36
    Epoch: 37
    Epoch: 38
    Epoch: 39
    Epoch: 40
    Epoch: 41
    Epoch: 42
    Epoch: 43
    Epoch: 44
    Epoch: 45
    Epoch: 46
    Epoch: 47
    Epoch: 48
    Epoch: 49
    Epoch: 50
    Epoch: 51
    Epoch: 52
    Epoch: 53
    Epoch: 54
    Epoch: 55
    Epoch: 56
    Epoch: 57
    Epoch: 58
    Epoch: 59
    Epoch: 60
    Epoch: 61
    Epoch: 62
    Epoch: 63
    Epoch: 64
    Epoch: 65
    Epoch: 66
    Epoch: 67
    Epoch: 68
    Epoch: 69
    Epoch: 70
    Epoch: 71
    Epoch: 72
    Epoch: 73
    Epoch: 74
    Epoch: 75
    Epoch: 76
    Epoch: 77
    Epoch: 78
    Epoch: 79
    Epoch: 80
    Epoch: 81
    Epoch: 82
    Epoch: 83
    Epoch: 84
    Epoch: 85
    Epoch: 86
    Epoch: 87
    Epoch: 88
    Epoch: 89
    Epoch: 90
    Epoch: 91
    Epoch: 92
    Epoch: 93
    Epoch: 94
    Epoch: 95
    Epoch: 96
    Epoch: 97
    Epoch: 98
    Epoch: 99
Saving Coach to: models.ignore/coach/ReacherBulletEnv-v0/final_1_90%/


-----------------------------------------------------------------------------------------------------
 Testing Agent+Coach
-----------------------------------------------------------------------------------------------------


test settings = {
    "force_recompute": False, 
    "graph_smoothing": 0, 
    "number_of_episodes_for_baseline": 30, 
    "number_of_epochs_for_optimal_parameters": 15, 
    "number_of_episodes_for_testing": 10, 
    "initial_epsilon": 100.1, 
    "initial_horizon": 16, 
    "reward_discount": 0.98, 
    "acceptable_performance_loss": 1, 
    "acceptable_performance_level": 0.8, 
    "confidence_interval_for_convergence": 90, 
}

[silver_spectacle] There were unviewed results
[silver_spectacle] So the display server is still running at: http://localhost:9900
[silver_spectacle] (use the stop server button to kill it now)



-------------------------------------------------------

 Environment: LunarLanderContinuous-v2

-------------------------------------------------------




-----------------------------------------------------------------------------------------------------
 Agent Model Exists, loading: ./subrepos/rl-trained-agents/sac/LunarLanderContinuous-v2_1/LunarLanderContinuous-v2.zip
-----------------------------------------------------------------------------------------------------


Wrapping the env with a `Monitor` wrapper
Wrapping the env in a DummyVecEnv.


-----------------------------------------------------------------------------------------------------
 Training Coach Model from scrach
-----------------------------------------------------------------------------------------------------


coach settings = {
    "force_retrain": False, 
    "learning_rate": 0.0001, 
    "hidden_sizes": [64, 64, 64, 64, ], 
    "loss_function": "state_prediction_loss", 
    "consistent_coach_loss": {"scale_future_state_loss": 1000000, }, 
    "delayed_coach_loss": {"epochs_of_delay": 50, }, 
    "value_plus_state_loss": {"value_proportion": 0.5, }, 
    "number_of_episodes": 100, 
    "number_of_epochs": 100, 
    "train_test_split": 0.7, 
    "batch_size": 128, 
    "with_card": True, 
}
Starting Train/Test
    Epoch: 0
    Epoch: 1
    Epoch: 2
    Epoch: 3
    Epoch: 4
    Epoch: 5
    Epoch: 6
    Epoch: 7
    Epoch: 8
    Epoch: 9
    Epoch: 10
    Epoch: 11
    Epoch: 12
    Epoch: 13
    Epoch: 14
    Epoch: 15
    Epoch: 16
    Epoch: 17
    Epoch: 18
    Epoch: 19
    Epoch: 20
    Epoch: 21
    Epoch: 22
    Epoch: 23
    Epoch: 24
    Epoch: 25
    Epoch: 26
    Epoch: 27
    Epoch: 28
    Epoch: 29
    Epoch: 30
    Epoch: 31
    Epoch: 32
    Epoch: 33
    Epoch: 34
    Epoch: 35
    Epoch: 36
    Epoch: 37
    Epoch: 38
    Epoch: 39
    Epoch: 40
    Epoch: 41
    Epoch: 42
    Epoch: 43
    Epoch: 44
    Epoch: 45
    Epoch: 46
    Epoch: 47
    Epoch: 48
    Epoch: 49
    Epoch: 50
    Epoch: 51
    Epoch: 52
    Epoch: 53
    Epoch: 54
    Epoch: 55
    Epoch: 56
    Epoch: 57
    Epoch: 58
    Epoch: 59
    Epoch: 60
    Epoch: 61
    Epoch: 62
    Epoch: 63
    Epoch: 64
    Epoch: 65
    Epoch: 66
    Epoch: 67
    Epoch: 68
    Epoch: 69
    Epoch: 70
    Epoch: 71
    Epoch: 72
    Epoch: 73
    Epoch: 74
    Epoch: 75
    Epoch: 76
    Epoch: 77
    Epoch: 78
    Epoch: 79
    Epoch: 80
    Epoch: 81
    Epoch: 82
    Epoch: 83
    Epoch: 84
    Epoch: 85
    Epoch: 86
    Epoch: 87
    Epoch: 88
    Epoch: 89
    Epoch: 90
    Epoch: 91
    Epoch: 92
    Epoch: 93
    Epoch: 94
    Epoch: 95
    Epoch: 96
    Epoch: 97
    Epoch: 98
    Epoch: 99
Saving Coach to: models.ignore/coach/LunarLanderContinuous-v2/final_1_95%/


-----------------------------------------------------------------------------------------------------
 Testing Agent+Coach
-----------------------------------------------------------------------------------------------------


test settings = {
    "force_recompute": False, 
    "graph_smoothing": 0, 
    "number_of_episodes_for_baseline": 30, 
    "number_of_epochs_for_optimal_parameters": 15, 
    "number_of_episodes_for_testing": 10, 
    "initial_epsilon": 100.1, 
    "initial_horizon": 16, 
    "reward_discount": 0.98, 
    "acceptable_performance_loss": 1, 
    "acceptable_performance_level": 0.8, 
    "confidence_interval_for_convergence": 90, 
    "min_reward_single_timestep": -100, 
    "max_reward_single_timestep": 70, 
    "horizons": {0.0: 1, 0.005: 10, 0.01: 20, }, 
}

[silver_spectacle] There were unviewed results
[silver_spectacle] So the display server is still running at: http://localhost:9900
[silver_spectacle] (use the stop server button to kill it now)



-------------------------------------------------------

 Environment: HopperBulletEnv-v0

-------------------------------------------------------




-----------------------------------------------------------------------------------------------------
 Agent Model Exists, loading: ./subrepos/rl-trained-agents/sac/HopperBulletEnv-v0_1/HopperBulletEnv-v0.zip
-----------------------------------------------------------------------------------------------------


Wrapping the env with a `Monitor` wrapper
Wrapping the env in a DummyVecEnv.


-----------------------------------------------------------------------------------------------------
 Training Coach Model from scrach
-----------------------------------------------------------------------------------------------------


coach settings = {
    "force_retrain": False, 
    "learning_rate": 0.0001, 
    "hidden_sizes": [64, 64, 64, 64, ], 
    "loss_function": "state_prediction_loss", 
    "consistent_coach_loss": {"scale_future_state_loss": 1000000, }, 
    "delayed_coach_loss": {"epochs_of_delay": 50, }, 
    "value_plus_state_loss": {"value_proportion": 0.5, }, 
    "number_of_episodes": 100, 
    "number_of_epochs": 100, 
    "train_test_split": 0.7, 
    "batch_size": 128, 
    "with_card": True, 
}
Starting Train/Test
    Epoch: 0
    Epoch: 1
    Epoch: 2
    Epoch: 3
    Epoch: 4
    Epoch: 5
    Epoch: 6
    Epoch: 7
    Epoch: 8
    Epoch: 9
    Epoch: 10
    Epoch: 11
    Epoch: 12
    Epoch: 13
    Epoch: 14
    Epoch: 15
    Epoch: 16
    Epoch: 17
    Epoch: 18
    Epoch: 19
    Epoch: 20
    Epoch: 21
    Epoch: 22
    Epoch: 23
    Epoch: 24
    Epoch: 25
    Epoch: 26
    Epoch: 27
    Epoch: 28
    Epoch: 29
    Epoch: 30
    Epoch: 31
    Epoch: 32
    Epoch: 33
    Epoch: 34
    Epoch: 35
    Epoch: 36
    Epoch: 37
    Epoch: 38
    Epoch: 39
    Epoch: 40
    Epoch: 41
    Epoch: 42
    Epoch: 43
    Epoch: 44
    Epoch: 45
    Epoch: 46
    Epoch: 47
    Epoch: 48
    Epoch: 49
    Epoch: 50
    Epoch: 51
    Epoch: 52
    Epoch: 53
    Epoch: 54
    Epoch: 55
    Epoch: 56
    Epoch: 57
    Epoch: 58
    Epoch: 59
    Epoch: 60
    Epoch: 61
    Epoch: 62
    Epoch: 63
    Epoch: 64
    Epoch: 65
    Epoch: 66
    Epoch: 67
    Epoch: 68
    Epoch: 69
    Epoch: 70
    Epoch: 71
    Epoch: 72
    Epoch: 73
    Epoch: 74
    Epoch: 75
    Epoch: 76
    Epoch: 77
    Epoch: 78
    Epoch: 79
    Epoch: 80
    Epoch: 81
    Epoch: 82
    Epoch: 83
    Epoch: 84
    Epoch: 85
    Epoch: 86
    Epoch: 87
    Epoch: 88
    Epoch: 89
    Epoch: 90
    Epoch: 91
    Epoch: 92
    Epoch: 93
    Epoch: 94
    Epoch: 95
    Epoch: 96
    Epoch: 97
    Epoch: 98
    Epoch: 99
Saving Coach to: models.ignore/coach/HopperBulletEnv-v0/final_1_95%/


-----------------------------------------------------------------------------------------------------
 Testing Agent+Coach
-----------------------------------------------------------------------------------------------------


test settings = {
    "force_recompute": False, 
    "graph_smoothing": 0, 
    "number_of_episodes_for_baseline": 30, 
    "number_of_epochs_for_optimal_parameters": 15, 
    "number_of_episodes_for_testing": 10, 
    "initial_epsilon": 100.1, 
    "initial_horizon": 16, 
    "reward_discount": 0.98, 
    "acceptable_performance_loss": 1, 
    "acceptable_performance_level": 0.8, 
    "confidence_interval_for_convergence": 90, 
    "min_reward_single_timestep": 18, 
    "max_reward_single_timestep": 110, 
    "horizons": {0.1: 26, 0.01: 16, 0.001: 11, }, 
}
baseline = {
    "max": 112.79861896992979, 
    "min": 105.7972408403108, 
    "range": 7.001378129618985, 
    "count": 30, 
    "sum": 3318.192922871891, 
    "average": 110.60643076239636, 
    "stdev": 1.49541705880447, 
    "median": 110.86516527143982, 
}
baseline_min = 110.14252736719435, baseline_max = 111.07033415759842,
baseline_confidence_size = 0.4639033952020384
----- finding optimal epsilon -------------------------------------------------------------------------------------------------------------------------------------
            reward_single_sum=23.782225113603104, 
            reward_single_sum=23.64456158893224, confidence_size=0.43458664371175004, confidence_max=24.147979994979423, new_horizon=2
        episode=0, horizon=32, effective_score=23.71, baseline_lowerbound=88.49 baseline_stdev=0.27, new_epsilon=66.7333, bad=True, gap_average=5.080839347839356
            reward_single_sum=23.32082771063188, 
            reward_single_sum=23.49582789087029, confidence_size=0.5524538265353485, confidence_max=23.960781627286437, new_horizon=2
        episode=1, horizon=32, effective_score=23.41, baseline_lowerbound=88.49 baseline_stdev=0.27, new_epsilon=44.4889, bad=True, gap_average=4.611064529418945
            reward_single_sum=22.509646942616765, 
            reward_single_sum=21.620617791612304, confidence_size=2.8065545744283025, confidence_max=24.871686941542837, new_horizon=10
        episode=2, horizon=32, effective_score=22.07, baseline_lowerbound=88.49 baseline_stdev=0.27, new_epsilon=29.6593, bad=True, gap_average=6.789228338944285
            reward_single_sum=21.744571579364766, 
            reward_single_sum=21.93055710471799, confidence_size=0.5871331962149853, confidence_max=22.424697538256364, new_horizon=16
        episode=3, horizon=32, effective_score=21.84, baseline_lowerbound=88.49 baseline_stdev=0.27, new_epsilon=19.7728, bad=True, gap_average=5.455929003263774
            reward_single_sum=21.325521712481095, 
            reward_single_sum=21.7592339810637, confidence_size=1.369175746375591, confidence_max=22.911553593147985, new_horizon=16
        episode=4, horizon=32, effective_score=21.54, baseline_lowerbound=88.49 baseline_stdev=0.27, new_epsilon=13.1819, bad=True, gap_average=5.836205733449836
            reward_single_sum=22.502120808243788, 
            reward_single_sum=37.124640442184514, confidence_size=46.16147774449985, confidence_max=75.97485836971397, new_horizon=18
        episode=5, horizon=32, effective_score=29.81, baseline_lowerbound=88.49 baseline_stdev=0.27, new_epsilon=8.7879, bad=True, gap_average=3.582458397437786
            reward_single_sum=23.779224017182937, 
            reward_single_sum=21.32656872747355, confidence_size=7.7427280253435855, confidence_max=30.295624397671823, new_horizon=16
        episode=6, horizon=32, effective_score=22.55, baseline_lowerbound=88.49 baseline_stdev=0.27, new_epsilon=5.8586, bad=True, gap_average=5.106674920944941
            reward_single_sum=50.06952550144251, 
            reward_single_sum=23.035646135029882, confidence_size=85.34259840031675, confidence_max=121.8951842185529, new_horizon=16
            reward_single_sum=21.98194350933737, confidence_size=26.84034538258848, confidence_max=58.536050431191725, new_horizon=16
        episode=7, horizon=32, effective_score=31.70, baseline_lowerbound=88.49 baseline_stdev=0.27, new_epsilon=3.9057, bad=True, gap_average=2.5657552493515836
            reward_single_sum=21.898631338307258, 
            reward_single_sum=21.673436921575643, confidence_size=0.710910794881972, confidence_max=22.49694492482342, new_horizon=16
        episode=8, horizon=32, effective_score=21.79, baseline_lowerbound=88.49 baseline_stdev=0.27, new_epsilon=2.6038, bad=True, gap_average=5.883469475640191
            reward_single_sum=82.53342527217553, 
            reward_single_sum=93.29120937493742, confidence_size=33.96098783735715, confidence_max=121.87330516091362, new_horizon=10
            reward_single_sum=23.364151632165647, confidence_size=63.47761163472663, confidence_max=129.87387372781947, new_horizon=10
            reward_single_sum=74.81699494146943, confidence_size=36.51311380488407, confidence_max=105.01455911007108, new_horizon=8
        episode=9, horizon=32, effective_score=68.50, baseline_lowerbound=88.49 baseline_stdev=0.27, new_epsilon=1.7359, bad=True, gap_average=1.4035652375352976
            reward_single_sum=94.18867538396988, 
            reward_single_sum=102.44124472291253, confidence_size=26.052336082374453, confidence_max=124.36729613581565, new_horizon=8
            reward_single_sum=95.41348214075806, confidence_size=7.507717057069591, confidence_max=104.85551780628307, new_horizon=8
        episode=10, horizon=8, effective_score=97.35, baseline_lowerbound=88.49 baseline_stdev=0.27, new_epsilon=2.6038, bad=False, gap_average=1.080256915382339
            reward_single_sum=22.072393869638496, 
            reward_single_sum=55.6489842220626, confidence_size=105.9971240997341, confidence_max=144.8578131455846, new_horizon=8
            reward_single_sum=101.66423659839562, confidence_size=67.3626753982179, confidence_max=127.1578802949168, new_horizon=8
            reward_single_sum=99.79994909822821, confidence_size=45.030191884783406, confidence_max=114.82658283186463, new_horizon=8
            reward_single_sum=102.72608344974766, confidence_size=34.576010878560716, confidence_max=110.95834032617523, new_horizon=8
            reward_single_sum=21.835904703344955, confidence_size=32.367349420382, confidence_max=99.65860807728492, new_horizon=8
        episode=11, horizon=8, effective_score=67.29, baseline_lowerbound=88.49 baseline_stdev=0.27, new_epsilon=1.7359, bad=True, gap_average=1.7133883739916995
            reward_single_sum=102.13413013566766, 
            reward_single_sum=98.1303384215545, confidence_size=12.639472999964703, confidence_max=112.77170727857578, new_horizon=8
            reward_single_sum=22.528936102281957, confidence_size=75.60886319548045, confidence_max=149.87333141531514, new_horizon=8
            reward_single_sum=106.74240145987088, confidence_size=47.135737120871056, confidence_max=129.51968865071478, new_horizon=8
            reward_single_sum=103.51188951299652, confidence_size=34.27929752343215, confidence_max=120.88883664990644, new_horizon=8
            reward_single_sum=104.02115293599304, confidence_size=27.094031834571574, confidence_max=116.60550659596566, new_horizon=8
            reward_single_sum=100.27335099240653, confidence_size=22.283111473075323, confidence_max=113.33199712461405, new_horizon=8
            reward_single_sum=104.85389832277409, confidence_size=19.09700691267168, confidence_max=111.87151914811481, new_horizon=8
            reward_single_sum=107.35209092318343, confidence_size=16.802729293843484, confidence_max=111.19697249459111, new_horizon=8
            reward_single_sum=22.24323313965046, confidence_size=19.859981230636322, confidence_max=107.03912342527423, new_horizon=8
        episode=12, horizon=32, effective_score=87.18, baseline_lowerbound=88.49 baseline_stdev=0.27, new_epsilon=1.1573, bad=True, gap_average=1.1315039532858957
            reward_single_sum=106.23976559597921, 
            reward_single_sum=103.01842578408841, confidence_size=10.16936955850705, confidence_max=114.79846524854085, new_horizon=6
            reward_single_sum=104.05371580305429, confidence_size=2.772506216953687, confidence_max=107.20980861132766, new_horizon=6
        episode=13, horizon=32, effective_score=104.44, baseline_lowerbound=88.49 baseline_stdev=0.27, new_epsilon=1.7359, bad=False, gap_average=0.9696263436698638
            reward_single_sum=102.47139380233143, 
            reward_single_sum=101.24440558955038, confidence_size=3.873449343544628, confidence_max=105.73134903948554, new_horizon=8
        episode=14, horizon=32, effective_score=101.86, baseline_lowerbound=88.49 baseline_stdev=0.27, new_epsilon=2.6038, bad=False, gap_average=1.0973182132749846
optimal_epsilon = 3.9057460752934006
optimal_horizon = 16
    scaled_epsilon: 3.9057, forecast_average: 4.8796, episode_reward:438.94, max_timestep_reward: 5.16, min_timestep_reward: -0.81
argv[0]=
argv[0]=

[silver_spectacle] There were unviewed results
[silver_spectacle] So the display server is still running at: http://localhost:9900
[silver_spectacle] (use the stop server button to kill it now)



-------------------------------------------------------

 Environment: HalfCheetahBulletEnv-v0

-------------------------------------------------------




-----------------------------------------------------------------------------------------------------
 Agent Model Exists, loading: subrepos/rl-trained-agents/sac/HalfCheetahBulletEnv-v0_1/HalfCheetahBulletEnv-v0.zip
-----------------------------------------------------------------------------------------------------


Wrapping the env with a `Monitor` wrapper
Wrapping the env in a DummyVecEnv.

[silver_spectacle] There were unviewed results
[silver_spectacle] So the display server is still running at: http://localhost:9900
[silver_spectacle] (use the stop server button to kill it now)



-------------------------------------------------------

 Environment: BipedalWalker-v3

-------------------------------------------------------




-----------------------------------------------------------------------------------------------------
 Agent Model Exists, loading: ./subrepos/rl-trained-agents/sac/Walker2d-v3_1/Walker2d-v3.zip
-----------------------------------------------------------------------------------------------------


Wrapping the env with a `Monitor` wrapper
Wrapping the env in a DummyVecEnv.

[silver_spectacle] There were unviewed results
[silver_spectacle] So the display server is still running at: http://localhost:9900
[silver_spectacle] (use the stop server button to kill it now)



-------------------------------------------------------

 Environment: AntBulletEnv-v0

-------------------------------------------------------




-----------------------------------------------------------------------------------------------------
 Agent Model Exists, loading: subrepos/rl-trained-agents/sac/AntBulletEnv-v0_1/AntBulletEnv-v0.zip
-----------------------------------------------------------------------------------------------------


Wrapping the env with a `Monitor` wrapper
Wrapping the env in a DummyVecEnv.

[silver_spectacle] There were unviewed results
[silver_spectacle] So the display server is still running at: http://localhost:9900
[silver_spectacle] (use the stop server button to kill it now)



-------------------------------------------------------

 Environment: ReacherBulletEnv-v0

-------------------------------------------------------




-----------------------------------------------------------------------------------------------------
 Agent Model Exists, loading: ./subrepos/rl-trained-agents/sac/ReacherBulletEnv-v0_1/ReacherBulletEnv-v0.zip
-----------------------------------------------------------------------------------------------------


Wrapping the env with a `Monitor` wrapper
Wrapping the env in a DummyVecEnv.


-----------------------------------------------------------------------------------------------------
 Training Coach Model from scrach
-----------------------------------------------------------------------------------------------------


coach settings = {
    "force_retrain": False, 
    "learning_rate": 0.0001, 
    "hidden_sizes": [64, 64, 64, 64, ], 
    "loss_function": "state_prediction_loss", 
    "consistent_coach_loss": {"scale_future_state_loss": 1000000, }, 
    "delayed_coach_loss": {"epochs_of_delay": 50, }, 
    "value_plus_state_loss": {"value_proportion": 0.5, }, 
    "number_of_episodes": 100, 
    "number_of_epochs": 100, 
    "train_test_split": 0.7, 
    "batch_size": 128, 
    "with_card": True, 
}
Starting Train/Test
    Epoch: 0
    Epoch: 1
    Epoch: 2
    Epoch: 3
    Epoch: 4
    Epoch: 5
    Epoch: 6
    Epoch: 7
    Epoch: 8
    Epoch: 9
    Epoch: 10
    Epoch: 11
    Epoch: 12
    Epoch: 13
    Epoch: 14
    Epoch: 15
    Epoch: 16
    Epoch: 17
    Epoch: 18
    Epoch: 19
    Epoch: 20
    Epoch: 21
    Epoch: 22
    Epoch: 23
    Epoch: 24
    Epoch: 25
    Epoch: 26
    Epoch: 27
    Epoch: 28
    Epoch: 29
    Epoch: 30
    Epoch: 31
    Epoch: 32
    Epoch: 33
    Epoch: 34
    Epoch: 35
    Epoch: 36
    Epoch: 37
    Epoch: 38
    Epoch: 39
    Epoch: 40
    Epoch: 41
    Epoch: 42
    Epoch: 43
    Epoch: 44
    Epoch: 45
    Epoch: 46
    Epoch: 47
    Epoch: 48
    Epoch: 49
    Epoch: 50
    Epoch: 51
    Epoch: 52
    Epoch: 53
    Epoch: 54
    Epoch: 55
    Epoch: 56
    Epoch: 57
    Epoch: 58
    Epoch: 59
    Epoch: 60
    Epoch: 61
    Epoch: 62
    Epoch: 63
    Epoch: 64
    Epoch: 65
    Epoch: 66
    Epoch: 67
    Epoch: 68
    Epoch: 69
    Epoch: 70
    Epoch: 71
    Epoch: 72
    Epoch: 73
    Epoch: 74
    Epoch: 75
    Epoch: 76
    Epoch: 77
    Epoch: 78
    Epoch: 79
    Epoch: 80
    Epoch: 81
    Epoch: 82
    Epoch: 83
    Epoch: 84
    Epoch: 85
    Epoch: 86
    Epoch: 87
    Epoch: 88
    Epoch: 89
    Epoch: 90
    Epoch: 91
    Epoch: 92
    Epoch: 93
    Epoch: 94
    Epoch: 95
    Epoch: 96
    Epoch: 97
    Epoch: 98
    Epoch: 99
Saving Coach to: models.ignore/coach/ReacherBulletEnv-v0/final_1_95%/


-----------------------------------------------------------------------------------------------------
 Testing Agent+Coach
-----------------------------------------------------------------------------------------------------


test settings = {
    "force_recompute": False, 
    "graph_smoothing": 0, 
    "number_of_episodes_for_baseline": 30, 
    "number_of_epochs_for_optimal_parameters": 15, 
    "number_of_episodes_for_testing": 10, 
    "initial_epsilon": 100.1, 
    "initial_horizon": 16, 
    "reward_discount": 0.98, 
    "acceptable_performance_loss": 1, 
    "acceptable_performance_level": 0.8, 
    "confidence_interval_for_convergence": 90, 
}

[silver_spectacle] There were unviewed results
[silver_spectacle] So the display server is still running at: http://localhost:9900
[silver_spectacle] (use the stop server button to kill it now)



-------------------------------------------------------

 Environment: LunarLanderContinuous-v2

-------------------------------------------------------




-----------------------------------------------------------------------------------------------------
 Agent Model Exists, loading: ./subrepos/rl-trained-agents/sac/LunarLanderContinuous-v2_1/LunarLanderContinuous-v2.zip
-----------------------------------------------------------------------------------------------------


Wrapping the env with a `Monitor` wrapper
Wrapping the env in a DummyVecEnv.


-----------------------------------------------------------------------------------------------------
 Training Coach Model from scrach
-----------------------------------------------------------------------------------------------------


coach settings = {
    "force_retrain": False, 
    "learning_rate": 0.0001, 
    "hidden_sizes": [64, 64, 64, 64, ], 
    "loss_function": "state_prediction_loss", 
    "consistent_coach_loss": {"scale_future_state_loss": 1000000, }, 
    "delayed_coach_loss": {"epochs_of_delay": 50, }, 
    "value_plus_state_loss": {"value_proportion": 0.5, }, 
    "number_of_episodes": 100, 
    "number_of_epochs": 100, 
    "train_test_split": 0.7, 
    "batch_size": 128, 
    "with_card": True, 
}
Starting Train/Test
    Epoch: 0
    Epoch: 1
    Epoch: 2
    Epoch: 3
    Epoch: 4
    Epoch: 5
    Epoch: 6
    Epoch: 7
    Epoch: 8
    Epoch: 9
    Epoch: 10
    Epoch: 11
    Epoch: 12
    Epoch: 13
    Epoch: 14
    Epoch: 15
    Epoch: 16
    Epoch: 17
    Epoch: 18
    Epoch: 19
    Epoch: 20
    Epoch: 21
    Epoch: 22
    Epoch: 23
    Epoch: 24
    Epoch: 25
    Epoch: 26
    Epoch: 27
    Epoch: 28
    Epoch: 29
    Epoch: 30
    Epoch: 31
    Epoch: 32
    Epoch: 33
    Epoch: 34
    Epoch: 35
    Epoch: 36
    Epoch: 37
    Epoch: 38
    Epoch: 39
    Epoch: 40
    Epoch: 41
    Epoch: 42
    Epoch: 43
    Epoch: 44
    Epoch: 45
    Epoch: 46
    Epoch: 47
    Epoch: 48
    Epoch: 49
    Epoch: 50
    Epoch: 51
    Epoch: 52
    Epoch: 53
    Epoch: 54
    Epoch: 55
    Epoch: 56
    Epoch: 57
    Epoch: 58
    Epoch: 59
    Epoch: 60
    Epoch: 61
    Epoch: 62
    Epoch: 63
    Epoch: 64
    Epoch: 65
    Epoch: 66
    Epoch: 67
    Epoch: 68
    Epoch: 69
    Epoch: 70
    Epoch: 71
    Epoch: 72
    Epoch: 73
    Epoch: 74
    Epoch: 75
    Epoch: 76
    Epoch: 77
    Epoch: 78
    Epoch: 79
    Epoch: 80
    Epoch: 81
    Epoch: 82
    Epoch: 83
    Epoch: 84
    Epoch: 85
    Epoch: 86
    Epoch: 87
    Epoch: 88
    Epoch: 89
    Epoch: 90
    Epoch: 91
    Epoch: 92
    Epoch: 93
    Epoch: 94
    Epoch: 95
    Epoch: 96
    Epoch: 97
    Epoch: 98
    Epoch: 99
Saving Coach to: models.ignore/coach/LunarLanderContinuous-v2/final_1_98%/


-----------------------------------------------------------------------------------------------------
 Testing Agent+Coach
-----------------------------------------------------------------------------------------------------


test settings = {
    "force_recompute": False, 
    "graph_smoothing": 0, 
    "number_of_episodes_for_baseline": 30, 
    "number_of_epochs_for_optimal_parameters": 15, 
    "number_of_episodes_for_testing": 10, 
    "initial_epsilon": 100.1, 
    "initial_horizon": 16, 
    "reward_discount": 0.98, 
    "acceptable_performance_loss": 1, 
    "acceptable_performance_level": 0.8, 
    "confidence_interval_for_convergence": 90, 
    "min_reward_single_timestep": -100, 
    "max_reward_single_timestep": 70, 
    "horizons": {0.0: 1, 0.005: 10, 0.01: 20, }, 
}

[silver_spectacle] There were unviewed results
[silver_spectacle] So the display server is still running at: http://localhost:9900
[silver_spectacle] (use the stop server button to kill it now)



-------------------------------------------------------

 Environment: HopperBulletEnv-v0

-------------------------------------------------------




-----------------------------------------------------------------------------------------------------
 Agent Model Exists, loading: ./subrepos/rl-trained-agents/sac/HopperBulletEnv-v0_1/HopperBulletEnv-v0.zip
-----------------------------------------------------------------------------------------------------


Wrapping the env with a `Monitor` wrapper
Wrapping the env in a DummyVecEnv.


-----------------------------------------------------------------------------------------------------
 Training Coach Model from scrach
-----------------------------------------------------------------------------------------------------


coach settings = {
    "force_retrain": False, 
    "learning_rate": 0.0001, 
    "hidden_sizes": [64, 64, 64, 64, ], 
    "loss_function": "state_prediction_loss", 
    "consistent_coach_loss": {"scale_future_state_loss": 1000000, }, 
    "delayed_coach_loss": {"epochs_of_delay": 50, }, 
    "value_plus_state_loss": {"value_proportion": 0.5, }, 
    "number_of_episodes": 100, 
    "number_of_epochs": 100, 
    "train_test_split": 0.7, 
    "batch_size": 128, 
    "with_card": True, 
}
Starting Train/Test
    Epoch: 0
    Epoch: 1
    Epoch: 2
    Epoch: 3
    Epoch: 4
    Epoch: 5
    Epoch: 6
    Epoch: 7
    Epoch: 8
    Epoch: 9
    Epoch: 10
    Epoch: 11
    Epoch: 12
    Epoch: 13
    Epoch: 14
    Epoch: 15
    Epoch: 16
    Epoch: 17
    Epoch: 18
    Epoch: 19
    Epoch: 20
    Epoch: 21
    Epoch: 22
    Epoch: 23
    Epoch: 24
    Epoch: 25
    Epoch: 26
    Epoch: 27
    Epoch: 28
    Epoch: 29
    Epoch: 30
    Epoch: 31
    Epoch: 32
    Epoch: 33
    Epoch: 34
    Epoch: 35
    Epoch: 36
    Epoch: 37
    Epoch: 38
    Epoch: 39
    Epoch: 40
    Epoch: 41
    Epoch: 42
    Epoch: 43
    Epoch: 44
    Epoch: 45
    Epoch: 46
    Epoch: 47
    Epoch: 48
    Epoch: 49
    Epoch: 50
    Epoch: 51
    Epoch: 52
    Epoch: 53
    Epoch: 54
    Epoch: 55
    Epoch: 56
    Epoch: 57
    Epoch: 58
    Epoch: 59
    Epoch: 60
    Epoch: 61
    Epoch: 62
    Epoch: 63
    Epoch: 64
    Epoch: 65
    Epoch: 66
    Epoch: 67
    Epoch: 68
    Epoch: 69
    Epoch: 70
    Epoch: 71
    Epoch: 72
    Epoch: 73
    Epoch: 74
    Epoch: 75
    Epoch: 76
    Epoch: 77
    Epoch: 78
    Epoch: 79
    Epoch: 80
    Epoch: 81
    Epoch: 82
    Epoch: 83
    Epoch: 84
    Epoch: 85
    Epoch: 86
    Epoch: 87
    Epoch: 88
    Epoch: 89
    Epoch: 90
    Epoch: 91
    Epoch: 92
    Epoch: 93
    Epoch: 94
    Epoch: 95
    Epoch: 96
    Epoch: 97
    Epoch: 98
    Epoch: 99
Saving Coach to: models.ignore/coach/HopperBulletEnv-v0/final_1_98%/


-----------------------------------------------------------------------------------------------------
 Testing Agent+Coach
-----------------------------------------------------------------------------------------------------


test settings = {
    "force_recompute": False, 
    "graph_smoothing": 0, 
    "number_of_episodes_for_baseline": 30, 
    "number_of_epochs_for_optimal_parameters": 15, 
    "number_of_episodes_for_testing": 10, 
    "initial_epsilon": 100.1, 
    "initial_horizon": 16, 
    "reward_discount": 0.98, 
    "acceptable_performance_loss": 1, 
    "acceptable_performance_level": 0.8, 
    "confidence_interval_for_convergence": 90, 
    "min_reward_single_timestep": 18, 
    "max_reward_single_timestep": 110, 
    "horizons": {0.1: 26, 0.01: 16, 0.001: 11, }, 
}
baseline = {
    "max": 112.79861896992979, 
    "min": 105.7972408403108, 
    "range": 7.001378129618985, 
    "count": 30, 
    "sum": 3318.192922871891, 
    "average": 110.60643076239636, 
    "stdev": 1.49541705880447, 
    "median": 110.86516527143982, 
}
baseline_min = 110.14252736719435, baseline_max = 111.07033415759842,
baseline_confidence_size = 0.4639033952020384
----- finding optimal epsilon -------------------------------------------------------------------------------------------------------------------------------------
            reward_single_sum=27.036228205379825, 
            reward_single_sum=23.778020348040336, confidence_size=10.285757397406753, confidence_max=35.692881674116826, new_horizon=2
        episode=0, horizon=32, effective_score=25.41, baseline_lowerbound=88.49 baseline_stdev=0.27, new_epsilon=66.7333, bad=True, gap_average=7.773102470066236
            reward_single_sum=23.934355173143025, 
            reward_single_sum=24.946014443909483, confidence_size=3.193682626632066, confidence_max=27.63386743515832, new_horizon=2
        episode=1, horizon=32, effective_score=24.44, baseline_lowerbound=88.49 baseline_stdev=0.27, new_epsilon=44.4889, bad=True, gap_average=5.370216006324405
            reward_single_sum=21.762176589185206, 
            reward_single_sum=22.461443211371524, confidence_size=2.2074978475392992, confidence_max=24.319307747817664, new_horizon=18
        episode=2, horizon=32, effective_score=22.11, baseline_lowerbound=88.49 baseline_stdev=0.27, new_epsilon=29.6593, bad=True, gap_average=6.233685266403925
            reward_single_sum=22.338478654126526, 
            reward_single_sum=21.725078355097125, confidence_size=1.9364285335881135, confidence_max=23.96820703819994, new_horizon=18
        episode=3, horizon=32, effective_score=22.03, baseline_lowerbound=88.49 baseline_stdev=0.27, new_epsilon=19.7728, bad=True, gap_average=5.94769172668457
            reward_single_sum=23.23053640251415, 
            reward_single_sum=22.344806158620912, confidence_size=2.7961403345429687, confidence_max=25.5838116151105, new_horizon=18
        episode=4, horizon=32, effective_score=22.79, baseline_lowerbound=88.49 baseline_stdev=0.27, new_epsilon=13.1819, bad=True, gap_average=4.235674771395597
            reward_single_sum=21.885696647740488, 
            reward_single_sum=21.87566536552057, confidence_size=0.03166751165569792, confidence_max=21.91234851828623, new_horizon=18
        episode=5, horizon=32, effective_score=21.88, baseline_lowerbound=88.49 baseline_stdev=0.27, new_epsilon=8.7879, bad=True, gap_average=5.683498764038086
            reward_single_sum=22.598292816969312, 
            reward_single_sum=21.538707979458735, confidence_size=3.3449776864462546, confidence_max=25.413478084660277, new_horizon=18
        episode=6, horizon=32, effective_score=22.07, baseline_lowerbound=88.49 baseline_stdev=0.27, new_epsilon=5.8586, bad=True, gap_average=6.352903820219494
            reward_single_sum=22.259802924137738, 
            reward_single_sum=33.074537399103185, confidence_size=34.14077308674149, confidence_max=61.80794324836194, new_horizon=18
        episode=7, horizon=32, effective_score=27.67, baseline_lowerbound=88.49 baseline_stdev=0.27, new_epsilon=3.9057, bad=True, gap_average=3.857447306315104
            reward_single_sum=80.91878981902786, 
            reward_single_sum=21.701151381974665, confidence_size=186.94272719243938, confidence_max=238.25269779294052, new_horizon=8
            reward_single_sum=22.37080783833615, confidence_size=57.315098599831245, confidence_max=98.97868161294411, new_horizon=8
        episode=8, horizon=32, effective_score=41.66, baseline_lowerbound=88.49 baseline_stdev=0.27, new_epsilon=2.6038, bad=True, gap_average=2.1632950926480228
            reward_single_sum=22.41999559817986, 
            reward_single_sum=23.32979669901228, confidence_size=2.87212903927413, confidence_max=25.7470251878702, new_horizon=10
        episode=9, horizon=32, effective_score=22.87, baseline_lowerbound=88.49 baseline_stdev=0.27, new_epsilon=1.7359, bad=True, gap_average=1.422914777483259
            reward_single_sum=103.58076076077451, 
            reward_single_sum=105.06124576446157, confidence_size=4.673707217334638, confidence_max=108.99471047995267, new_horizon=6
        episode=10, horizon=10, effective_score=104.32, baseline_lowerbound=88.49 baseline_stdev=0.27, new_epsilon=2.6038, bad=False, gap_average=1.1085679918632911
            reward_single_sum=95.8096303693312, 
            reward_single_sum=90.20151440108769, confidence_size=17.70412534483839, confidence_max=110.70969773004782, new_horizon=10
            reward_single_sum=104.59164709807004, confidence_size=12.227786119215658, confidence_max=109.09538340871197, new_horizon=10
        episode=11, horizon=8, effective_score=96.87, baseline_lowerbound=88.49 baseline_stdev=0.27, new_epsilon=3.9057, bad=False, gap_average=1.3286022143577463
            reward_single_sum=85.22543354392198, 
            reward_single_sum=93.92594489706465, confidence_size=27.466433367723653, confidence_max=117.04212258821696, new_horizon=8
            reward_single_sum=89.62200550753221, confidence_size=7.334036491650949, confidence_max=96.92516447449056, new_horizon=8
        episode=12, horizon=18, effective_score=89.59, baseline_lowerbound=88.49 baseline_stdev=0.27, new_epsilon=5.8586, bad=False, gap_average=1.9248962532095357
            reward_single_sum=22.716777920904377, 
            reward_single_sum=66.64377100024979, confidence_size=138.67205954768363, confidence_max=183.35233400826064, new_horizon=7
            reward_single_sum=23.802589287773255, confidence_size=42.236887732102275, confidence_max=79.95793380174473, new_horizon=7
        episode=13, horizon=8, effective_score=37.72, baseline_lowerbound=88.49 baseline_stdev=0.27, new_epsilon=3.9057, bad=True, gap_average=2.8745407118941797
            reward_single_sum=92.5726001497231, 
            reward_single_sum=21.881778063538164, confidence_size=223.1621425145868, confidence_max=280.3893316212173, new_horizon=8
            reward_single_sum=21.95689126105877, confidence_size=68.76886795623722, confidence_max=114.23929111434387, new_horizon=8
            reward_single_sum=22.70404480605361, confidence_size=41.41661360281282, confidence_max=81.19544217290621, new_horizon=8
        episode=14, horizon=10, effective_score=39.78, baseline_lowerbound=88.49 baseline_stdev=0.27, new_epsilon=2.6038, bad=True, gap_average=1.8292110146784255
optimal_epsilon = 5.858619112940101
optimal_horizon = 7
    scaled_epsilon: 5.8586, forecast_average: 2.0000, episode_reward:23.80, max_timestep_reward: 3.39, min_timestep_reward: -1.12
    scaled_epsilon: 5.8586, forecast_average: 3.0000, episode_reward:24.95, max_timestep_reward: 3.22, min_timestep_reward: -0.18
    scaled_epsilon: 5.8586, forecast_average: 3.1600, episode_reward:328.97, max_timestep_reward: 4.41, min_timestep_reward: -1.89
    scaled_epsilon: 5.8586, forecast_average: 3.4950, episode_reward:24.59, max_timestep_reward: 3.06, min_timestep_reward: -2.57
    scaled_epsilon: 5.8586, forecast_average: 3.3960, episode_reward:22.78, max_timestep_reward: 3.66, min_timestep_reward: -1.38
    scaled_epsilon: 5.8586, forecast_average: 3.4967, episode_reward:23.34, max_timestep_reward: 3.38, min_timestep_reward: -2.46
    scaled_epsilon: 5.8586, forecast_average: 3.5183, episode_reward:318.57, max_timestep_reward: 5.76, min_timestep_reward: -2.05
    scaled_epsilon: 5.8586, forecast_average: 3.5223, episode_reward:457.98, max_timestep_reward: 3.99, min_timestep_reward: -0.69
    scaled_epsilon: 5.8586, forecast_average: 3.4780, episode_reward:198.68, max_timestep_reward: 4.74, min_timestep_reward: -0.14
    scaled_epsilon: 5.8586, forecast_average: 3.4302, episode_reward:23.35, max_timestep_reward: 3.60, min_timestep_reward: -0.30
argv[0]=
argv[0]=


-------------------------------------------------------

 Environment: HalfCheetahBulletEnv-v0

-------------------------------------------------------




-----------------------------------------------------------------------------------------------------
 Agent Model Exists, loading: subrepos/rl-trained-agents/sac/HalfCheetahBulletEnv-v0_1/HalfCheetahBulletEnv-v0.zip
-----------------------------------------------------------------------------------------------------


Wrapping the env with a `Monitor` wrapper
Wrapping the env in a DummyVecEnv.


-------------------------------------------------------

 Environment: BipedalWalker-v3

-------------------------------------------------------




-----------------------------------------------------------------------------------------------------
 Agent Model Exists, loading: ./subrepos/rl-trained-agents/sac/Walker2d-v3_1/Walker2d-v3.zip
-----------------------------------------------------------------------------------------------------


Wrapping the env with a `Monitor` wrapper
Wrapping the env in a DummyVecEnv.


-------------------------------------------------------

 Environment: AntBulletEnv-v0

-------------------------------------------------------




-----------------------------------------------------------------------------------------------------
 Agent Model Exists, loading: subrepos/rl-trained-agents/sac/AntBulletEnv-v0_1/AntBulletEnv-v0.zip
-----------------------------------------------------------------------------------------------------


Wrapping the env with a `Monitor` wrapper
Wrapping the env in a DummyVecEnv.


-------------------------------------------------------

 Environment: ReacherBulletEnv-v0

-------------------------------------------------------




-----------------------------------------------------------------------------------------------------
 Agent Model Exists, loading: ./subrepos/rl-trained-agents/sac/ReacherBulletEnv-v0_1/ReacherBulletEnv-v0.zip
-----------------------------------------------------------------------------------------------------


Wrapping the env with a `Monitor` wrapper
Wrapping the env in a DummyVecEnv.


-----------------------------------------------------------------------------------------------------
 Training Coach Model from scrach
-----------------------------------------------------------------------------------------------------


coach settings = {
    "force_retrain": False, 
    "learning_rate": 0.0001, 
    "hidden_sizes": [64, 64, 64, 64, ], 
    "loss_function": "state_prediction_loss", 
    "consistent_coach_loss": {"scale_future_state_loss": 1000000, }, 
    "delayed_coach_loss": {"epochs_of_delay": 50, }, 
    "value_plus_state_loss": {"value_proportion": 0.5, }, 
    "number_of_episodes": 100, 
    "number_of_epochs": 100, 
    "train_test_split": 0.7, 
    "batch_size": 128, 
    "with_card": True, 
}
Starting Train/Test
    Epoch: 0
    Epoch: 1
    Epoch: 2
    Epoch: 3
    Epoch: 4
    Epoch: 5
    Epoch: 6
    Epoch: 7
    Epoch: 8
    Epoch: 9
    Epoch: 10
    Epoch: 11
    Epoch: 12
    Epoch: 13
    Epoch: 14
    Epoch: 15
    Epoch: 16
    Epoch: 17
    Epoch: 18
    Epoch: 19
    Epoch: 20
    Epoch: 21
    Epoch: 22
    Epoch: 23
    Epoch: 24
    Epoch: 25
    Epoch: 26
    Epoch: 27
    Epoch: 28
    Epoch: 29
    Epoch: 30
    Epoch: 31
    Epoch: 32
    Epoch: 33
    Epoch: 34
    Epoch: 35
    Epoch: 36
    Epoch: 37
    Epoch: 38
    Epoch: 39
    Epoch: 40
    Epoch: 41
    Epoch: 42
    Epoch: 43
    Epoch: 44
    Epoch: 45
    Epoch: 46
    Epoch: 47
    Epoch: 48
    Epoch: 49
    Epoch: 50
    Epoch: 51
    Epoch: 52
    Epoch: 53
    Epoch: 54
    Epoch: 55
    Epoch: 56
    Epoch: 57
    Epoch: 58
    Epoch: 59
    Epoch: 60
    Epoch: 61
    Epoch: 62
    Epoch: 63
    Epoch: 64
    Epoch: 65
    Epoch: 66
    Epoch: 67
    Epoch: 68
    Epoch: 69
    Epoch: 70
    Epoch: 71
    Epoch: 72
    Epoch: 73
    Epoch: 74
    Epoch: 75
    Epoch: 76
    Epoch: 77
    Epoch: 78
    Epoch: 79
    Epoch: 80
    Epoch: 81
    Epoch: 82
    Epoch: 83
    Epoch: 84
    Epoch: 85
    Epoch: 86
    Epoch: 87
    Epoch: 88
    Epoch: 89
    Epoch: 90
    Epoch: 91
    Epoch: 92
    Epoch: 93
    Epoch: 94
    Epoch: 95
    Epoch: 96
    Epoch: 97
    Epoch: 98
    Epoch: 99
Saving Coach to: models.ignore/coach/ReacherBulletEnv-v0/final_1_98%/


-----------------------------------------------------------------------------------------------------
 Testing Agent+Coach
-----------------------------------------------------------------------------------------------------


test settings = {
    "force_recompute": False, 
    "graph_smoothing": 0, 
    "number_of_episodes_for_baseline": 30, 
    "number_of_epochs_for_optimal_parameters": 15, 
    "number_of_episodes_for_testing": 10, 
    "initial_epsilon": 100.1, 
    "initial_horizon": 16, 
    "reward_discount": 0.98, 
    "acceptable_performance_loss": 1, 
    "acceptable_performance_level": 0.8, 
    "confidence_interval_for_convergence": 90, 
}

[silver_spectacle] There were unviewed results
[silver_spectacle] So the display server is still running at: http://localhost:9900
[silver_spectacle] (use the stop server button to kill it now)



-------------------------------------------------------

 Environment: LunarLanderContinuous-v2

-------------------------------------------------------




-----------------------------------------------------------------------------------------------------
 Agent Model Exists, loading: ./subrepos/rl-trained-agents/sac/LunarLanderContinuous-v2_1/LunarLanderContinuous-v2.zip
-----------------------------------------------------------------------------------------------------


Wrapping the env with a `Monitor` wrapper
Wrapping the env in a DummyVecEnv.


-----------------------------------------------------------------------------------------------------
 Training Coach Model from scrach
-----------------------------------------------------------------------------------------------------


coach settings = {
    "force_retrain": False, 
    "learning_rate": 0.0001, 
    "hidden_sizes": [64, 64, 64, 64, ], 
    "loss_function": "state_prediction_loss", 
    "consistent_coach_loss": {"scale_future_state_loss": 1000000, }, 
    "delayed_coach_loss": {"epochs_of_delay": 50, }, 
    "value_plus_state_loss": {"value_proportion": 0.5, }, 
    "number_of_episodes": 100, 
    "number_of_epochs": 100, 
    "train_test_split": 0.7, 
    "batch_size": 128, 
    "with_card": True, 
}
Starting Train/Test
    Epoch: 0
    Epoch: 1
    Epoch: 2
    Epoch: 3
    Epoch: 4
    Epoch: 5
    Epoch: 6
    Epoch: 7
    Epoch: 8
    Epoch: 9
    Epoch: 10
    Epoch: 11
    Epoch: 12
    Epoch: 13
    Epoch: 14
    Epoch: 15
    Epoch: 16
    Epoch: 17
    Epoch: 18
    Epoch: 19
    Epoch: 20
    Epoch: 21
    Epoch: 22
    Epoch: 23
    Epoch: 24
    Epoch: 25
    Epoch: 26
    Epoch: 27
    Epoch: 28
    Epoch: 29
    Epoch: 30
    Epoch: 31
    Epoch: 32
    Epoch: 33
    Epoch: 34
    Epoch: 35
    Epoch: 36
    Epoch: 37
    Epoch: 38
    Epoch: 39
    Epoch: 40
    Epoch: 41
    Epoch: 42
    Epoch: 43
    Epoch: 44
    Epoch: 45
    Epoch: 46
    Epoch: 47
    Epoch: 48
    Epoch: 49
    Epoch: 50
    Epoch: 51
    Epoch: 52
    Epoch: 53
    Epoch: 54
    Epoch: 55
    Epoch: 56
    Epoch: 57
    Epoch: 58
    Epoch: 59
    Epoch: 60
    Epoch: 61
    Epoch: 62
    Epoch: 63
    Epoch: 64
    Epoch: 65
    Epoch: 66
    Epoch: 67
    Epoch: 68
    Epoch: 69
    Epoch: 70
    Epoch: 71
    Epoch: 72
    Epoch: 73
    Epoch: 74
    Epoch: 75
    Epoch: 76
    Epoch: 77
    Epoch: 78
    Epoch: 79
    Epoch: 80
    Epoch: 81
    Epoch: 82
    Epoch: 83
    Epoch: 84
    Epoch: 85
    Epoch: 86
    Epoch: 87
    Epoch: 88
    Epoch: 89
    Epoch: 90
    Epoch: 91
    Epoch: 92
    Epoch: 93
    Epoch: 94
    Epoch: 95
    Epoch: 96
    Epoch: 97
    Epoch: 98
    Epoch: 99
Saving Coach to: models.ignore/coach/LunarLanderContinuous-v2/final_1_99%/


-----------------------------------------------------------------------------------------------------
 Testing Agent+Coach
-----------------------------------------------------------------------------------------------------


test settings = {
    "force_recompute": False, 
    "graph_smoothing": 0, 
    "number_of_episodes_for_baseline": 30, 
    "number_of_epochs_for_optimal_parameters": 15, 
    "number_of_episodes_for_testing": 10, 
    "initial_epsilon": 100.1, 
    "initial_horizon": 16, 
    "reward_discount": 0.98, 
    "acceptable_performance_loss": 1, 
    "acceptable_performance_level": 0.8, 
    "confidence_interval_for_convergence": 90, 
    "min_reward_single_timestep": -100, 
    "max_reward_single_timestep": 70, 
    "horizons": {0.0: 1, 0.005: 10, 0.01: 20, }, 
}

[silver_spectacle] There were unviewed results
[silver_spectacle] So the display server is still running at: http://localhost:9900
[silver_spectacle] (use the stop server button to kill it now)



-------------------------------------------------------

 Environment: HopperBulletEnv-v0

-------------------------------------------------------




-----------------------------------------------------------------------------------------------------
 Agent Model Exists, loading: ./subrepos/rl-trained-agents/sac/HopperBulletEnv-v0_1/HopperBulletEnv-v0.zip
-----------------------------------------------------------------------------------------------------


Wrapping the env with a `Monitor` wrapper
Wrapping the env in a DummyVecEnv.


-----------------------------------------------------------------------------------------------------
 Training Coach Model from scrach
-----------------------------------------------------------------------------------------------------


coach settings = {
    "force_retrain": False, 
    "learning_rate": 0.0001, 
    "hidden_sizes": [64, 64, 64, 64, ], 
    "loss_function": "state_prediction_loss", 
    "consistent_coach_loss": {"scale_future_state_loss": 1000000, }, 
    "delayed_coach_loss": {"epochs_of_delay": 50, }, 
    "value_plus_state_loss": {"value_proportion": 0.5, }, 
    "number_of_episodes": 100, 
    "number_of_epochs": 100, 
    "train_test_split": 0.7, 
    "batch_size": 128, 
    "with_card": True, 
}
Starting Train/Test
    Epoch: 0
    Epoch: 1
    Epoch: 2
    Epoch: 3
    Epoch: 4
    Epoch: 5
    Epoch: 6
    Epoch: 7
    Epoch: 8
    Epoch: 9
    Epoch: 10
    Epoch: 11
    Epoch: 12
    Epoch: 13
    Epoch: 14
    Epoch: 15
    Epoch: 16
    Epoch: 17
    Epoch: 18
    Epoch: 19
    Epoch: 20
    Epoch: 21
    Epoch: 22
    Epoch: 23
    Epoch: 24
    Epoch: 25
    Epoch: 26
    Epoch: 27
    Epoch: 28
    Epoch: 29
    Epoch: 30
    Epoch: 31
    Epoch: 32
    Epoch: 33
    Epoch: 34
    Epoch: 35
    Epoch: 36
    Epoch: 37
    Epoch: 38
    Epoch: 39
    Epoch: 40
    Epoch: 41
    Epoch: 42
    Epoch: 43
    Epoch: 44
    Epoch: 45
    Epoch: 46
    Epoch: 47
    Epoch: 48
    Epoch: 49
    Epoch: 50
    Epoch: 51
    Epoch: 52
    Epoch: 53
    Epoch: 54
    Epoch: 55
    Epoch: 56
    Epoch: 57
    Epoch: 58
    Epoch: 59
    Epoch: 60
    Epoch: 61
    Epoch: 62
    Epoch: 63
    Epoch: 64
    Epoch: 65
    Epoch: 66
    Epoch: 67
    Epoch: 68
    Epoch: 69
    Epoch: 70
    Epoch: 71
    Epoch: 72
    Epoch: 73
    Epoch: 74
    Epoch: 75
    Epoch: 76
    Epoch: 77
    Epoch: 78
    Epoch: 79
    Epoch: 80
    Epoch: 81
    Epoch: 82
    Epoch: 83
    Epoch: 84
    Epoch: 85
    Epoch: 86
    Epoch: 87
    Epoch: 88
    Epoch: 89
    Epoch: 90
    Epoch: 91
    Epoch: 92
    Epoch: 93
    Epoch: 94
    Epoch: 95
    Epoch: 96
    Epoch: 97
    Epoch: 98
    Epoch: 99
Saving Coach to: models.ignore/coach/HopperBulletEnv-v0/final_1_99%/


-----------------------------------------------------------------------------------------------------
 Testing Agent+Coach
-----------------------------------------------------------------------------------------------------


test settings = {
    "force_recompute": False, 
    "graph_smoothing": 0, 
    "number_of_episodes_for_baseline": 30, 
    "number_of_epochs_for_optimal_parameters": 15, 
    "number_of_episodes_for_testing": 10, 
    "initial_epsilon": 100.1, 
    "initial_horizon": 16, 
    "reward_discount": 0.98, 
    "acceptable_performance_loss": 1, 
    "acceptable_performance_level": 0.8, 
    "confidence_interval_for_convergence": 90, 
    "min_reward_single_timestep": 18, 
    "max_reward_single_timestep": 110, 
    "horizons": {0.1: 26, 0.01: 16, 0.001: 11, }, 
}
baseline = {
    "max": 112.79861896992979, 
    "min": 105.7972408403108, 
    "range": 7.001378129618985, 
    "count": 30, 
    "sum": 3318.192922871891, 
    "average": 110.60643076239636, 
    "stdev": 1.49541705880447, 
    "median": 110.86516527143982, 
}
baseline_min = 110.14252736719435, baseline_max = 111.07033415759842,
baseline_confidence_size = 0.4639033952020384
----- finding optimal epsilon -------------------------------------------------------------------------------------------------------------------------------------
            reward_single_sum=22.370764710867906, 
            reward_single_sum=25.179023369136356, confidence_size=8.865323678797633, confidence_max=32.64021771879976, new_horizon=2
        episode=0, horizon=32, effective_score=23.77, baseline_lowerbound=88.49 baseline_stdev=0.27, new_epsilon=66.7333, bad=True, gap_average=6.137502288818359
            reward_single_sum=24.436391051280197, 
            reward_single_sum=23.59201117772307, confidence_size=2.6656023528693655, confidence_max=26.679803467370995, new_horizon=2
        episode=1, horizon=32, effective_score=24.01, baseline_lowerbound=88.49 baseline_stdev=0.27, new_epsilon=44.4889, bad=True, gap_average=3.687372298467727
            reward_single_sum=23.44114356741136, 
            reward_single_sum=22.233712417287638, confidence_size=3.81171012655574, confidence_max=26.649138118905235, new_horizon=2
        episode=2, horizon=32, effective_score=22.84, baseline_lowerbound=88.49 baseline_stdev=0.27, new_epsilon=29.6593, bad=True, gap_average=3.957793045043945
            reward_single_sum=21.37104670956045, 
            reward_single_sum=21.158744833394355, confidence_size=0.6702106461193793, confidence_max=21.93510641759678, new_horizon=16
        episode=3, horizon=32, effective_score=21.26, baseline_lowerbound=88.49 baseline_stdev=0.27, new_epsilon=19.7728, bad=True, gap_average=6.889262751529091
            reward_single_sum=34.66252900437519, 
            reward_single_sum=22.699683190739606, confidence_size=37.765217938585835, confidence_max=66.44632403614321, new_horizon=20
        episode=4, horizon=32, effective_score=28.68, baseline_lowerbound=88.49 baseline_stdev=0.27, new_epsilon=13.1819, bad=True, gap_average=2.8833480411105685
            reward_single_sum=48.1958348739124, 
            reward_single_sum=22.756923160117804, confidence_size=80.30748368392895, confidence_max=115.78386270094401, new_horizon=6
            reward_single_sum=22.415759302931153, confidence_size=24.928109546476165, confidence_max=56.050948658796614, new_horizon=6
        episode=5, horizon=32, effective_score=31.12, baseline_lowerbound=88.49 baseline_stdev=0.27, new_epsilon=8.7879, bad=True, gap_average=5.287864638537895
            reward_single_sum=56.018989151564575, 
            reward_single_sum=21.88058695009887, confidence_size=107.77069430619385, confidence_max=146.7204823570255, new_horizon=6
            reward_single_sum=61.20471469893109, confidence_size=36.01781978394304, confidence_max=82.38591671747454, new_horizon=9
        episode=6, horizon=32, effective_score=46.37, baseline_lowerbound=88.49 baseline_stdev=0.27, new_epsilon=5.8586, bad=True, gap_average=3.974618867126822
            reward_single_sum=22.011323515269503, 
            reward_single_sum=21.869021698398807, confidence_size=0.44922915591313917, confidence_max=22.38940176274729, new_horizon=9
        episode=7, horizon=32, effective_score=21.94, baseline_lowerbound=88.49 baseline_stdev=0.27, new_epsilon=3.9057, bad=True, gap_average=4.507457532380757
            reward_single_sum=92.48705076736505, 
            reward_single_sum=23.32026552586465, confidence_size=218.35094754621707, confidence_max=276.2546056928318, new_horizon=4
            reward_single_sum=22.064771881283257, confidence_size=67.94125181907185, confidence_max=113.89861454390947, new_horizon=4
            reward_single_sum=93.4425267497086, confidence_size=47.74600985193875, confidence_max=105.57466358299413, new_horizon=6
        episode=8, horizon=32, effective_score=57.83, baseline_lowerbound=88.49 baseline_stdev=0.27, new_epsilon=2.6038, bad=True, gap_average=2.766080151296685
            reward_single_sum=22.092281077473103, 
            reward_single_sum=104.00021689158314, confidence_size=258.57317691027754, confidence_max=321.6194258948055, new_horizon=10
            reward_single_sum=22.006621313741658, confidence_size=79.76505072448163, confidence_max=129.13142381874758, new_horizon=10
            reward_single_sum=103.3901209181627, confidence_size=55.46742072622099, confidence_max=118.33973077646112, new_horizon=8
            reward_single_sum=98.3923280634725, confidence_size=41.76339084097512, confidence_max=111.73970449386172, new_horizon=8
            reward_single_sum=99.34877996036225, confidence_size=33.70717165049281, confidence_max=108.57889635462536, new_horizon=8
        episode=9, horizon=32, effective_score=74.87, baseline_lowerbound=88.49 baseline_stdev=0.27, new_epsilon=1.7359, bad=True, gap_average=1.4577692778586335
            reward_single_sum=103.38527463182632, 
            reward_single_sum=100.54005155684564, confidence_size=8.982015749802919, confidence_max=110.94467884413889, new_horizon=8
            reward_single_sum=100.26390457957976, confidence_size=2.9130425631780668, confidence_max=104.30945281926198, new_horizon=8
        episode=10, horizon=8, effective_score=101.40, baseline_lowerbound=88.49 baseline_stdev=0.27, new_epsilon=2.6038, bad=False, gap_average=1.0613326461434014
            reward_single_sum=101.65416030354675, 
            reward_single_sum=100.84376215518792, confidence_size=2.5583262683962147, confidence_max=103.80728749776354, new_horizon=8
        episode=11, horizon=6, effective_score=101.25, baseline_lowerbound=88.49 baseline_stdev=0.27, new_epsilon=3.9057, bad=False, gap_average=1.491958970921014
            reward_single_sum=23.930370804517818, 
            reward_single_sum=21.958499714348612, confidence_size=6.224952041273998, confidence_max=29.169387300707207, new_horizon=6
        episode=12, horizon=8, effective_score=22.94, baseline_lowerbound=88.49 baseline_stdev=0.27, new_epsilon=2.6038, bad=True, gap_average=4.6839837573823475
            reward_single_sum=21.827367692493542, 
            reward_single_sum=99.12674258289624, confidence_size=244.02452265372276, confidence_max=304.5015777914175, new_horizon=8
            reward_single_sum=96.30921816065594, confidence_size=73.90466855969004, confidence_max=146.32577803837194, new_horizon=8
            reward_single_sum=94.85724780275831, confidence_size=44.1378237805435, confidence_max=122.1679678402445, new_horizon=8
            reward_single_sum=96.84196703962799, confidence_size=31.992629186447452, confidence_max=113.78513784213385, new_horizon=8
            reward_single_sum=21.650766792405925, confidence_size=31.89975634365259, confidence_max=103.6686413554589, new_horizon=8
        episode=13, horizon=8, effective_score=71.77, baseline_lowerbound=88.49 baseline_stdev=0.27, new_epsilon=1.7359, bad=True, gap_average=1.2577749177761368
            reward_single_sum=107.19626379347075, 
            reward_single_sum=104.14197266990581, confidence_size=9.64201760402559, confidence_max=115.31113583571387, new_horizon=8
            reward_single_sum=104.79095062756716, confidence_size=2.7127242351938605, confidence_max=108.0891199321751, new_horizon=8
        episode=14, horizon=8, effective_score=105.38, baseline_lowerbound=88.49 baseline_stdev=0.27, new_epsilon=2.6038, bad=False, gap_average=1.1657386238022995
optimal_epsilon = 3.9057460752934006
optimal_horizon = 6
    scaled_epsilon: 3.9057, forecast_average: 3.2061, episode_reward:417.00, max_timestep_reward: 5.41, min_timestep_reward: 0.23
    scaled_epsilon: 3.9057, forecast_average: 3.1146, episode_reward:535.48, max_timestep_reward: 5.42, min_timestep_reward: -1.27
    scaled_epsilon: 3.9057, forecast_average: 3.1875, episode_reward:23.52, max_timestep_reward: 3.44, min_timestep_reward: 0.37
    scaled_epsilon: 3.9057, forecast_average: 3.3281, episode_reward:23.75, max_timestep_reward: 3.34, min_timestep_reward: -1.42
    scaled_epsilon: 3.9057, forecast_average: 3.2382, episode_reward:457.25, max_timestep_reward: 5.16, min_timestep_reward: -1.23
    scaled_epsilon: 3.9057, forecast_average: 3.2330, episode_reward:681.16, max_timestep_reward: 5.14, min_timestep_reward: 0.24
    scaled_epsilon: 3.9057, forecast_average: 3.3425, episode_reward:25.32, max_timestep_reward: 3.05, min_timestep_reward: -1.96
    scaled_epsilon: 3.9057, forecast_average: 3.3310, episode_reward:550.79, max_timestep_reward: 4.87, min_timestep_reward: -0.47
    scaled_epsilon: 3.9057, forecast_average: 3.2664, episode_reward:24.80, max_timestep_reward: 3.40, min_timestep_reward: -0.53
    scaled_epsilon: 3.9057, forecast_average: 3.1591, episode_reward:168.98, max_timestep_reward: 4.57, min_timestep_reward: -2.87
argv[0]=
argv[0]=


-------------------------------------------------------

 Environment: HalfCheetahBulletEnv-v0

-------------------------------------------------------




-----------------------------------------------------------------------------------------------------
 Agent Model Exists, loading: subrepos/rl-trained-agents/sac/HalfCheetahBulletEnv-v0_1/HalfCheetahBulletEnv-v0.zip
-----------------------------------------------------------------------------------------------------


Wrapping the env with a `Monitor` wrapper
Wrapping the env in a DummyVecEnv.


-------------------------------------------------------

 Environment: BipedalWalker-v3

-------------------------------------------------------




-----------------------------------------------------------------------------------------------------
 Agent Model Exists, loading: ./subrepos/rl-trained-agents/sac/Walker2d-v3_1/Walker2d-v3.zip
-----------------------------------------------------------------------------------------------------


Wrapping the env with a `Monitor` wrapper
Wrapping the env in a DummyVecEnv.


-------------------------------------------------------

 Environment: AntBulletEnv-v0

-------------------------------------------------------




-----------------------------------------------------------------------------------------------------
 Agent Model Exists, loading: subrepos/rl-trained-agents/sac/AntBulletEnv-v0_1/AntBulletEnv-v0.zip
-----------------------------------------------------------------------------------------------------


Wrapping the env with a `Monitor` wrapper
Wrapping the env in a DummyVecEnv.


-------------------------------------------------------

 Environment: ReacherBulletEnv-v0

-------------------------------------------------------




-----------------------------------------------------------------------------------------------------
 Agent Model Exists, loading: ./subrepos/rl-trained-agents/sac/ReacherBulletEnv-v0_1/ReacherBulletEnv-v0.zip
-----------------------------------------------------------------------------------------------------


Wrapping the env with a `Monitor` wrapper
Wrapping the env in a DummyVecEnv.


-----------------------------------------------------------------------------------------------------
 Training Coach Model from scrach
-----------------------------------------------------------------------------------------------------


coach settings = {
    "force_retrain": False, 
    "learning_rate": 0.0001, 
    "hidden_sizes": [64, 64, 64, 64, ], 
    "loss_function": "state_prediction_loss", 
    "consistent_coach_loss": {"scale_future_state_loss": 1000000, }, 
    "delayed_coach_loss": {"epochs_of_delay": 50, }, 
    "value_plus_state_loss": {"value_proportion": 0.5, }, 
    "number_of_episodes": 100, 
    "number_of_epochs": 100, 
    "train_test_split": 0.7, 
    "batch_size": 128, 
    "with_card": True, 
}
Starting Train/Test
    Epoch: 0
    Epoch: 1
    Epoch: 2
    Epoch: 3
    Epoch: 4
    Epoch: 5
    Epoch: 6
    Epoch: 7
    Epoch: 8
    Epoch: 9
    Epoch: 10
    Epoch: 11
    Epoch: 12
    Epoch: 13
    Epoch: 14
    Epoch: 15
    Epoch: 16
    Epoch: 17
    Epoch: 18
    Epoch: 19
    Epoch: 20
    Epoch: 21
    Epoch: 22
    Epoch: 23
    Epoch: 24
    Epoch: 25
    Epoch: 26
    Epoch: 27
    Epoch: 28
    Epoch: 29
    Epoch: 30
    Epoch: 31
    Epoch: 32
    Epoch: 33
    Epoch: 34
    Epoch: 35
    Epoch: 36
    Epoch: 37
    Epoch: 38
    Epoch: 39
    Epoch: 40
    Epoch: 41
    Epoch: 42
    Epoch: 43
    Epoch: 44
    Epoch: 45
    Epoch: 46
    Epoch: 47
    Epoch: 48
    Epoch: 49
    Epoch: 50
    Epoch: 51
    Epoch: 52
    Epoch: 53
    Epoch: 54
    Epoch: 55
    Epoch: 56
    Epoch: 57
    Epoch: 58
    Epoch: 59
    Epoch: 60
    Epoch: 61
    Epoch: 62
    Epoch: 63
    Epoch: 64
    Epoch: 65
    Epoch: 66
    Epoch: 67
    Epoch: 68
    Epoch: 69
    Epoch: 70
    Epoch: 71
    Epoch: 72
    Epoch: 73
    Epoch: 74
    Epoch: 75
    Epoch: 76
    Epoch: 77
    Epoch: 78
    Epoch: 79
    Epoch: 80
    Epoch: 81
    Epoch: 82
    Epoch: 83
    Epoch: 84
    Epoch: 85
    Epoch: 86
    Epoch: 87
    Epoch: 88
    Epoch: 89
    Epoch: 90
    Epoch: 91
    Epoch: 92
    Epoch: 93
    Epoch: 94
    Epoch: 95
    Epoch: 96
    Epoch: 97
    Epoch: 98
    Epoch: 99
Saving Coach to: models.ignore/coach/ReacherBulletEnv-v0/final_1_99%/


-----------------------------------------------------------------------------------------------------
 Testing Agent+Coach
-----------------------------------------------------------------------------------------------------


test settings = {
    "force_recompute": False, 
    "graph_smoothing": 0, 
    "number_of_episodes_for_baseline": 30, 
    "number_of_epochs_for_optimal_parameters": 15, 
    "number_of_episodes_for_testing": 10, 
    "initial_epsilon": 100.1, 
    "initial_horizon": 16, 
    "reward_discount": 0.98, 
    "acceptable_performance_loss": 1, 
    "acceptable_performance_level": 0.8, 
    "confidence_interval_for_convergence": 90, 
}

[silver_spectacle] There were unviewed results
[silver_spectacle] So the display server is still running at: http://localhost:9900
[silver_spectacle] (use the stop server button to kill it now)

