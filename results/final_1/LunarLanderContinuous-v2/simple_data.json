{"override_save_path": false, "increment_factor": 1.2, "force_recompute": false, "graph_smoothing": 0, "number_of_episodes_for_baseline": 30, "number_of_epochs_for_optimal_parameters": 15, "number_of_episodes_for_testing": 10, "initial_epsilon": 3.5, "initial_horizon": 10, "reward_discount": 0.98, "acceptable_performance_levels": [0.8, 0.9, 0.95, 0.98, 0.99], "acceptable_performance_loss": 1, "acceptable_performance_level": 0.8, "confidence_interval_for_convergence": 90, "min_reward_single_timestep": -100, "max_reward_single_timestep": 70, "horizons": {"0.0": 1, "0.005": 10, "0.01": 20}, "api": "v2", "agent": {"gamma": 0.99, "path": "./subrepos/rl-trained-agents/sac/LunarLanderContinuous-v2_1/LunarLanderContinuous-v2.zip"}, "coach": {"path": "models.ignore/coach/LunarLanderContinuous-v2/baseline_3"}, "plot": {"optimal_reward_points": [[0.8, 24.379314870747702], [0.9, 24.379314870747702], [0.95, 24.379314870747702], [0.98, 24.379314870747702], [0.99, 24.379314870747702]], "random_reward_points": [[0.8, 0], [0.9, 0], [0.95, 0], [0.98, 0], [0.99, 0]], "theory_reward_points": [[0.8, -1382.1921871868658], [0.9, -1147.7636035105968], [0.95, -272.1728434797325], [0.98, -181.55968398375245], [0.99, -789.6088228940749]], "ppac_reward_points": [[0.8, 18.338991171279584], [0.9, 29.78578812010371], [0.95, 28.71854445103682], [0.98, 21.794890596473387], [0.99, 33.46903521864767]], "n_step_horizon_reward_points": [[0.8, 31.318687142244165], [0.9, 23.451058628124223], [0.95, 19.925282376014533], [0.98, 26.803961669012338], [0.99, 27.344629266868182]], "n_step_planlen_reward_points": [[0.8, 26.07628366837466], [0.9, 26.142236805029594], [0.95, 22.131154708589207], [0.98, 27.19910597148738], [0.99, 21.087559358890314]], "ppac_plan_length_points": [[0.8, 4.336091435223905], [0.9, 3.9984992506815984], [0.95, 5.80779866969924], [0.98, 3.8824836770939832], [0.99, 3.5658058672407456]], "n_step_horizon_plan_length_points": [[0.8, 6], [0.9, 6], [0.95, 8], [0.98, 6], [0.99, 6]], "n_step_planlen_plan_length_points": [[0.8, 6], [0.9, 6], [0.95, 8], [0.98, 6], [0.99, 6]]}, "0.8": {"optimal_epsilon": 1.4065715020576135, "optimal_horizon": 6}, "0.9": {"optimal_epsilon": 1.1721429183813445, "optimal_horizon": 6}, "0.95": {"optimal_epsilon": 1.1721429183813445, "optimal_horizon": 8}, "0.98": {"optimal_epsilon": 0.8139881377648227, "optimal_horizon": 6}, "0.99": {"optimal_epsilon": 0.8139881377648227, "optimal_horizon": 6}}