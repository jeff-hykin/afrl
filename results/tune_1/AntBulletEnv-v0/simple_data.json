{"override_save_path": false, "increment_factor": 1.2, "force_recompute": false, "graph_smoothing": 0, "number_of_episodes_for_baseline": 30, "number_of_epochs_for_optimal_parameters": 50, "number_of_episodes_for_testing": 150, "initial_epsilon": 3.5, "initial_horizon": 10, "reward_discount": 0.98, "acceptable_performance_levels": [0.5, 0.8, 0.9, 0.95, 0.97, 0.99, 1.0], "acceptable_performance_loss": 1, "acceptable_performance_level": 0.8, "confidence_interval_for_convergence": 90, "min_sample_size": 12, "min_reward_single_timestep": 19, "max_reward_single_timestep": 105, "horizon_cap": 20, "horizons": {"0": 1, "0.0007": 4, "0.0015": 16, "0.002": 26}, "api": "v2", "agent": {"gamma": 0.98, "path": "subrepos/rl-trained-agents/sac/AntBulletEnv-v0_1/AntBulletEnv-v0.zip"}, "coach": {"path": "models.ignore/coach/AntBulletEnv-v0/baseline_3"}, "plot": {"optimal_reward_points": [[0.5, 101.97566229531819], [0.8, 101.97566229531819], [0.9, 101.97566229531819], [0.95, 101.97566229531819], [0.97, 101.97566229531819], [0.99, 101.97566229531819], [1.0, 101.97566229531819]], "random_reward_points": [[0.5, 0], [0.8, 0], [0.9, 0], [0.95, 0], [0.97, 0], [0.99, 0], [1.0, 0]], "theory_reward_points": [[0.5, 3.059269868859548], [0.8, 3.671123842631458], [0.9, 3.8750751672220947], [0.95, 3.9770508295174127]], "ppac_reward_points": [[0.5, 52.405293662396396], [0.8, 80.19076536005187], [0.9, 87.34795973863558], [0.95, 90.62198227195518]], "n_step_horizon_reward_points": [[0.5, 50.38158597393884], [0.8, 68.5180625806458], [0.9, 70.09826453238126], [0.95, 70.25091972381294]], "n_step_planlen_reward_points": [[0.5, 47.1231207426528], [0.8, 79.69529117786662], [0.9, 78.29397945033148], [0.95, 97.9248843550236]], "ppac_plan_length_points": [[0.5, 14.364973333333333], [0.8, 2.4360733333333333], [0.9, 2.4253466666666665], [0.95, 1.6792666666666667]], "n_step_horizon_plan_length_points": [[0.5, 20], [0.8, 4], [0.9, 4], [0.95, 4]], "n_step_planlen_plan_length_points": [[0.5, 20], [0.8, 4], [0.9, 4], [0.95, 4]]}}