{"override_save_path": false, "increment_factor": 1.2, "force_recompute": true, "graph_smoothing": 0, "number_of_episodes_for_baseline": 30, "number_of_epochs_for_optimal_parameters": 50, "number_of_episodes_for_testing": 50, "initial_epsilon": 3.5, "initial_horizon": 10, "reward_discount": 0.98, "acceptable_performance_levels": [0.5, 0.8, 0.9, 0.95, 0.97, 0.99, 1.0], "acceptable_performance_loss": 1, "acceptable_performance_level": 0.8, "confidence_interval_for_convergence": 90, "min_reward_single_timestep": -23, "max_reward_single_timestep": 5, "horizons": {"0.1": 26, "0.01": 16, "0.001": 11}, "api": "v2", "agent": {"gamma": 0.98, "path": "./subrepos/rl-trained-agents/sac/BipedalWalker-v3_1/BipedalWalker-v3.zip"}, "coach": {"path": "models.ignore/coach/BipedalWalker-v3/baseline_3"}, "0.5": {"optimal_epsilon": 0.20823838778972237, "optimal_horizon": 20}, "plot": {"optimal_reward_points": [[0.5, 4.887292294975761], [0.8, 4.887292294975761], [0.9, 4.887292294975761], [0.95, 4.887292294975761], [0.97, 4.887292294975761], [0.99, 4.887292294975761], [1.0, 4.887292294975761]], "random_reward_points": [[0.5, 0], [0.8, 0], [0.9, 0], [0.95, 0], [0.97, 0], [0.99, 0], [1.0, 0]], "theory_reward_points": [[0.5, -245.41524982827053]], "ppac_reward_points": [[0.5, -0.24937187425030327]], "n_step_horizon_reward_points": [[0.5, -2.5950713391988995]], "n_step_planlen_reward_points": [[0.5, -12.579331745301332]], "ppac_plan_length_points": [[0.5, 12.187750591862462]], "n_step_horizon_plan_length_points": [[0.5, 20]], "n_step_planlen_plan_length_points": [[0.5, 20]]}}