{"override_save_path": false, "increment_factor": 1.2, "force_recompute": false, "graph_smoothing": 0, "number_of_episodes_for_baseline": 30, "number_of_epochs_for_optimal_parameters": 50, "number_of_episodes_for_testing": 250, "initial_epsilon": 3.5, "initial_horizon": 10, "reward_discount": 0.98, "acceptable_performance_levels": [0.95, 0.9, 0.8, 0.5], "acceptable_performance_loss": 1, "acceptable_performance_level": 0.8, "confidence_interval_for_convergence": 90, "min_sample_size": 30, "min_reward_single_timestep": -60, "max_reward_single_timestep": 100, "horizon_ceiling": 25, "horizon_floor": 4, "horizons": {"0": 1, "0.001": 11, "0.01": 13, "0.1": 26}, "api": "v2", "agent": {"gamma": 0.98, "path": "subrepos/rl-trained-agents/sac/HalfCheetahBulletEnv-v0_1/HalfCheetahBulletEnv-v0.zip"}, "coach": {"path": "models.ignore/coach/HalfCheetahBulletEnv-v0/baseline_3"}, "plot": {"optimal_reward_points": [[0.95, 104.0365893650006], [0.9, 104.0365893650006], [0.8, 104.0365893650006], [0.5, 104.0365893650006]], "random_reward_points": [[0.95, 0], [0.9, 0], [0.8, 0], [0.5, 0]], "theory_reward_points": [[0.95, 98.83475989675055]], "ppac_reward_points": [[0.95, 100.57691483676597]], "n_step_horizon_reward_points": [[0.95, 52.696459676404565]], "n_step_planlen_reward_points": [[0.95, 100.67412282843806]], "n_step_median_reward_points": [[0.95, 78.38025759557105]], "ppac_plan_length_points_average": [[0.95, 1.740444]], "ppac_plan_length_points_median": [[0.95, 2.042]], "n_step_horizon_plan_length_points": [[0.95, 4]], "n_step_planlen_plan_length_points": [[0.95, 2]], "n_step_median_plan_length_points": [[0.95, 3]]}}