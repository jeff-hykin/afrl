{"override_save_path": false, "increment_factor": 1.2, "force_recompute": false, "graph_smoothing": 0, "number_of_episodes_for_baseline": 30, "number_of_epochs_for_optimal_parameters": 50, "number_of_episodes_for_testing": 150, "initial_epsilon": 3.5, "initial_horizon": 10, "reward_discount": 0.98, "acceptable_performance_levels": [0.5, 0.8, 0.9, 0.95, 0.97, 0.99, 1.0], "acceptable_performance_loss": 1, "acceptable_performance_level": 0.8, "confidence_interval_for_convergence": 90, "min_sample_size": 12, "min_reward_single_timestep": 0, "max_reward_single_timestep": 100, "horizon_ceiling": 20, "horizon_floor": 4, "api": "v2", "agent": {"gamma": 0.98, "path": "./subrepos/rl-trained-agents/sac/ReacherBulletEnv-v0_1/ReacherBulletEnv-v0.zip"}, "coach": {"path": "models.ignore/coach/ReacherBulletEnv-v0/baseline_3"}, "plot": {"optimal_reward_points": [[0.5, 14.748507074818418], [0.8, 14.748507074818418], [0.9, 14.748507074818418], [0.95, 14.748507074818418], [0.97, 14.748507074818418], [0.99, 14.748507074818418], [1.0, 14.748507074818418]], "random_reward_points": [[0.5, 0], [0.8, 0], [0.9, 0], [0.95, 0], [0.97, 0], [0.99, 0], [1.0, 0]], "theory_reward_points": [[0.5, 7.374253537409209], [0.8, 11.798805659854736], [0.9, 13.273656367336576]], "ppac_reward_points": [[0.5, 9.393959399313072], [0.8, 13.29391316864952], [0.9, 14.515898470787333]], "n_step_horizon_reward_points": [[0.5, 6.56829664273197], [0.8, 7.472452813473445], [0.9, 6.636006943153069]], "n_step_planlen_reward_points": [[0.5, 8.07928920344063], [0.8, 13.479448975759192], [0.9, 14.374407791358264]], "ppac_plan_length_points": [[0.5, 3.2546666666666666], [0.8, 0.6585333333333333], [0.9, 0.7548444444444444]], "n_step_horizon_plan_length_points": [[0.5, 4], [0.8, 4], [0.9, 4]], "n_step_planlen_plan_length_points": [[0.5, 4], [0.8, 1], [0.9, 1]]}}