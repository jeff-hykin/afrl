\def\year{2022}\relax
%File: formatting-instruction.tex
\documentclass[letterpaper]{article} % DO NOT CHANGE THIS
\usepackage{aaai22}  % DO NOT CHANGE THIS
\usepackage{times}  % DO NOT CHANGE THIS
\usepackage{helvet}  % DO NOT CHANGE THIS
\usepackage{courier}  % DO NOT CHANGE THIS
\usepackage[hyphens]{url}  % DO NOT CHANGE THIS
\usepackage{graphicx} % DO NOT CHANGE THIS
\urlstyle{rm} % DO NOT CHANGE THIS
\def\UrlFont{\rm}  % DO NOT CHANGE THIS
\usepackage{natbib}  % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\usepackage{caption} % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\DeclareCaptionStyle{ruled}{labelfont=normalfont,labelsep=colon,strut=off} % DO NOT CHANGE THIS
\frenchspacing  % DO NOT CHANGE THIS
\setlength{\pdfpagewidth}{8.5in}  % DO NOT CHANGE THIS
\setlength{\pdfpageheight}{11in}  % DO NOT CHANGE THIS
% \usepackage{lineno}
% \linenumbers

% These are recommended to typeset algorithms but not required. See the subsubsection on algorithms. Remove them if you don't have algorithms in your paper.
% \usepackage{algorithm}
% \usepackage{algorithmic}

%
% These are are recommended to typeset listings but not required. See the subsubsection on listing. Remove this block if you don't have listings in your paper.
\usepackage{newfloat}
\usepackage{listings}
\lstset{%
	basicstyle={\footnotesize\ttfamily},% footnotesize acceptable for monospace
	numbers=left,numberstyle=\footnotesize,xleftmargin=2em,% show line numbers, remove this entire line if you don't want the numbers.
	aboveskip=0pt,belowskip=0pt,%
	showstringspaces=false,tabsize=2,breaklines=true}
% \floatstyle{ruled}
% \newfloat{listing}{tb}{lst}{}
% \floatname{listing}{Listing}
%

\usepackage[utf8]{inputenc}
\usepackage[ruled,vlined,linesnumbered]{algorithm2e}
\usepackage{amsmath, amsthm}
\usepackage{amsfonts}
\usepackage[dvipsnames]{xcolor}
\newcommand{\GS}[1] {{\color{red} \textbf{[GS]: #1}}}
\newcommand{\JC}[1] {{\color{blue} \textbf{[JC]: #1}}}
\newcommand{\JA}[1] {{\color{red} \textbf{[JA]: #1}}}

\DeclareMathOperator*{\argmax}{arg\,max}


\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{assumption}{Assumption}
\newtheorem{definition}{Definition}[]

\pdfinfo{
/Title A Framework for Predictable Actor-Critic Control
/Author Josiah Coad, Guni Sharon
/Keywords reinforcement learning, interpretability, multiagent, predicability
}

\setcounter{secnumdepth}{1} %May be changed to 1 or 2 if section numbers are desired.

%Example, Multiple Authors, ->> remove \iffalse,\fi and place them surrounding AAAI title to use it
\title{A Framework for Predictable Actor-Critic Control}
\author { 
Anonymous 
    % Josiah Coad, \textsuperscript{\rm 1}
    % James Ault, \textsuperscript{\rm 2}
    % Guni Sharon \textsuperscript{\rm 2}
}
\affiliations {
    % Affiliations
    % \textsuperscript{\rm 1} General Motors\\
    % \textsuperscript{\rm 2} Texas A&M University\\
    % jcoad@gm.com, jault@tamu.edu, gsharon@tamu.edu
}


\begin{document}
    \maketitle
    \begin{abstract}
        Reinforcement learning (RL) algorithms commonly provide a one action plan per time step. Doing this allows the RL agent to quickly adapt and respond to stochastic environments yet it restricts the ability to predict the agent's future behavior. This paper proposes an actor-critic framework that predicts and follows an $n$-step plan. Committing to the next $n$ actions presents a trade-off between behavior predictability and reduced performance. In order to balance this trade-off, a dynamic plan-following criteria is proposed for determining when it is too costly to follow the preplanned actions and a replanning procedure should be initiated instead. 
        Performance degradation bounds are presented for the proposed criteria when assuming access to accurate state-action values. Experimental results, using several robotics domains, suggest that the performance bounds are also satisfied in the general (approximation) case on expectancy. Additionally, the experimental section presents a study of the predictability versus performance degradation trade-off and demonstrates the benefits of applying the proposed plan-following criteria. 
    \end{abstract}

    \section{Introduction}

        Deep reinforcement-learning (RL) algorithms are considered state of the art for solving Markov decision processes~\cite{haarnoja2018soft,schulman2017proximal,mnih2015human}. Such algorithms can perform at, and even surpass, human-level control in various domains. Examples include robotics~\cite{haarnoja2018soft,dey2021jirl}, traffic management~\cite{Ault2021signals,ault2020learning}, autonomous driving~\cite{8793742,sallab2017deep}, and energy management~\cite{gao2014machine,vandael2015reinforcement}. 

        Common RL algorithms train a policy which, given the current state of the environment, returns a single action to be applied at the current time step. While allowing the RL agent flexibility to quickly react to changes in stochastic environments, such a one-step planning horizon limits the ability to predict the agent's behavior. This is a limiting characteristic of common RL algorithms, as predictability was shown to be beneficial in multiagent domains~\cite{wen2019probabilistic,sun2018probabilistic,kim2020communication,das2019tarmac,raileanu2018modeling} and for regulating an RL-agent's performance in safety-critical tasks~\cite{8897630}, e.g. a human operator overseeing an autonomous driving controller. Additionally, it is shown to be helpful for an agent to learn a stable policy by relying on its own predicted plan~\cite{racaniere2017imagination}.

        \begin{figure}
            \includegraphics[width=0.48\textwidth]{figures/pac-system.png}
            \caption{An overview of the PrAC framework. A predicted (imagined) plan is computed in order to provide predictable control. At each step the previously predicted plan is evaluated to determine whether replanning should be initiated at the expense of reduced predictability.}
            \label{pac-system}
        \end{figure}

        In this paper we take a first step toward reconciling state-of-the-art RL with $n$-step planning while bounding the potential performance degradation. \JA{add in coach description}
        The proposed approach, denoted predictable actor-critic (PrAC), trains an environment model approximator which is used to produce and store \textit{imaginary plans}~\cite{racaniere2017imagination}. At each time step, PrAC uses a state-action ($Q$) value approximator to evaluate the performance degradation affiliated with following the (previously computed) imaginary plan. If the expected future discounted reward is reduced by more than some threshold, $\epsilon$, a replanning operation is performed and a new (presumably better) plan is followed. The proposed approach is illustrated in Figure \ref{pac-system}.  We show that, under simplifying assumptions, applying PrAC on top of a static policy would not degrade the expected sum of discounted rewards by more than $\frac{\epsilon}{1-\gamma}$, where $\gamma$ is the domain discount factor.

        Our experimental results show that in some domains, such as LunarLander~\cite{gym}, Walker, and Hopper~\cite{pybullet}, PrAC results in little or no performance degradation while predicting an average of 12, 5, and 4 actions into the future, respectively. Meanwhile for other domains, such as Cheetah and Humanoid~\cite{pybullet}, PrAC incurs substantial ($\sim$22\%) performance degradation to produce an average of only 3 and 2 forecasted actions, respectively.\JA{update performance numbers}

        \subsection{Preliminaries} 
            This paper addresses control problems modeled as \textit{Markov Decision Processes} (MDPs)~\cite{puterman2014markov}, each with state space, $\mathcal{S}$, action space, $\mathcal{A}$, transition probabilities, $P$, reward function, $R$, and discount factor, $\gamma$. 
            An agent is assumed to start from state $s_0$ and select action $a_0$ according to a \textit{policy}, $\pi$. The policy is commonly assumed to be stochastic (soft policy)~\cite{haarnoja2018soft}, i.e., mapping states to a distribution over actions.
            Given a soft policy, an action is selected by sampling the affiliated distribution, $a_t \sim \pi(s_t)$. Based on the chosen action and transition probability, $P(s_{t+1} | s_t, a_t)$, the agent receives a reward, $r_t \sim R(s_t, a_t, s_{t+1})$, from the environment and observes the next state, $s_{t+1}$. A transition is a tuple of the form $(s_t,a_t,r_t,s_{t+1})$. A set of consecutive transitions generates a (stochastic) trajectory, $\tau = (s_0, a_0, r_0, s_1, s_1, a_1, r_1, s_2,\ldots)$. A solution for a given MDP is a policy, $\pi^* = \argmax_{\pi} V^{\pi}(s_0)$, where $V^{\pi}(s)=\mathbb{E}_{\tau \sim \pi| s_0=s}[\sum_{t=0}^{\infty}\gamma^t r_t]$ is called the state value function. In this paper, we quantify an algorithm's \textit{performance} as $V(s_0)$. An action-state ($Q$) value is defined over a policy, $\pi$, as $Q^\pi(s,a)=\mathbb{E}_{\tau \sim \pi | s_0=s, a_0=a}[\sum_{t=0}^{\infty}\gamma^t r_t]$. $Q^*$ is the $Q$ value obtained under the optimal policy ($\pi^*$). %We define a \textit{$Q$-greedy policy} as $\pi^Q(s)$ which returns $\argmax_a\hat{Q}(s,a)$, where $\hat{Q}$ is some approximation function for the $q$-values.

            State-of-the-art RL algorithms~\cite{haarnoja2018soft} use an actor-critic framework where an action-state value approximator ($Q$) is trained. Following the Policy-Gradient Theorem~\cite{reinforce}, the approximated $Q$ values are used to compute and apply a policy gradient step with respect to the expected sum of discounted rewards.

    \section{Related Work}

        This paper proposes extending the common actor-critic framework to compute and follow $n$-step plans. In order to do so, we suggest training a model approximator for the underlying state transition probabilities ($P$).   
        Training a model of the environment, i.e., approximating $P(s'|s,a)$ based on observed transitions, is commonly done as part of a \textit{model-based RL}~\cite{kaiser2019model,xu2020prediction} approach. Such a model can be used to produce imaginary trajectories~\cite{racaniere2017imagination} that improve training sample efficiency in many domains. The imagined trajectory is a simulated future trajectory based on the learned model and current policy. We differentiate between the imaginary trajectory, which is a set of predicted $(s,a,r,s')$ transitions, and the imaginary plan, which is the affiliated sequence of actions in the imaginary trajectory. 

        For computing the imaginary trajectory, Racaniere et al.~\citeyear{racaniere2017imagination} trained an environment model with a recurrent architecture. The proposed environment model extended on the notion of action-conditional next-step predictors~\cite{oh2015action,chiappa2017recurrent,leibfried2017deep}, which predicts the next state, and potentially the reward, received after following a given action at a given state (or history of states).
        It is important to note that modeling errors can compound over long trajectories resulting in an unlikely imaginary trajectory and an affiliated risky imaginary plan. Such modeling errors were shown to be common in complex domains~\cite{talvitie2014model,talvitie2015agnostic}. \JA{present as baseline method}

        In order to reduce environment modeling errors, Ke et al.~\citeyear{ke2018modeling} proposed building a latent-variable autoregressive model~\cite{gulrajani2016pixelvae} by leveraging recent ideas in variational inference~\cite{zhang2018advances}. Another recent approach for reducing modeling errors~\cite{nagabandi2018neural} suggests training a prediction model, $\hat{f}_\phi(s_t, a_t)$, that outputs not the next state, $s_{t+1}$, but the resulting change to the current state, $s_{t+a}-s_t$. The affiliated ($l2$) loss function is defined as $L_\phi(s_t,a_t,s_{t+1})=\Vert (s_{t+1}-s_t) -  \hat{f}_\phi(s_t, a_t) \Vert ^2$; see Eq2 in Nagabandi et al.

        %The agent generates an imaginary $n$-step plan of its next n actions it intends to take (including the action it plans to take in the current timestep). In this work, when we say plan, we are referring to such an imaginary plan. 
        Kim et al.~\citeyear{kim2020communication} proposed to utilize imaginary plans towards intention sharing in multiagent RL scenarios. It was demonstrated that broadcasting agents' future intentions (the imagined plans) improve coordination and overall performance over sharing just the current and past states of agents~\cite{foerster2016learning,sukhbaatar2016learning,jiang2018learning,das2019tarmac}.  


        %\cite{sun2018probabilistic} - AVs have to accurately predict the behavior of surrounding vehicles and plan accordingly. Such prediction should also be interactive, since the distribution over all possible trajectories of the predicted vehicle depends not only on historical information, but also on future plans of other vehicles that interact with it. To achieve such interaction-aware predictions, we propose a probabilistic prediction approach based on hierarchical inverse reinforcement learning (IRL).

        %\cite{kim2020communication} - Communication is one of the core components for learning coordinated behavior in multiagent systems. In this paper, we propose a new communication scheme named Intention Sharing (IS) for multiagent reinforcement learning in order to enhance the coordination among agents. In the proposed IS scheme, each agent generates an imagined trajectory by modeling the environment dynamics and other agents’ actions. The imagined trajectory is a simulated future trajectory of each agent based on the learned model of the environment dynamics and other agents and represents each agent’s future action plan. Each agent compresses this imagined trajectory capturing its future action plan to generate its intention message for communication by applying an attention mechanism to learn the relative importance of the components in the imagined trajectory based on the received message from other agents.


        % communication protocol for coordination among multiple agents (Foerster et al. (2016); Sukhbaatar et al.
        % (2016); Jiang & Lu (2018); Das et al. (2019))
        % It has been shown that due to the capability of sharing observation information, this kind of communication scheme has good performance as compared to communication-free MARL algorithms such as independent learning, which is widely used in MARL, in partially observable environments.
        % The message-generation networks in the aforementioned algorithms are conditioned on the current observation or a hidden state of LSTM. Under partially
        % observable environments, such messages which encode past and current observations are useful but
        % do not capture any future intentions.

        %Strouse et al. (2018) introduced information-regularizer to share or hide agent’s intention to other agents for a multi-goal MARL setting in which some agents know the goal and other agents do not know the goal. By maximizing (or minimizing) the mutual information between the goal and action, an agent knowing the goal learns to share (or hide) its intention to other agents not knowing the goal in cooperative (or competitive) tasks. They showed that sharing intention is effective in the cooperative case.

        %predict other agents’ behaviors: Neil C Rabinowitz, Frank Perbet, H Francis Song, Chiyuan Zhang, SM Eslami, and Matthew Botvinick. Machine theory of mind; Roberta Raileanu, Emily Denton, Arthur Szlam, and Rob Fergus. Modeling others using oneself in multiagent reinforcement learning; 


        %Jakob N Foerster, Gregory Farquhar, Triantafyllos Afouras, Nantas Nardelli, and Shimon Whiteson. Counterfactual multiagent policy gradients. In Thirty-second AAAI conference on artificial intelligence, 2018

        %Sainbayar Sukhbaatar, Rob Fergus, et al. Learning multiagent communication with backpropagation. In Advances in neural information processing systems, pp. 2244–2252, 2016.

        %Jiechuan Jiang and Zongqing Lu. Learning attentional communication for multiagent cooperation. In Advances in neural information processing systems, pp. 7254–7264, 2018.

        %Abhishek Das, Th ́eophile Gervet, Joshua Romoff, Dhruv Batra, Devi Parikh, Mike Rabbat, and Joelle Pineau. Tarmac: Targeted multiagent communication. In International Conference on Machine Learning, pp. 1538–1546, 2019.

        %DJ Strouse, Max Kleiman-Weiner, Josh Tenenbaum, Matt Botvinick, and David J Schwab. Learning to share and hide intentions using information regularization. In Advances in Neural Information Processing Systems, pp. 10249–10259, 2018.

        %\cite{racaniere2017imagination} - proposing Imagination-Augmented Agents, which use approximate environment models by "learning to interpret" their imperfect predictions. Our algorithm can be trained directly on low-level observations with little domain knowledge, similarly to recent model-free successes. 

        % The environment model is any recurrent architecture which can be trained in an unsupervised
        % fashion from agent trajectories: given a past state and current action, the environment model predicts
        % the next state and any number of signals from the environment. In this work, we will consider
        % in particular environment models that build on recent successes of action-conditional next-step
        % predictors [Junhyuk Oh, Xiaoxiao Guo, Honglak Lee, Richard L Lewis, and Satinder Singh. Action-conditional video
        % prediction using deep networks in atari games. In Advances in Neural Information Processing Systems,
        % pages 2863–2871, 2015. ; Silvia Chiappa, Sébastien Racaniere, Daan Wierstra, and Shakir Mohamed. Recurrent environment simulators. In 5th International Conference on Learning Representations, 2017. ; Felix Leibfried, Nate Kushman, and Katja Hofmann. A deep learning approach for joint video frame and reward prediction in atari games.], which receive as input the current observation (or history of observations) and
        % current action, and predict the next observation, and potentially the next reward. We roll out the
        % environment model over multiple time steps into the future, by initializing the imagined trajectory
        % with the present time real observation, and subsequently feeding simulated observations into the
        % model. The I2A therefore learns to combine information from its
        % model-free and imagination-augmented paths. 2As can thus be thought of as augmenting model-free agents by
        % providing additional information from model-based planning, and as having strictly more expressive
        % power than the underlying model-free agent.

        % In complex domains for which a simulator is
        % not available to the agent, recent successes are dominated by model-free methods. In such
        % domains, the performance of model-based agents employing standard planning methods usually
        % suffers from model errors resulting from function approximation [Erik Talvitie. Model regularization for stable sample rollouts. In UAI, pages 780–789, 2014.], [Erik Talvitie. Agnostic system identification for monte carlo planning. In AAAI, pages 2986–2992, 2015.].
        % These errors compound
        % during planning, causing over-optimism and poor agent performance.


        % \cite{ke2018modeling} - In model-based reinforcement learning, the agent interleaves between model
        % learning and planning. These two components are inextricably intertwined. If the model is not able to provide sensible long-term prediction, the executed planner would exploit model flaws, which can yield catastrophic failures. This paper
        % focuses on building a model that reasons about the long-term future and demonstrates how to use this for efficient planning and exploration. To this end, we
        % build a latent-variable autoregressive model by leveraging recent ideas in variational inference.


        % \cite{wen2019probabilistic} - The social skill is critical in daily life for reasoning about the
        % potential consequences of others’ behaviors so as to plan ahead. Our hypothesis is that it is beneficial for each agent to account for how the opponents would react to its future behaviors. Our framework adopts
        % variational Bayes methods to approximate the opponents’ conditional policies.

    \section{Predictable Actor-Critic control} \label{section:PAC}

        %In this section, we present our method (PAC) for generating an imaginary future plan for any agent which learns $Q$-values. We then show that the suboptimaity (over a $Q$-optimal policy) incurred by PAC is bounded by a constant value.

        As stated above, Racaniere et al.~\citeyear{racaniere2017imagination} used imagined trajectories to provide context to model-based algorithms. However, using those trajectories as a planned control sequence presents a challenge. Following noisy predictions of imaginary trajectories may reduce final performance or prevent policy convergence altogether. This noise or variance in the imagined trajectory is determined by the two components which produce it: the agent's policy and the model of the environment. When the policy is suboptimal or the model is inaccurate, approximation of the trajectory may fail to correctly predict the future. Even with an optimal policy and fully known model, a stochastic environment could push the agent off a predicted trajectory. As such, new predictions must be produced when the current one is evaluated as insufficient.

        Perpetually changing the predicted (imaginary) trajectory, however, does not lend well to predicting which actions an agent will take in the future.
        In our approach, Predictable Actor-Critic (PrAC), we suggest a method of merging these two goals in a generalized framework by reviewing the current plan at each new state and preserving planned steps for as long as some performance degradation threshold is met.
        A predictable control plan is therefore a trade-off between maximizing the length of predicted plans and minimizing performance degradation. We utilize values learned by the critic component of a $Q$-learning actor-critic RL algorithm to evaluate trajectories imagined through an approximated model. \JA{introduce coach} 

        Algorithm~\ref{algo:PAC} presents the PrAC framework when applied on top of Soft Actor-Critic (SAC)~\cite{haarnoja2018soft}. PrAC can be extended to other algorithms which learn and store Q-values such as DQN~\cite{mnih2015human}. However, studying the impact of PrAC in such cases is beyond the scope of this paper. We note that SAC might seem incompatible with PrAC as it does not converge on exact $Q$ values but some combination of the $Q$ values and a bias towards high entropy in the $Q$-value distribution. However, if the temperature parameter ($\alpha$) in SAC is reduced over time to zero, as proposed by Haarnoja et al.~\citeyear{haarnoja2018soft}, then the learned $Q$-values are eventually unbiased.  

        PrAC receives, as input, a performance degradation tolerance value, $\epsilon$. In line~\ref{ln:init}, PrAC starts by initializing three function approximators, a policy ($\pi_\theta$), an action-value function ($Q_\psi$), and a transition function ($P_\phi$).  
        % SAC is augmented with a learned environment model $\hat{P}$ similar to \cite{racaniere2017imagination}, which predicts a future state given the current state and an action. PAC further deviates by forming a plan and acting according to that plan as much as possible rather than drawing directly from the policy $\pi$. 
        At the beginning of each episode a new plan is imagined (Line~\ref{ln:init_plan}). The initial plan is a simple rollout of the current policy ($\pi$) on top of the current model approximation ($P$). Next, for each environment step, a first-in-first-out action is retrieved from the imagined plan queue (Line~\ref{ln:dequeue}) and executed (Line~\ref{ln:execute}). Once the next state is observed, a replan operation is initiated, where the current imaginary plan is reevaluated and adjusted (Line~\ref{ln:replan}). Finally, in Lines \ref{ln:loopgrad}--\ref{ln:endgrad}, the policy, action-state approximator, and model approximator are updated using gradient descent. As presented, the policy ($\pi$) and state-action approximator ($Q$) updates follow the SAC, entropy-adjusted loss function. As such, these include a learning rate parameter, $\lambda_Q$ and $\lambda_\pi$, respectively, as well as the temperature parameter, $\alpha$. We note that these update rules are not inherent to PrAC and thus refer the reader to Haarnoja et al.~\citeyear{haarnoja2018soft} which provides full details regarding these parameters and suggested value assignments.
        The gradient step update for the model approximator ($P$) follows the model loss function proposed in Nagabandi et al.~\citeyear{nagabandi2018neural} along with a learning rate parameter, $\lambda_P$.    %The plan is initially filled using the current policy (e.g. underlying SAC policy). Upon revisiting the plan, we use the action-value function to compare the expected reward for following the plan versus generating a new action (and thus new plan) from the current state. If the Q values diverge by more than a threshold $\epsilon$, we break from our current plan and generate a new one.

        Algorithm 2 details how an imaginary plan is evaluated and updated within the \textit{Replan} procedure. As input, it takes the current state of the environment ($s$), the current imagined plan ($plan$), the policy ($\pi$), state-action function ($Q$), environment model ($P$), and performance degradation tolerance parameter ($\epsilon$). Each action in the current imagined plan, starting from the earliest planned action and moving forwards in time, is checked against the plan-following criteria. This criteria determines whether an action and its succeeding plan, should still be considered as the projected plan. The proposed criteria compares the state-action value of the preplanned (imaginary) action against the state-action value of an action drawn from the behavior policy ($\pi$). For an action to remain in the imaginary (predicted) plan its value must be within $\epsilon$ of the value that a newly recalculated imagined plan would have taken. The tolerance parameter $\epsilon$ bounds the acceptable degradation in performance for facilitating a predictable control plan.
        The criteria is checked for each action in Lines~\ref{ln:loopplan}-\ref{ln:criteria}; if passed, the plan is preserved in Line~\ref{ln:reappend}. The imaginary plan validation process continues as the following state is sampled from the approximated environment model. It should be noted that the model may predict different state transitions than it predicted when the plan was originally formed. So long as the planed action meets the criteria, no adjustment is necessary. However, once an action is found in violation of the criteria then the future plan from that point onward should be recomputed. The preservation of the plan up to this action provides the predictable portion of the plan.

        In Algorithm 2, the imagination core function from Racaniere et al.~\citeyear{racaniere2017imagination} is used in Lines~\ref{ln:loopimagine}-\ref{ln:endimagine}. An action is first sampled from the policy at the final state in the current imaginary plan. Then the state transition is predicted by the environment model given both the state and action. This process could be repeated to produce an arbitrary length imaginary plan. 

        % \begin{lemma}
        % When assuming unbiased model approximation, $\hat{P}(s'|s,a)=\mathbb{E}[P(s'|s,a)]$, and a static policy, $\pi$, the imaginary plan is an unbiased estimation of the future plan.
        % \end{lemma}

        % \begin{proof}
        % The states visited along the most likely trajectory, $\tau$, is defined by $\argmax_{(s_o,...,s_T)} \prod_i \mathbb{E}[\pi(a_i|s_i)P(s_{i+1}|s_i,a_i)]=\mathbb{E}[\prod_iP(s_{i+1}|s_i,a_i)]$.
        % \end{proof}

        Line~\ref{ln:loopimagine} of Algorithm~\ref{algo:replan} presents a method for slowly building up the length of the imaginary plan (1 step beyond the current plan length per step) as we can consistently follow the plan that we have. However, if we break from our current plan, this logic resets our plan length. This approach is preferable for computational reasons. In cases where computational resources are sufficient, one can set a constant $n$-step prediction horizon. This would always result in a constant length imaginary plan, even when the model approximation is not reliable enough to allow following lengthy plans. In such cases, the plan following criteria will often truncate the imaginary plan.  


        \begin{algorithm}[h]%H
            % \SetAlgoLined\
            \SetKwInOut{Input}{Input}
            \SetKwInOut{Output}{Output}
            \Input{Suboptimality tolerance $\epsilon$}
            Initialize: policy ($\pi$) parameters $\theta$, action-value function ($Q$) parameters $\psi$, model approximation ($P$) parameters $\phi$, empty replay buffer $\mathcal{D}$ \label{ln:init}\\
            \For{each episode}{
                reset environment: $s \gets s_0$\\
                $plan \leftarrow$REPLAN($s, plan:=\text{empty array}, \pi_\theta, P_\phi, Q_\psi, \epsilon$) \label{ln:init_plan}\\
                \For{each environment step}{
                    $a \gets dequeue(plan)$ \label{ln:dequeue}\\
                    execute $a$ and observe $r, s'$ \label{ln:execute}\\
                    store $(s, a, r, s')$ in $\mathcal{D}$\\
                    $s \gets s'$ \\
                    $plan \gets$REPLAN($s, plan, \pi_\theta, P_\phi, Q_\psi, \epsilon$)\label{ln:replan} \textit{\color{OliveGreen} // reuses as much of the old plan as possible} \\
                }
                \For{each gradient step}{\label{ln:loopgrad}
                $\psi = \psi - \lambda_Q \nabla_\psi [Q(s_t,a_t)-(r_t+\gamma (Q(s_{t+1},a_{t+1})-\alpha \log \pi(a_{t+1}, a_{t+1})))]$\\
                
                $\theta = \theta - \lambda_\pi \nabla_\theta [\alpha \log \pi(a_t|s_t) - Q(s_t,a_t)]$\\
                
                $\phi = \phi - \lambda_P \nabla_\phi [P(s_t,a_t)-s_{t+1}]^2$\label{ln:endgrad} \\
                }
            }
            \caption{Predictable Actor-Critic (PrAC)}
            \label{algo:PAC}
        \end{algorithm}

        \begin{algorithm}[h]
            % \SetAlgoLined\
            \SetKwInOut{Input}{Input}
            \SetKwInOut{Output}{Output}
            \Input{state $s$, current plan $plan$, policy $\pi$, action-value function $Q$, model approximation $P$, performance degradation tolerance $\epsilon$}
            \Output{a new plan $plan'$}
            Initialize an empty array $plan'$\\
            \For{$t$ in $[0,..., length(plan)-1]$}{\label{ln:loopplan}
                $a \gets plan[t]$\\
                \If{$Q(s, a) + \epsilon < Q(s, \pi(\cdot|s))$}{\label{ln:criteria}
                    \textit{break} \textit{\color{OliveGreen} // 
            critic says that following our last plan is  $\epsilon$-suboptimal. Abort.}\\
                }
                append $a$ to $plan'$\label{ln:reappend}\\ 
                $s \sim P(\cdot|s, a)$\\
            }
            \For{2 iterations}{\label{ln:loopimagine}
                $a \sim \pi(\cdot|s)$\\
                append $a$ to $plan'$ \\
                $s \sim P(\cdot|s, a)$ \label{ln:endimagine}\\
            }
            \Return{$plan'$}
            \caption{Replan}
            \label{algo:replan}
        \end{algorithm}

        \subsection{Performance bounds}

            We show that, under simplifying assumptions, the proposed plan-following criterion bounds the performance degradation incurred by PrAC. 

            % For our theoretical analysis, we assume function approximation is not used and PAC has been executed to convergence visiting each state infinitely often. In this case the entropy term is reduced to 0 and may be omitted. Furthermore, we also assume that the model is unbiased; and as such, bounding the value function bounds the performance degradation of the forecast plan taken by PAC.

            \begin{lemma}
            Assuming known state-action ($Q$) values for a policy $\pi$, and a discount factor, $0<\gamma<1$, applying PrAC on top of $\pi$ would not degrade the sum of discounted rewards by more than $\frac{\epsilon}{1-\gamma}$.
            \end{lemma}

            \begin{proof}
            At every state, $s$, PrAC applies action, $a^p$, which satisfies: 
            \begin{equation}\label{eq:local_bound}
            V^\pi(s) \le Q^\pi(s,a^p) + \epsilon 
            \end{equation}
            By definition: 
            \begin{equation}\label{eq:q_def}
            Q^\pi(s,a^p)=E_{s'\sim P(s'|s,a^p)}\left[R(s,a^p)+\gamma V^\pi(s')\right] 
            \end{equation}
            Combining \ref{eq:local_bound} and~\ref{eq:q_def} yields:
            \begin{equation}\label{eq:v_bound}
            V^\pi(s) \le E_{s'\sim P(s'|s,a^p)}\left[R(s,a^p)+\gamma V^\pi(s')\right] + \epsilon
            \end{equation}
            Equation~\ref{eq:v_bound} results in a recursive definition over successive state values $V^\pi(s)$ and $V^\pi(s')$. Considering the PrAC state-value definition, $V^p(s)=E_{\tau \sim PrAC|s_0 = s}\sum_t \gamma^t R(s_t,a^p)$, and expanding the recursive term results in:
            \begin{equation}\label{eq:global_bound}
            V^\pi(s) \le V^p(s) + \sum_t \gamma^t \epsilon
            \end{equation}
            
            As a result, assuming $\gamma < 1$, the performance degradation of PrAC can be bounded by $\sum_{t=0}^\infty \gamma^t \epsilon = \frac{\epsilon}{1-\gamma}$.
            \end{proof}

            In many realistic scenarios, it is not reasonable to assume that $Q^\pi$ is known but that it is only approximated. Nonetheless, we present empirical results showing that this bound also holds, in expectancy, for cases where $Q^\pi$ is only approximated, as is the case in actor-critic algorithms. 

    \section{Experimental Study} 

        % Experimental results suggest that in some domains, e.g., LunarLanderContinuous-v2~\cite{gym}, setting $\varepsilon=0.01\delta$, results in no performance degradation while providing a 10 step forecasted plan on average. Similarly for BipedalWalker-v3\JC{cite pybullet} which results in no performance degradation at $\varepsilon=0.01\delta$ and yet can provide a 4.9 forecasted plan on average.  In other domains however, e.g., Humanoid~\cite{pybullet}, the same $\varepsilon$ factor, $0.01$, yields 23\% reduced performance and a 2.1 step forecasted plan on average.


        The experimental study is designed towards the following objectives:
        \begin{enumerate}
            \item Investigate to what extent does the theoretical performance bound for PrAC hold when $Q$-values are approximated.
            
            \item Present the trade-off between performance degradation and predicted plan length in several domains, both for the case when we use PrAC during training and the case where we train the baseline policy and then apply PrAC. 
        \end{enumerate}


        %has been designed to investigate the plans generated by PAC applied to SAC and the empirical performance degradation of those policies under varied values of $\epsilon$. We show empirically that PAC respects the lower bound performance degradation constraints derived in Section \ref{section:PAC}. We follow this with an examination of the effect of $\epsilon$ on the forecast. 

        \subsection{Domains}
            Results for six continuous-action domains are presented in this section. These include: HumanoidBulletEnv-v0, HopperBulletEnv-v0, HalfCheetahBulletEnv-v0, Walker2DBulletEnv-v0, and AntBulletEnv-v0. 
            These domains were selected to be similar to those used to study SAC~\cite{haarnoja2018soft}. 
            Additionally, we present results for the LunarLanderContinuous-v2. %This environment requires considerably fewer computational resources facilitating future research.
            A snapshot from all six domains can be seen in Figure~\ref{domains}.



            The domains included here have state spaces which are vectors of high-level features (i.e. not images). For the PyBullet robotic domains, the state space consists of physics descriptors for the agent. For example, for the Ant domain, the state space consists of the position and orientation of the torso and the joint angles, the velocity of the agent and the external forces applied to each of the links at the center of mass.

            The action spaces in all environments are continuous. For the PyBullet robotic domains, the action spaces are the amount of torque applied to each actuator. Humanoid is especially challenging because it has 17 actuators. Refer to Table \ref{table:state-action-dim} for the state and action space size for each environment we use.

            \begin{center}
                \begin{table}
                    \centering
                    \caption{State and Action Space Size for Environments}
                    \begin{tabular}{ | c c c | }
                        \hline
                        Environment & Action Dim. & State Dim. \\
                        \hline
                        Lander & 2 & 8 \\ 
                        Hopper & 3 & 15 \\  
                        Walker & 6 & 22 \\    
                        Cheetah & 6 & 26 \\  
                        Ant & 8 & 28 \\  
                        Humanoid & 17 & 44 \\
                        \hline
                    \end{tabular}
                    \label{table:state-action-dim}
                \end{table}
            \end{center}


            \begin{figure}
                \includegraphics[width=0.48\textwidth]{figures/domains.png}
                \caption{Snapshots from the domains used in the experimental section.}
                \label{domains}
            \end{figure}

        \subsection{Hyper-parameter settings}

            The reported PrAC implementation utilizes the open-source Stable Baselines~\cite{stable-baselines} implementation of SAC. Stable Baselines maintains a well-documented library of benchmark reinforcement-learning algorithms. The code for our experiments is available at [omitted for blind review].

            In our experiments many hyper-parameters were constant between the domains and were selected to match the tuned parameters made available in the Stable Baselines Zoo~\cite{rl-zoo3}. These were as follows: A 3e-4 learning rate for both the actor ($\lambda_\pi$) and the critic ($\lambda_Q$), a 1e6 buffer size, 100 random actions before starting learning, a minibatch size for each gradient update of 256, a soft polyak update coefficient ($\tau$) of 0.005, a discount factor ($\gamma$) of 0.98, and a training frequency of every step with one gradient step per training and a target network update frequency of every step.
            The entropy coefficient ($\alpha$) in SAC, equivalent to the inverse of reward scale in the original SAC paper, was dynamically adjusted using the Stable Baselines implementation.

            \begin{figure*}
                \includegraphics[width=\textwidth]{figures/bounds-combined.png}
                \caption{Left: normalized performance between random performance (red) and optimal performance (green) along with the theoretical performance bounds (yellow) and observed PrAC performance (blue) for different epsilon coefficient values. Right: average forecast as a function of the epsilon coefficient value. Shaded regions represent a 1 standard deviation over 20 runs per setting.}
                \label{bounds}
            \end{figure*}

            The Humanoid and Cheetah domains required some parameters to be adjusted from other domains. In the Humanoid domain a batch size of 64 was used and the number of random acts pre-training was raised to 1,000. In the Cheetah domain the number of random acts pre-training was raised to 10,000, learning rate ($\lambda_\pi$, $\lambda_Q$) set to 7.3e-4, a 3e-5 buffer size, $\tau=0.02$, and an update frequency of once every 8 steps with 8 gradient steps.
            The environment model for all domains uses a small network with 4 hidden layers, each with 64 nodes and relu activation functions. The Adam optimizer with learning rate ($\lambda_P$) of 1e-4 was used to optimize parameters.



        \subsection{Computing resources}

            Modest compute was necessary to conduct our experiments. Amazon EC2 type t3.large instances were used. This type contains 2 vCPUs and 2 GB of memory. Experiments were run on the Amazon Linux 2 operating system and require the following open-source Python packages: NumPy (1.19.5), Gym (0.18.0), Torch (1.8.1), PyBullet (3.1.4) and Stable Baselines3 (1.1.0).

        \subsection{Performance bounds}

            We start by investigating the performance degradation introduced by PrAC as a function of the tolerance parameter $\epsilon$. The theoretical analysis in Section~\ref{section:PAC} relies on several limiting assumptions. However, we find that in practice this bound still holds in empirical evaluation. In order to present a clearer trend over several domains, we report the normalized performance for PrAC where the normalized performance is defined as follows.

            \begin{definition}[Normalized performance]
                $$NP(PrAC)=\frac{V^{PrAC}(s_0) -V^{rand}(s_0)}{V^{\pi^*}(s_0)-V^{rand}(s_0)}$$
                $V^{rand}$ is the expected sum of discounted returns following a random policy.\\
                $V^{\pi^*}$ is the expected sum of discounted returns following a $Q$-greedy policy, i.e., $\pi^*(s)=\argmax_aQ(s,a)$. 
            \end{definition}

            In the left-hand side of Figure \ref{bounds}, the theoretical bound is plotted in yellow as a function of the $\epsilon$ coefficient on the horizontal axis with $\gamma = 0.98$. The lower-bound formula derived from Section \ref{section:PAC} is used here. The bound decreases as $\epsilon$ increases. On the vertical axis, the performance of a random baseline (red) and the SAC policy $\pi$ value (green) is plotted. The blue line, $V^{PrAC}(s_0)$, represents the normalized performance of PrAC. In Lander, Hopper and Walker environments, we can see that the forecast decreases very little while providing a multistep action plan. Performance of other environments, namely Humanoid, Cheetah and Ant do slightly worse and experience some noticeable performance degradation. This is likely because they are harder environments to model (Table \ref{table:state-action-dim} shows the increased state space size for these environments, making an environment model harder to learn.) Nevertheless, all environments respect the theoretical bounds in expectation.  

            %$V^{PrAC}(s_0)$ decreases with epsilon increase in humanoid, cheetah, ant
            %other domains $V^{PrAC}(s_0)$ remains close to $V^{\pi}(s_0)$
            %

            \begin{figure*}
                \includegraphics[width=\textwidth]{figures/training.png}
                \caption{Top: training curves from representative environments. The $y$ axis is the normalized episode reward. Bottom: The associated forecast for the environments. The baseline algorithm shows SAC without any modifications. Experiments run with 5 replications of each setting. Shown with 1 std. Mean and std aggregated over rolling window of 20 episodes. The forecast of PrAC(0.1) for cheetah and humanoid has been capped at 16 and 20, respectively, for computational considerations. Humanoid-0.1 is stopped after 3e6 steps as it fails to improve beyond random.}
                \label{figure:training}
            \end{figure*}

            The hyper-parameter epsilon that we introduce with PrAC has the intuition that $\epsilon=0$ reduces to a plan generator with no additional care given to retaining the current plan, e.g. the type of plan generators in \cite{racaniere2017imagination} and \cite{kim2020communication}. We would expect a very small forecast with $\epsilon=0$. On the other hand, a $\epsilon=\infty$ is a plan generator which never deviates from the current plan and thus would have a high forecast and low performance. The epsilon coefficient equal to $\epsilon / (V^{\pi}(s_0)-V^{rand}(s_0))$ is used to generalize the epsilon hyper-parameter better across domains.



        \subsection{Predictability-performance trade-off}
            The design of PrAC leads to a trade-off between agent performance and plan reliability. We seek to characterize this relationship through a series of experiments that showcase both use cases for PrAC. PrAC can be applied both during training and after training. This makes PrAC a versatile addition to any RL agent which learns Q-values. If predictability is desired during training, then training-\textit{with}-PrAC should be considered, else training-\textit{then}-PrAC can be used to apply PrAC to an already fully trained agent. We analyze and report results on both training-\textit{with}-PrAC and training-\textit{then}-PrAC.

            \textbf{Forecast}: This is our measure (scalar) of predictability or how far into the future PrAC provides a reliable plan. This value is computed per time step and in hindsight. When applying action $a_t$, forecast$_t$ is defined as $f_t=t+1-i$, where $a_t$ was originally determined during the $Replan$ procedure (Algorithm~\ref{algo:replan}) at time step $i$. The forecast value of a full episode (of length $T$) is defined as $\frac{1}{T}\sum_{t=0}^Tf_t$.

            In Figure~\ref{figure:training} the learning curves of 3 representative domains are presented when PrAC's imagined plans are used while the agent is learning. Learning curves for decreasing values of $\epsilon$ are shown against a learning curve for baseline Soft Actor-Critic.

            In the LunarLander domain (the leftmost plot in Figure~\ref{figure:training}), values smaller than $0.01$ achieve the domain goal of a 200 point reward. The rate of reaching this reward is similar to the baseline for each as well. However, PrAC additionally offers a forecast of about 5 during this training which provides additional predictability. When the epsilon coefficient is increased to $0.1$, forecast length is increased dramatically; but the agent is too stubborn on keeping its old plan instead of replanning, so the episode reward never reaches the final baseline episode reward.

            A similar trend is observed in the Cheetah domain (the center plot in Figure~\ref{figure:training}). When the $\epsilon$ coefficient is large, e.g. 0.1, episode reward is greatly reduced with an improvement in forecasting. Using PrAC for this coefficient is impractical. In Cheetah, we can start to observe the trade-off at $0.01$ and $0.001$. For a small incurred reward penalty, we gain a forecast. The fact that the episode returns from PrAC(0.001) are slightly higher than the baseline for Cheetah is likely a random artifact.

            With humanoid (the rightmost plot in Figure~\ref{figure:training}), we can see that a large epsilon coefficient totally obstructs the policy from learning. The episode reward stays consistent with that of a random agent. However, for smaller values of the epsilon coefficient, the agent does indeed learn and obtains a final episode reward within 75\% of the baseline agent learned policy. 

            Overall, we observe a trend across the studied environments\textemdash decreased performance yields improved forecasting. This tradeoff occurs in both training-with-PrAC and training-then-PrAC. Next, we observe the effect of first training a baseline model to convergence and then applying PrAC on a static underlying policy. The results from these experiments are shown in the right-hand side of Figure~\ref{bounds} and in Figure~\ref{forecast-dist}. In Figure~\ref{bounds} and Figure~\ref{forecast-dist}, agents were trained to convergence without using PrAC's imagined plans. Only post-convergence were the imagined plans followed. At low values of $\epsilon$, some domains see no loss in performance, while others experience very little. As $\epsilon$ increases, performance decreases across all environments. An opposite reaction applies to forecast lengths. As $\epsilon$ increases, forecasts improve. Across environments, our empirical findings show that an imagined plan can predict, on average, 5 steps into the future at a cost of a 25\% performance degradation.

            The forecast distributions shown in Figure~\ref{forecast-dist} give us greater insight into the type of reliability we can expect out of a PrAC agent. We see that the forecast distributions are generally right skewed, although for four of the environments, the distribution is not right skewed on PrAC(0.1). We speculate that this is because these PrAC(0.1) has chosen to be completely stubborn in this case and not change plans for the sacrifice of totally degraded performance. Such a historical forecast distribution as presented here could help a supervising agent interpret the plans being generated by the PrAC agent to know how confident it can be in the plans generated.

            % Put discussion here about why better forecast when train with PAC

            % \subsection{Conclusions}

            % \JC{Not sure what to put here. Recap the trend that we see?}

            %However, those domains were based in the Mujoco physics simulator. Mujoco requires a license that can be a limiting factor for those seeking to replicate and further research. For this reason we have elected to present results in the Bullet open-source physics simulator. Similar environments to those in Mujoco are available in Bullet allowing wider accessibility. Additionally we have chosen to present results from LunarLander environment. This environment requires considerably fewer computational resources facilitating future research. Our implementation of PAC is built on top of the open-source Stable Baselines~\cite{stable-baselines} implementation of SAC. Stable Baslines maintains a well-documented library of benchmark reinforcement learning algorithms. All code for our experiments is available at (omitted for blind review).



            %\textbf{Performance Range}: To normalize and better interpret PAC results, we define a performance interval $$\delta:=V^{\pi}(s_0)-V^{rand}(s_0)$$ where $V^{\pi}(s_0)$ is the SOTA performance of the baseline SAC agent from Stable Baselines \cite{stable-baselines} and $V^{rand}(s_0)$ is the performance of a random agent.

            %\textbf{Normalized Performance}: We report the normalized performance of our algorithm $$\frac{V^{PAC}(s_0) - V^{rand}(s_0)}{\delta}$$
            %This allows us to evaluate the trade-off between performance and forecast.



            %The forecast of an action $a$ is the number of steps in the past that we planned on taking $a$. More formally, if at timestep $t$ we execute an action $a$ that we had originally chosen $t+1-h$ steps ago, our forecast for $a$ would be $h$.

            %A baseline RL algorithm such as SAC has a forecast of 0 since at timestep t, we generate and execute an action we generated $t+1-0$ timesteps ago. The intuition behind this metric is that it can be thought of as a proxy for the level of confidence we can have in the plan that is generated. A planner which changes plans at every timestep, and thus can not be counted on to produce reliable plans, would have a forecast of 1, even though it might generate $n>1$ step plans. 





            % \begin{figure}
            %     \includegraphics[width=0.5\textwidth]{figures/results_stacked.png}
            %     \caption{Train-\textit{then}-PAC. Error bars are 1 std. 10 runs for each setting. Baseline has performance=1 (i.e. $V^\pi$) and forecast=1.}
            %     \label{results_stacked}
            % \end{figure}



            \begin{figure}[!h]
                \includegraphics[width=0.5\textwidth]{figures/forecast_dist.png}
                \caption{A histogram of the forecast length when applying  PrAC after training the underlying policy. The mean forecast is shown with a red bar. The columns are separated by epsilon coefficient and the rows are separated by environment. The $x$ axis is the forecast and the $y$ axis is the normalized count. We run 10 replications for each plot. Each environment has had the 0.1 forecast capped to 25 for computational considerations.}
                \label{forecast-dist}
            \end{figure}


    \section{Summary}
        Our research examines augmentations to RL algorithms that provide robust long-horizon planning. Doing so is of high importance in domains that require human supervision and in multiagent settings that require communicating behavior intentions. As a result, this research is expected to have a substantial impact on state-of-the-art deep RL technology for autonomous driving and similar safety critical / multiagent domains.

        In this work, we have proposed a solution to give our intention (forecast) of what we plan to do in the future. Furthermore, we propose an easily adopted method to allow for reliability in this plan via action-value evaluations of our plan. We have proven a theoretical lower bound for the performance degradation for the PrAC policy. We have also shown empirically that PrAC respects these bounds in the expectation even under biases that result from model approximation. Finally, we have shown empirically the trade-off between plan consistency and performance degradation. Reinforcement learning has great promise for real world applications but many of these applications are in safety critical and multiagent settings. By proposing a system for generating reliable future intention, we open up the door to applications where predictability is of high importance.


    \bibliography{ref.bib}
    % \bibliographystyle{aaai22}

\end{document}