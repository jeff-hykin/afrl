\documentclass{article}
\usepackage{neurips_2022} % for first submission 
% \usepackage[preprint]{neurips_2022}
% \usepackage[final]{neurips_2022}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
% custom
\usepackage[ruled,vlined,linesnumbered]{algorithm2e}


\title{A Framework for Predictable Actor-Critic Control}
\author{%
    % Josiah Coad, \textsuperscript{\rm 1}
    % Jeff Hykin, \textsuperscript{\rm 2}
    % James Ault, \textsuperscript{\rm 2}
    % Guni Sharon \textsuperscript{\rm 2}
    Jeff Hykin
    \texttt{hippo@cs.cranberry-lemon.edu} \\
    % examples of more authors
    % \And
    % Coauthor \\
    % Affiliation \\
    % Address \\
    % \texttt{email} \\
    % \AND
    % Coauthor \\
    % Affiliation \\
    % Address \\
    % \texttt{email} \\
    % \And
    % Coauthor \\
    % Affiliation \\
    % Address \\
    % \texttt{email} \\
    % \And
    % Coauthor \\
    % Affiliation \\
    % Address \\
    % \texttt{email} \\
}

\newcommand{\AND }{ \land  }
\newcommand{\OR  }{ \lor   }
\newcommand{\NOT }{ \lnot  }
\newcommand{\SUM }{ \Sigma }
\newcommand{\MULT}{ \cdot  }

% TODO: refernce format: Citations may be author/year or numeric. Use natbib \url{http://mirrors.ctan.org/macros/latex/contrib/natbib/natnotes.pdf}

% for graphics:
%    \usepackage[pdftex]{graphicx} ...
%    \includegraphics[width=0.8\linewidth]{myfile.pdf}
% (\url{http://mirrors.ctan.org/macros/latex/required/graphics/grfguide.pdf})

\begin{document}
    \maketitle
    \begin{abstract}
        Simple tasks such as walking across a street often involve choosing predictability over a small but immediate gratification. Current reinforcement learning (RL) techniques often fail to capitalize on the benefits of planning and commitment. Even when a plan is formed, if there is any benefit, no matter how small, most agent designs will not hesitate to abandon the plan. This paper proposes a method allowing an agent follow a plan so long as there is minimal impact on total reward. The work includes both theoretical bounds on the reward impact and supporting experimental data.
    \end{abstract}

    % 
    % Intro
    % 
    \section{Introduction}
    
        The typical structure of an RL problem, is to provide an observation value, expect the agent to produce a single action value, and respond with a new observation and reward. This design of a Markov decision process (MDP), has been used to model many domains. Deep reinforcement learning are considered state of the art for solving MDP's, even surpassing human control in some areas~\cite{haarnoja2018soft,schulman2017proximal,mnih2015human}. Examples include: (FIXME insert cited examples)


    % 
    % 
    % 
    % Acknowledgments
    % 
    % 
    % 
        % \begin{ack}
        %     FIXME
        %     Do {\bf not} include this section in the anonymized submission, only in the final paper.
            
        %     Use unnumbered first level headings for the acknowledgments.
        %     All acknowledgments go at the end of the paper before the list of references.
        %     - declare funding (financial activities supporting the submitted work)
        %     - competing interests (related financial activities outside the submitted work)
        % \end{ack}


    % 
    % 
    % 
    % References
    % 
    % 
    % 
    \section*{References}
        Use unnumbered first-level heading for references.
        Any choice of citation style is acceptable as long as you are consistent.
        Reference section does not count towards the page limit.
    \medskip
    
    \bibliography{ref.bib}



    % 
    % 
    % 
    % Checklist
    % 
    % 
    % 
    \section*{Checklist}
    % change the default \answerTODO{} to \answerYes{}, \answerNo{}, or \answerNA{}
    % examples:
    % \begin{itemize}
    %   \item Did you include the license to the code and datasets? \answerYes{See Section~\ref{gen_inst}.}
    %   \item Did you include the license to the code and datasets? \answerNo{The code and the data are proprietary.}
    %   \item Did you include the license to the code and datasets? \answerNA{}
    % \end{itemize}
    \begin{enumerate}


        \item For all authors...
            \begin{enumerate}
                \item Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?
                    \answerTODO{}
                \item Did you describe the limitations of your work?
                    \answerTODO{}
                \item Did you discuss any potential negative societal impacts of your work?
                    \answerTODO{}
                \item Have you read the ethics review guidelines and ensured that your paper conforms to them?
                    \answerTODO{}
            \end{enumerate}


        \item If you are including theoretical results...
            \begin{enumerate}
                \item Did you state the full set of assumptions of all theoretical results?
                    \answerTODO{}
                \item Did you include complete proofs of all theoretical results?
                    \answerTODO{}
            \end{enumerate}


        \item If you ran experiments...
            \begin{enumerate}
                    \item Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)?
                        \answerTODO{}
                    \item Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)?
                        \answerTODO{}
                    \item Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)?
                        \answerTODO{}
                    \item Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)?
                        \answerTODO{}
            \end{enumerate}


        \item If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...
            \begin{enumerate}
                \item If your work uses existing assets, did you cite the creators?
                    \answerTODO{}
                \item Did you mention the license of the assets?
                    \answerTODO{}
                \item Did you include any new assets either in the supplemental material or as a URL?
                    \answerTODO{}
                \item Did you discuss whether and how consent was obtained from people whose data you're using/curating?
                    \answerTODO{}
                \item Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content?
                    \answerTODO{}
            \end{enumerate}


        \item If you used crowdsourcing or conducted research with human subjects...
            \begin{enumerate}
                \item Did you include the full text of instructions given to participants and screenshots, if applicable?
                    \answerTODO{}
                \item Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable?
                    \answerTODO{}
                \item Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation?
                    \answerTODO{}
            \end{enumerate}


    \end{enumerate}


    % 
    % 
    % Appendix
    % 
    % 
    \appendix
    \section{Appendix}
    
        % 
        % epsilon optimizer
        % 
        \begin{algorithm}[ht]
            
            \newcommand{\planlensPerEpsilon}{planlens_\epsilon}
            \newcommand{\discountedRewards}{discountedRewards}
            \newcommand{\episodeRewards}{e_r}
            \newcommand{\minimumAcceptableReward}{b_{lower}}
            \newcommand{\numberOfbaselineSamples}{b_{sz}}
            \newcommand{\initialEpsilon}{\epsilon_{i}}
            \newcommand{\runningEpsilon}{\epsilon_{r}}
            \newcommand{\finalEpsilon}{\epsilon_{o}}
            \newcommand{\allEpsilons}{a_\epsilon}
            \newcommand{\intialHorizon}{h_{i}}
            \newcommand{\finalHorizon}{h_{o}}
            \newcommand{\planLengths}{planlens}
            
            % \SetAlgoLined\
            \SetKwInOut{Input}{Input}
            \SetKwInOut{Output}{Output}
            \Input{ \\
                $\mathrm{\minimumAcceptableReward}$, minimum acceptable reward \\
                $\mathrm{iter}$ , number of iterations of refinement \\
                $\mathrm{inc}$, scale multiplier for epsilon \\
                $\mathrm{\numberOfbaselineSamples}$, number of baseline samples \\
                $\mathrm{b_{min}}$, min confidence interval of baseline \\
                $\mathrm{b_{ci}}$, baseline confidence interval width \\
                $\mathrm{ci_{lvl}}$, confidence level  \\
                $\mathrm{\initialEpsilon}$, initial epsilon \\
                $\mathrm{\intialHorizon}$, initial horizon \\
            }
            \Output{
                \\
                $\mathrm{\finalEpsilon}$, a refined epsilon\\
                $\mathrm{\finalHorizon}$, a refined horizon\\
            }
            \caption{Find Epsilon}
            \label{algo:find_epsilon}
            \vspace{8pt}
            
            
            \(\runningEpsilon \gets  \initialEpsilon \)                                                           \\
            \(\allEpsilons \gets []\) \# empty array                                                              \\
            \(\planlensPerEpsilon \gets \{\}\) \# empty hashmap                                                   \\
            \# Note: requesting a non-existent key for this hash-map returns an array containing $\intialHorizon$ \\
            \For{$i$ in $[0,..., iter]$}{
                % \label{ln:loopplan1}
                \(\episodeRewards \gets []\) \# empty array \\
                \For{$j$ in $[0,..., \numberOfbaselineSamples]$}{
                    % \label{ln:loopsample1}
                    \(  \discountedRewards, \planLengths \gets runEpisode(\epsilon=\runningEpsilon) \)                      \\
                    $\episodeRewards.push(\SUM(\discountedRewards))$                                                        \\
                    $\planlensPerEpsilon[\runningEpsilon] \gets concat(\planlensPerEpsilon[\runningEpsilon], \planLengths)$ \\
                    \If{$length(\episodeRewards) < 2$} {
                        $continue$
                    }
                    
                    $s_{max}, s_{min} \gets confidenceInterval(ci_{lvl}, \episodeRewards)$ \\
                    $s_{ci} \gets s_{max} - s_{min}$ \\
                    \If{$ (s_{max} < \minimumAcceptableReward) \OR (s_{ci} < b_{ci}) $}{
                        $break$
                    }
                }
                
                \If{$mean(\episodeRewards) \geq \minimumAcceptableReward$}{
                    $\runningEpsilon \gets \runningEpsilon \MULT inc $
                }
                \Else{
                    $\runningEpsilon \gets \runningEpsilon / inc $
                }
                
                $\allEpsilons.push(\runningEpsilon)$
            }
            $\finalEpsilon \gets median(\allEpsilons)$

            $\finalHorizon \gets max(median(\planlensPerEpsilon[\finalEpsilon]), 1) \MULT 2$

            \Return{
                $\mathrm{\finalEpsilon}, \mathrm{\finalHorizon}$
            }
        \end{algorithm}


\end{document}